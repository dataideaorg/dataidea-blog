[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the DATAIDEA blog",
    "section": "",
    "text": "GridSearchCV\n\n\n\n\n\n\n2024\n\n\nMachine Learning\n\n\n\nGridSearchCV is a method in the scikit-learn library, which is a popular machine learning library in Python. It’s used for hyperparameter optimization.\n\n\n\n\n\nJul 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding K-Nearest Neighbors (KNN) Regression\n\n\n\n\n\n\n2024\n\n\nMachine Learning\n\n\n\nLearn Programming for Data Science\n\n\n\n\n\nJul 8, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nClassification Metrics Practice\n\n\n\n\n\n\nData Analysis\n\n\nMachine Learning\n\n\n\nLearn Programming for Data Science. Demonstrate loading, preparing, training, and evaluating a machine learning model using the Iris dataset\n\n\n\n\n\nJul 4, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA for Feature Selection\n\n\n\n\n\n\nData Analysis\n\n\n\nIn this notebook, we demonstrate how ANOVA (Analysis of Variance) can be used to identify better features for machine learning models\n\n\n\n\n\nJul 2, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Supervised Machine Learning\n\n\n\n\n\n\nData Analysis\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nJun 25, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing Our New Programming for Data Science App!\n\n\n\n\n\n\nAnnouncements\n\n\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Cost Function in Linear Regression\n\n\n\n\n\n\nMachine Learning\n\n\nData Analysis\n\n\n\n\n\n\n\n\n\nJun 11, 2024\n\n\nJuma\n\n\n\n\n\n\n\n\n\n\n\n\nWho Will Win Euro 2024? The Opta Predictions\n\n\n\n\n\n\nEUROS\n\n\n\n\n\n\n\n\n\nJun 11, 2024\n\n\nJuma\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of Machine Learning\n\n\n\n\n\n\nData Analysis\n\n\nMachine Learning\n\n\nAI\n\n\n\n\n\n\n\n\n\nJun 5, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nWhy You Should Use ChatGPT Sparingly.\n\n\n\n\n\n\nAI\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Missing Data in Pandas, When to Use bfill and ffill Methods\n\n\n\n\n\n\nData Analysis\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised Learning using Scikit Learn\n\n\n\n\n\n\nData Analysis\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nOpenAI Suspends ChatGPT Voice Allegedly Mimicking Scarlett Johansson in “Her”\n\n\n\n\n\n\nMovies\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Time Series\n\n\n\n\n\n\nData Analysis\n\n\n\n\n\n\n\n\n\nMay 18, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Forecasting\n\n\n\n\n\n\nData Analysis\n\n\n\n\n\n\n\n\n\nMay 18, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Analysis\n\n\n\n\n\n\nData Analysis\n\n\n\n\n\n\n\n\n\nMay 18, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate on DATAIDEA Fantasy Football League Standings\n\n\n\n\n\n\nFun Stuff\n\n\n\n\n\n\n\n\n\nApr 27, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Missing Data\n\n\n\n\n\n\nData Analysis\n\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nBudget-Friendly Options for Deploying Machine Learning Models\n\n\n\n\n\n\nAI\n\n\nSoftware\n\n\nOther\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Options for Deploying Machine Learning Models\n\n\n\n\n\n\nData Analysis\n\n\nAI\n\n\nSoftware\n\n\nOther\n\n\n2023\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nPython Virtual Environments\n\n\n\n\n\n\nData Analysis\n\n\nSoftware\n\n\nOther\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nPuns Only Programmers Will Get\n\n\n\n\n\n\nFun Stuff\n\n\n2023\n\n\n\n\n\n\n\n\n\nApr 20, 2023\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nAll Software used in Mr. Robot\n\n\n\n\n\n\nSoftware\n\n\n2023\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nTo 5 Operating Systems\n\n\n\n\n\n\nSoftware\n\n\n2023\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nHow The Decision Tree Classifiers Works\n\n\n\n\n\n\nData Analysis\n\n\nMachine Learning\n\n\n2022\n\n\n\n\n\n\n\n\n\nJun 25, 2022\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Demographic Data, Understanding and Utilizing It\n\n\n\n\n\n\nData Analysis\n\n\n\n\n\n\n\n\n\nJun 4, 2022\n\n\nJuma Shafara\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Imbalanced Datasets\n\n\n\n\n\n\nData Analysis\n\n\n2022\n\n\n\n\n\n\n\n\n\nMay 12, 2022\n\n\nJuma Shafara\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "posts/time_series_intro/index.html",
    "href": "posts/time_series_intro/index.html",
    "title": "What is Time Series",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nAny data recorded with some fixed interval of time is called as time series data. This fixed interval can be hourly, daily, monthly or yearly. e.g. hourly temperature reading, daily changing fuel prices, monthly electricity bill, annul company profit report etc. In time series data, time will always be independent variable and there can be one or many dependent variable.\nSales forecasting time series with shampoo sales for every month will look like this,\n\n\n\nShampoo_Sales\n\n\nIn above example since there is only one variable dependent on time so its called as univariate time series. If there are multiple dependent variables, then its called as multivariate time series.\nObjective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals.\n[ad]"
  },
  {
    "objectID": "posts/time_series_intro/index.html#what-is-time-series",
    "href": "posts/time_series_intro/index.html#what-is-time-series",
    "title": "What is Time Series",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nAny data recorded with some fixed interval of time is called as time series data. This fixed interval can be hourly, daily, monthly or yearly. e.g. hourly temperature reading, daily changing fuel prices, monthly electricity bill, annul company profit report etc. In time series data, time will always be independent variable and there can be one or many dependent variable.\nSales forecasting time series with shampoo sales for every month will look like this,\n\n\n\nShampoo_Sales\n\n\nIn above example since there is only one variable dependent on time so its called as univariate time series. If there are multiple dependent variables, then its called as multivariate time series.\nObjective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals.\n[ad]"
  },
  {
    "objectID": "posts/time_series_intro/index.html#time-series-characteristics",
    "href": "posts/time_series_intro/index.html#time-series-characteristics",
    "title": "What is Time Series",
    "section": "Time Series Characteristics",
    "text": "Time Series Characteristics\nMean, standard deviation and seasonality defines different characteristics of the time series.\n\n\n\nTime_Series_Characteristics\n\n\nImportant characteristics of the time series are as below\n\nTrend\nTrend represent the change in dependent variables with respect to time from start to end. In case of increasing trend dependent variable will increase with time and vice versa. It’s not necessary to have definite trend in time series, we can have a single time series with increasing and decreasing trend. In short trend represent the varying mean of time series data.\n\n\n\nTrend\n\n\n\n\nSeasonality\nIf observations repeats after fixed time interval then they are referred as seasonal observations. These seasonal changes in data can occur because of natural events or man-made events. For example every year warm cloths sales increases just before winter season. So seasonality represent the data variations at fixed intervals.\n\n\n\nSeasonality\n\n\n\n\nIrregularities\nThis is also called as noise. Strange dips and jump in the data are called as irregularities. These fluctuations are caused by uncontrollable events like earthquakes, wars, flood, pandemic etc. For example because of COVID-19 pandemic there is huge demand for hand sanitizers and masks.\n\n\n\nIrregularities\n\n\n\n\nCyclicity\nCyclicity occurs when observations in the series repeats in random pattern. Note that if there is any fixed pattern then it becomes seasonality, in case of cyclicity observations may repeat after a week, months or may be after a year. These kinds of patterns are much harder to predict.\n\n\n\nCyclicity\n\n\nTime series data which has above characteristics is called as ‘Non-Stationary Data’. For any analysis on time series data we must convert it to ‘Stationary Data’\nThe general guideline is to estimate the trend and seasonality in the time series, and then make the time series stationary for data modeling. In data modeling step statistical techniques are used for time series analysis and forecasting. Once we have the predictions, in the final step forecasted values converted into the original scale by applying trend and seasonality constraints back.\n[ad]\n\n\n\n\n\nPart 2 of this Time Series Analysis series will introduce you to doing time series analysis. The link to part 2 is here"
  },
  {
    "objectID": "posts/bffill_and_ffill/index.html",
    "href": "posts/bffill_and_ffill/index.html",
    "title": "Handling Missing Data in Pandas, When to Use bfill and ffill Methods",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThe bfill (backward fill) and ffill (forward fill) methods are used in data analysis and manipulation, particularly for handling missing data in pandas DataFrames or Series. Here’s a detailed explanation of when and how to use each method:\n\nffill (Forward Fill)\nDescription: ffill propagates the last valid observation forward to the next valid one.\nUse Cases:\n\nTime Series Data: When dealing with time series data, if a certain value is missing, it might be logical to assume that the last observed value remains in effect until a new value is observed. For example, if you have daily stock prices and some days are missing, you might want to fill those missing days with the last known stock price.\nData Smoothing: In some cases, for smoothing purposes, forward filling can help maintain a continuous data series without introducing significant bias.\nSurvey Data: In survey data, if a respondent skips a question but answers the following ones, you might assume their previous answer holds until they provide a new one.\n\nExample:\nimport pandas as pd\n\n# Sample data with missing values\ndata = pd.Series([1, 2, None, 4, None, None, 7])\nfilled_data = data.ffill()\nprint(filled_data)\nOutput:\n0    1.0\n1    2.0\n2    2.0\n3    4.0\n4    4.0\n5    4.0\n6    7.0\ndtype: float64\n\n\n\n\n\n\nbfill (Backward Fill)\nDescription: bfill propagates the next valid observation backward to fill the missing values.\nUse Cases:\n\nPredictive Models: In certain predictive models, you might want to assume the next known value affects the previous unknown period. This can be useful when the data naturally reflects future events impacting current states.\nData Preparation: In data preparation, backward filling can sometimes be used to prepare data for algorithms that require no missing values, ensuring that any gap is filled with the next available data point.\nInterim Reporting: When generating interim reports, you might use backward fill to assume that future values (like projected sales or stock levels) should be filled backward to reflect estimates.\n\nExample:\nimport pandas as pd\n\n# Sample data with missing values\ndata = pd.Series([1, 2, None, 4, None, None, 7])\nfilled_data = data.bfill()\nprint(filled_data)\nOutput:\n0    1.0\n1    2.0\n2    4.0\n3    4.0\n4    7.0\n5    7.0\n6    7.0\ndtype: float64\n\n\n\n\n\n\nChoosing Between ffill and bfill\n\nContext: Choose based on the context and the logical assumption that fits the nature of your data. If the past influences the present, use ffill. If the future influences the present, use bfill.\nData Patterns: Consider the patterns in your data and what makes sense for your specific analysis or model. Ensure that the method you choose maintains the integrity and meaning of your data.\n\nIn summary, use ffill to carry forward the last known value to fill missing data points and bfill to use the next known value to fill previous missing data points. Both methods are useful for maintaining data continuity and preparing datasets for analysis.\n\n\n\n\n\n\nYou may also like:\n\n\n\nHandling Missing Data\n\n \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/openai_suspends_her/index.html",
    "href": "posts/openai_suspends_her/index.html",
    "title": "OpenAI Suspends ChatGPT Voice Allegedly Mimicking Scarlett Johansson in “Her”",
    "section": "",
    "text": "IMDb\n\n\nOpenAI is suspending its ChatGPT voice, Sky, after users claimed it resembled Scarlett Johansson’s voice from the 2013 film Her.\nThe company released a statement on X (formerly Twitter) today, announcing that it is “working to pause the use of Sky” to address the issue. Many users noted the voice’s similarity to Johansson’s portrayal of an AI companion in the Spike Jonze-directed movie.\nSky is one of several ChatGPT voices available to users. OpenAI provided an explanation on its website about how these voices are selected and created.\n“We support the creative community and worked closely with the voice acting industry to ensure we took the right steps to cast ChatGPT’s voices. Each actor receives compensation above top-of-market rates, and this will continue for as long as their voices are used in our products.”\nThe statement further clarified, “We believe that AI voices should not deliberately mimic a celebrity’s distinctive voice—Sky’s voice is not an imitation of Scarlett Johansson but belongs to a different professional actress using her own natural speaking voice. To protect their privacy, we cannot share the names of our voice talents.”\n\n\n\n\n\nBackground on the Allegations\nThe controversy began last week when OpenAI revealed its new GPT-4o model during a livestream event, showcasing its ability to have realistic conversations about any topic. This functionality drew comparisons to Her, where Joaquin Phoenix’s character falls in love with an AI named Samantha, voiced by Johansson.\nAdding to the speculation, OpenAI CEO Sam Altman posted the word “her” on X after the event. In a subsequent post, he likened the new voice and video mode to scenes from sci-fi movies. In a September interview with The San Francisco Standard, Altman even cited Her as his favorite sci-fi film, praising its depiction of AI-human interactions as “incredibly prophetic.”\n\n\n\n\n\n\nMoving Forward\nIt remains unclear how OpenAI will resolve the concerns about Sky mimicking Johansson’s voice or prevent similar issues in the future. For more information on artificial intelligence developments, check out the experimental AI-made video game that failed.\nJuma Shafara contributor with DATAIDEA. Follow him on Twitter @juma_shafara.\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/sklearn_unsupervised_learning/index.html",
    "href": "posts/sklearn_unsupervised_learning/index.html",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "",
    "text": "Photo by DATAIDEA\nThe following topics are covered in this tutorial:\nLet’s install the required libraries."
  },
  {
    "objectID": "posts/sklearn_unsupervised_learning/index.html#introduction-to-unsupervised-learning",
    "href": "posts/sklearn_unsupervised_learning/index.html#introduction-to-unsupervised-learning",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "Introduction to Unsupervised Learning",
    "text": "Introduction to Unsupervised Learning\nUnsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels. Unsupervised learning is generally use to discover patterns in data and reduce high-dimensional data to fewer dimensions. Here’s how unsupervised learning fits into the landscape of machine learning algorithms(source):\n\nHere are the topics in machine learning that we’re studying in this course (source):\n\nHere’s a cheatsheet to help you decide which model to pick for a given problem. Can you identify the unsupervised learning algorithms?\n\nHere is a full list of unsupervised learning algorithms available in Scikit-learn: https://scikit-learn.org/stable/unsupervised_learning.html"
  },
  {
    "objectID": "posts/sklearn_unsupervised_learning/index.html#clustering",
    "href": "posts/sklearn_unsupervised_learning/index.html#clustering",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "Clustering",
    "text": "Clustering\nClustering is the process of grouping objects from a dataset such that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (Wikipedia). Scikit-learn offers several clustering algorithms. You can learn more about them here: https://scikit-learn.org/stable/modules/clustering.html\nHere is a visual representation of clustering:\n\nHere are some real-world applications of clustering:\n\nCustomer segmentation\nProduct recommendation\nFeature engineering\nAnomaly/fraud detection\nTaxonomy creation\n\nWe’ll use the Iris flower dataset to study some of the clustering algorithms available in scikit-learn. It contains various measurements for 150 flowers belonging to 3 different species.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nsns.set_style('darkgrid')\n%matplotlib inline\nLet’s load the popular iris and penguin datasets. These datasets are already built in seaborn\n# load the iris dataset\niris_df = sns.load_dataset('iris')\niris_df.head()\n\n\n\n\n\n\n\n\nsepal_length\n\n\nsepal_width\n\n\npetal_length\n\n\npetal_width\n\n\nspecies\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\nsetosa\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\nsetosa\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n\n# load the penguin dataset\nsns.get_dataset_names()\nping_df = sns.load_dataset('penguins')\nping_df.head()\n\n\n\n\n\n\n\n\nspecies\n\n\nisland\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nflipper_length_mm\n\n\nbody_mass_g\n\n\nsex\n\n\n\n\n\n\n0\n\n\nAdelie\n\n\nTorgersen\n\n\n39.1\n\n\n18.7\n\n\n181.0\n\n\n3750.0\n\n\nMale\n\n\n\n\n1\n\n\nAdelie\n\n\nTorgersen\n\n\n39.5\n\n\n17.4\n\n\n186.0\n\n\n3800.0\n\n\nFemale\n\n\n\n\n2\n\n\nAdelie\n\n\nTorgersen\n\n\n40.3\n\n\n18.0\n\n\n195.0\n\n\n3250.0\n\n\nFemale\n\n\n\n\n3\n\n\nAdelie\n\n\nTorgersen\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n4\n\n\nAdelie\n\n\nTorgersen\n\n\n36.7\n\n\n19.3\n\n\n193.0\n\n\n3450.0\n\n\nFemale\n\n\n\n\n\nsns.scatterplot(data=ping_df, x='bill_length_mm', y='bill_depth_mm', hue='species')\nplt.title('Penguin Bill Depth against Bill Length per Species')\nplt.ylabel('Bill Depth')\nplt.xlabel('Bill Length')\nplt.show()\n\n\n\npng\n\n\nsns.scatterplot(data=iris_df, x='sepal_length', y='petal_length', hue='species')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\npng\n\n\nWe’ll attempt to cluster observations using numeric columns in the data.\nnumeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nX = iris_df[numeric_cols]\nX.head()\n\n\n\n\n\n\n\n\nsepal_length\n\n\nsepal_width\n\n\npetal_length\n\n\npetal_width\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n\n\n\n\n\n\n\n\n\nK Means Clustering\nThe K-means algorithm attempts to classify objects into a pre-determined number of clusters by finding optimal central points (called centroids) for each cluster. Each object is classifed as belonging the cluster represented by the closest centroid.\n\nHere’s how the K-means algorithm works:\n\nPick K random objects as the initial cluster centers.\nClassify each object into the cluster whose center is closest to the point.\nFor each cluster of classified objects, compute the centroid (mean).\nNow reclassify each object using the centroids as cluster centers.\nCalculate the total variance of the clusters (this is the measure of goodness).\nRepeat steps 1 to 6 a few more times and pick the cluster centers with the lowest total variance.\n\nHere’s a video showing the above steps:\n\n \n\nLet’s apply K-means clustering to the Iris dataset.\nfrom sklearn.cluster import KMeans\nmodel = KMeans(n_clusters=3, random_state=42)\nmodel.fit(X)\n\n\n\nKMeans(n_clusters=3, random_state=42)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  KMeans?Documentation for KMeansiFitted\n\nKMeans(n_clusters=3, random_state=42)\n\n\n\n\n\nWe can check the cluster centers for each cluster.\nmodel.cluster_centers_\narray([[ 6.85384615e+00,  3.07692308e+00,  5.71538462e+00,\n         2.05384615e+00, -8.88178420e-16],\n       [ 5.00600000e+00,  3.42800000e+00,  1.46200000e+00,\n         2.46000000e-01,  1.00000000e+00],\n       [ 5.88360656e+00,  2.74098361e+00,  4.38852459e+00,\n         1.43442623e+00,  2.00000000e+00]])\nWe can now classify points using the model.\n# making predictions on X (clustering)\npreds = model.predict(X)\n# assign each row to their cluster\nX['clusters'] = preds\n# looking at some samples\nX.sample(n=5)\n\n\n\n\n\n\n\n\nsepal_length\n\n\nsepal_width\n\n\npetal_length\n\n\npetal_width\n\n\nclusters\n\n\n\n\n\n\n88\n\n\n5.6\n\n\n3.0\n\n\n4.1\n\n\n1.3\n\n\n2\n\n\n\n\n24\n\n\n4.8\n\n\n3.4\n\n\n1.9\n\n\n0.2\n\n\n1\n\n\n\n\n131\n\n\n7.9\n\n\n3.8\n\n\n6.4\n\n\n2.0\n\n\n0\n\n\n\n\n81\n\n\n5.5\n\n\n2.4\n\n\n3.7\n\n\n1.0\n\n\n2\n\n\n\n\n132\n\n\n6.4\n\n\n2.8\n\n\n5.6\n\n\n2.2\n\n\n0\n\n\n\n\n\nLet’s use seaborn and pyplot to visualize the clusters\nsns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds)\ncenters_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\nplt.plot(centers_x, centers_y, 'xb')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\npng\n\n\nAs you can see, K-means algorithm was able to classify (for the most part) different specifies of flowers into separate clusters. Note that we did not provide the “species” column as an input to KMeans.\nWe can check the “goodness” of the fit by looking at model.inertia_, which contains the sum of squared distances of samples to their closest cluster center. Lower the inertia, better the fit.\nmodel.inertia_\n78.8556658259773\n\n\n\n\n\nLet’s try creating 6 clusters.\nmodel = KMeans(n_clusters=6, random_state=42)\n# fitting the model\nmodel.fit(X)\n# making predictions on X (clustering)\npreds = model.predict(X)\n# assign each row to their cluster\nX['clusters'] = preds\n# looking at some samples\nX.sample(n=5)\n\n\n\n\n\n\n\n\nsepal_length\n\n\nsepal_width\n\n\npetal_length\n\n\npetal_width\n\n\nclusters\n\n\n\n\n\n\n13\n\n\n4.3\n\n\n3.0\n\n\n1.1\n\n\n0.1\n\n\n1\n\n\n\n\n20\n\n\n5.4\n\n\n3.4\n\n\n1.7\n\n\n0.2\n\n\n5\n\n\n\n\n127\n\n\n6.1\n\n\n3.0\n\n\n4.9\n\n\n1.8\n\n\n0\n\n\n\n\n40\n\n\n5.0\n\n\n3.5\n\n\n1.3\n\n\n0.3\n\n\n5\n\n\n\n\n70\n\n\n5.9\n\n\n3.2\n\n\n4.8\n\n\n1.8\n\n\n0\n\n\n\n\n\nLet’s visualize the clusters\nsns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds)\ncenters_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\nplt.plot(centers_x, centers_y, 'xb')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\npng\n\n\n# Let's calculate the new model inertia\nmodel.inertia_\n50.560990643274856\n\n\n\n\n\n\n\nSo, what number of clusters is good enough?\nIn most real-world scenarios, there’s no predetermined number of clusters. In such a case, you can create a plot of “No. of clusters” vs “Inertia” to pick the right number of clusters.\noptions = range(2, 11)\ninertias = []\n\nfor n_clusters in options:\n    model = KMeans(n_clusters, random_state=42).fit(X)\n    inertias.append(model.inertia_)\n\nplt.title(\"No. of clusters vs. Inertia\")\nplt.plot(options, inertias, '-o')\nplt.xlabel('No. of clusters (K)')\nplt.ylabel('Inertia')\nText(0, 0.5, 'Inertia')\n\n\n\npng\n\n\nThe chart is creates an “elbow” plot, and you can pick the number of clusters beyond which the reduction in inertia decreases sharply.\nMini Batch K Means: The K-means algorithm can be quite slow for really large dataset. Mini-batch K-means is an iterative alternative to K-means that works well for large datasets. Learn more about it here: https://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans\n\nEXERCISE: Perform clustering on the Mall customers dataset on Kaggle. Study the segments carefully and report your observations."
  },
  {
    "objectID": "posts/sklearn_unsupervised_learning/index.html#summary-and-references",
    "href": "posts/sklearn_unsupervised_learning/index.html#summary-and-references",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "Summary and References",
    "text": "Summary and References\n\nThe following topics were covered in this tutorial:\n\nOverview of unsupervised learning algorithms in Scikit-learn\nClustering algorithms: K Means, DBScan, Hierarchical clustering etc.\nDimensionality reduction (PCA) and manifold learning (t-SNE)\n\nCheck out these resources to learn more:\n\nhttps://www.coursera.org/learn/machine-learning\nhttps://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\nhttps://scikit-learn.org/stable/unsupervised_learning.html\nhttps://scikit-learn.org/stable/modules/clustering.html"
  },
  {
    "objectID": "posts/sklearn_unsupervised_learning/index.html#credit",
    "href": "posts/sklearn_unsupervised_learning/index.html#credit",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "Credit",
    "text": "Credit\n\nDo you seriously want to learn Programming and Data Analysis with Python?\nIf you’re serious about learning Programming, Data Analysis with Python and getting prepared for Data Science roles, I highly encourage you to enroll in my Programming for Data Science Course, which I’ve taught to hundreds of students. Don’t waste your time following disconnected, outdated tutorials\nMy Complete Programming for Data Science Course has everything you need in one place.\nThe course offers:\n\nDuration: Usually 3-4 months\nSessions: Four times a week (one on one)\nLocation: Online or/and at UMF House, Sir Apollo Kagwa Road\n\nWhat you’l learn:\n\nFundamentals of programming\nData manipulation and analysis\nVisualization techniques\nIntroduction to machine learning\nDatabase Management with SQL (optional)\nWeb Development with Django (optional)\n\nBest\nJuma Shafara\nData Scientist, Instructor\njumashafara0@gmail.com / dataideaorg@gmail.com\n+256701520768 / +256771754118"
  },
  {
    "objectID": "posts/ml_model_deployment/index.html",
    "href": "posts/ml_model_deployment/index.html",
    "title": "Exploring Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models is a crucial step in turning your data science projects into practical, real-world applications. With a variety of deployment options available, it’s essential to choose the one that best fits your needs. In this blog, we’ll explore the different deployment options and help you determine the best approach for your machine learning models.\n\n\nCloud platforms provide scalable and flexible environments for deploying machine learning models, making them a popular choice. Here’s a look at some of the leading cloud services:\nAmazon Web Services (AWS):\n\nSageMaker: A comprehensive service that offers tools to build, train, and deploy models at scale.\nLambda: Ideal for deploying lightweight models that need to respond to API requests.\nEC2 Instances: Customizable virtual servers suitable for deploying larger, more complex models.\n\nGoogle Cloud Platform (GCP):\n\nAI Platform: An integrated toolset designed for deploying and managing models efficiently.\nCloud Functions: A serverless environment perfect for lightweight model deployments.\nCompute Engine: Scalable virtual machines for more extensive and customized deployments.\n\nMicrosoft Azure:\n\nAzure Machine Learning: An end-to-end platform for building, training, and deploying models.\nAzure Functions: Serverless compute service ideal for deploying lightweight models.\nVirtual Machines: Provide a custom deployment environment tailored to your needs.\n\n\n\n\nFor organizations that require control over their infrastructure due to security, compliance, or performance needs, on-premises deployment is an excellent choice. Here are some options:\n\nDocker Containers: Ensures consistency across different environments by containerizing models.\nKubernetes: Manages and orchestrates containerized applications, offering scaling and management capabilities.\nDedicated Hardware: Utilizes specific hardware like GPUs or TPUs for high-performance requirements.\n\n\n\n\nEdge deployment is perfect for real-time processing and reduced latency by deploying models on edge devices like IoT devices and mobile phones. Some popular solutions include:\n\nTensorFlow Lite: Deploys TensorFlow models on mobile and embedded devices.\nONNX Runtime: Runs models trained in various frameworks on edge devices.\nNVIDIA Jetson: A platform for deploying AI models on edge devices with GPU acceleration.\n\n\n\n\nHybrid deployment combines multiple environments to leverage the advantages of each. For example, you might use the cloud for training and initial deployment but edge devices for inference to minimize latency and bandwidth usage.\n\n\n\nServerless computing allows you to deploy models without managing the underlying infrastructure. This option is ideal for applications with variable traffic and includes services like:\n\nAWS Lambda\nGoogle Cloud Functions\nAzure Functions\n\nThese services automatically scale with the load, simplifying the deployment process.\n\n\n\nExposing your model as a web service via RESTful or GraphQL APIs is a flexible and widely used approach. This allows various applications and services to access your model via HTTP requests. Some popular frameworks include:\n\nFlask/Django: Web application frameworks for Python.\nFastAPI: A modern, high-performance web framework for building APIs with Python 3.7+.\nTensorFlow Serving: Serves TensorFlow models as APIs, streamlining deployment.\n\n\n\n\nWhen real-time inference isn’t necessary, batch processing is a great option for handling large volumes of data at scheduled intervals. Key tools include:\n\nApache Spark: A unified analytics engine for large-scale data processing.\nHadoop: A framework for distributed storage and processing of big data."
  },
  {
    "objectID": "posts/ml_model_deployment/index.html#exploring-options-for-deploying-machine-learning-models",
    "href": "posts/ml_model_deployment/index.html#exploring-options-for-deploying-machine-learning-models",
    "title": "Exploring Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models is a crucial step in turning your data science projects into practical, real-world applications. With a variety of deployment options available, it’s essential to choose the one that best fits your needs. In this blog, we’ll explore the different deployment options and help you determine the best approach for your machine learning models.\n\n\nCloud platforms provide scalable and flexible environments for deploying machine learning models, making them a popular choice. Here’s a look at some of the leading cloud services:\nAmazon Web Services (AWS):\n\nSageMaker: A comprehensive service that offers tools to build, train, and deploy models at scale.\nLambda: Ideal for deploying lightweight models that need to respond to API requests.\nEC2 Instances: Customizable virtual servers suitable for deploying larger, more complex models.\n\nGoogle Cloud Platform (GCP):\n\nAI Platform: An integrated toolset designed for deploying and managing models efficiently.\nCloud Functions: A serverless environment perfect for lightweight model deployments.\nCompute Engine: Scalable virtual machines for more extensive and customized deployments.\n\nMicrosoft Azure:\n\nAzure Machine Learning: An end-to-end platform for building, training, and deploying models.\nAzure Functions: Serverless compute service ideal for deploying lightweight models.\nVirtual Machines: Provide a custom deployment environment tailored to your needs.\n\n\n\n\nFor organizations that require control over their infrastructure due to security, compliance, or performance needs, on-premises deployment is an excellent choice. Here are some options:\n\nDocker Containers: Ensures consistency across different environments by containerizing models.\nKubernetes: Manages and orchestrates containerized applications, offering scaling and management capabilities.\nDedicated Hardware: Utilizes specific hardware like GPUs or TPUs for high-performance requirements.\n\n\n\n\nEdge deployment is perfect for real-time processing and reduced latency by deploying models on edge devices like IoT devices and mobile phones. Some popular solutions include:\n\nTensorFlow Lite: Deploys TensorFlow models on mobile and embedded devices.\nONNX Runtime: Runs models trained in various frameworks on edge devices.\nNVIDIA Jetson: A platform for deploying AI models on edge devices with GPU acceleration.\n\n\n\n\nHybrid deployment combines multiple environments to leverage the advantages of each. For example, you might use the cloud for training and initial deployment but edge devices for inference to minimize latency and bandwidth usage.\n\n\n\nServerless computing allows you to deploy models without managing the underlying infrastructure. This option is ideal for applications with variable traffic and includes services like:\n\nAWS Lambda\nGoogle Cloud Functions\nAzure Functions\n\nThese services automatically scale with the load, simplifying the deployment process.\n\n\n\nExposing your model as a web service via RESTful or GraphQL APIs is a flexible and widely used approach. This allows various applications and services to access your model via HTTP requests. Some popular frameworks include:\n\nFlask/Django: Web application frameworks for Python.\nFastAPI: A modern, high-performance web framework for building APIs with Python 3.7+.\nTensorFlow Serving: Serves TensorFlow models as APIs, streamlining deployment.\n\n\n\n\nWhen real-time inference isn’t necessary, batch processing is a great option for handling large volumes of data at scheduled intervals. Key tools include:\n\nApache Spark: A unified analytics engine for large-scale data processing.\nHadoop: A framework for distributed storage and processing of big data."
  },
  {
    "objectID": "posts/ml_model_deployment/index.html#key-considerations-for-model-deployment",
    "href": "posts/ml_model_deployment/index.html#key-considerations-for-model-deployment",
    "title": "Exploring Options for Deploying Machine Learning Models",
    "section": "Key Considerations for Model Deployment",
    "text": "Key Considerations for Model Deployment\nWhen deciding on a deployment option, keep these factors in mind:\n\nScalability: Can the solution handle the expected load?\nLatency: Is real-time inference required?\nCost: What are the costs associated with running the model in production?\nSecurity: Does the deployment meet your security and compliance requirements?\nEase of Use: How straightforward is it to deploy, manage, and update the model?\nIntegration: How well does the deployment option integrate with your existing systems and workflows?\n\nSelecting the right deployment option is essential for balancing performance, cost, and operational complexity. By carefully considering your specific requirements and constraints, you can ensure a successful deployment of your machine learning models.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/budget_ml_deploy_options/index.html",
    "href": "posts/budget_ml_deploy_options/index.html",
    "title": "Budget-Friendly Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models doesn’t have to break the bank. If you’re working within a budget, there are several cost-effective options available that still provide robust capabilities. In this blog, we’ll explore some of the best budget-friendly deployment strategies for machine learning models.\n\n\nServerless computing is a great way to minimize costs because you only pay for what you use, without worrying about managing infrastructure. Here are some economical serverless options:\nAWS Lambda:\n\nCost: Pay-per-request pricing.\nPros: Automatically scales with demand, integrates well with other AWS services.\nCons: Limited to short-duration tasks and smaller memory limits.\n\nGoogle Cloud Functions:\n\nCost: Pay-per-use based on the number of invocations, compute time, and memory allocated.\nPros: Excellent for event-driven architectures, integrates smoothly with other Google Cloud services.\nCons: Similar constraints on execution time and memory as AWS Lambda.\n\nAzure Functions:\n\nCost: Pay-per-execution pricing model.\nPros: Scalable and integrates with a wide range of Azure services.\nCons: Limited by the maximum execution duration and resource allocation.\n\n\n\n\nContainers can be a cost-effective way to deploy models by ensuring efficient use of resources and consistent performance across different environments.\nDocker:\n\nCost: Free to use; additional costs come from the underlying infrastructure you deploy it on.\nPros: Lightweight, portable, and scalable. Works well with various cloud and on-premises environments.\nCons: Requires some setup and management expertise.\n\nKubernetes (K8s):\n\nCost: Free, but incurs costs related to the infrastructure on which it runs.\nPros: Manages containerized applications at scale, great for complex deployments.\nCons: More complex setup and management compared to Docker alone.\n\n\n\n\nPaaS solutions can reduce operational overhead and are generally cost-effective for small to medium-scale deployments.\nGoogle App Engine:\n\nCost: Offers a free tier and then pay-as-you-go pricing.\nPros: Fully managed, scalable service for web applications and APIs.\nCons: Less control over the underlying infrastructure.\n\nHeroku:\n\nCost: Free tier available, with affordable pricing plans for higher usage.\nPros: Easy to use, supports multiple programming languages, and integrates with various databases.\nCons: Might become expensive as you scale up.\n\n\n\n\nIf you need more control over your deployment environment but still want to keep costs low, consider budget-friendly cloud services:\nDigitalOcean:\n\nCost: Very competitive pricing with flexible plans starting as low as $5 per month.\nPros: Simple to set up, provides good performance for the price, supports Docker and Kubernetes.\nCons: Fewer advanced features compared to larger cloud providers.\n\nLinode:\n\nCost: Affordable plans starting at $5 per month.\nPros: Good performance, simple pricing structure, supports various deployment configurations.\nCons: Smaller ecosystem and fewer services compared to larger cloud providers.\n\nVultr:\n\nCost: Plans also starting at $5 per month.\nPros: Wide range of instances, including high-frequency compute instances.\nCons: Similar to DigitalOcean and Linode in terms of limitations.\n\n\n\n\nFor specific use cases where you need low latency and reduced data transfer costs, edge deployment can be both effective and economical:\nRaspberry Pi:\n\nCost: Low-cost hardware starting around $35.\nPros: Great for small-scale deployments, supports various machine learning frameworks.\nCons: Limited computational power, suitable for less intensive tasks.\n\nNVIDIA Jetson Nano:\n\nCost: Around $99.\nPros: Affordable, powerful enough for real-time AI tasks on edge devices.\nCons: Higher cost than Raspberry Pi but offers better performance."
  },
  {
    "objectID": "posts/budget_ml_deploy_options/index.html#budget-friendly-options-for-deploying-machine-learning-models",
    "href": "posts/budget_ml_deploy_options/index.html#budget-friendly-options-for-deploying-machine-learning-models",
    "title": "Budget-Friendly Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models doesn’t have to break the bank. If you’re working within a budget, there are several cost-effective options available that still provide robust capabilities. In this blog, we’ll explore some of the best budget-friendly deployment strategies for machine learning models.\n\n\nServerless computing is a great way to minimize costs because you only pay for what you use, without worrying about managing infrastructure. Here are some economical serverless options:\nAWS Lambda:\n\nCost: Pay-per-request pricing.\nPros: Automatically scales with demand, integrates well with other AWS services.\nCons: Limited to short-duration tasks and smaller memory limits.\n\nGoogle Cloud Functions:\n\nCost: Pay-per-use based on the number of invocations, compute time, and memory allocated.\nPros: Excellent for event-driven architectures, integrates smoothly with other Google Cloud services.\nCons: Similar constraints on execution time and memory as AWS Lambda.\n\nAzure Functions:\n\nCost: Pay-per-execution pricing model.\nPros: Scalable and integrates with a wide range of Azure services.\nCons: Limited by the maximum execution duration and resource allocation.\n\n\n\n\nContainers can be a cost-effective way to deploy models by ensuring efficient use of resources and consistent performance across different environments.\nDocker:\n\nCost: Free to use; additional costs come from the underlying infrastructure you deploy it on.\nPros: Lightweight, portable, and scalable. Works well with various cloud and on-premises environments.\nCons: Requires some setup and management expertise.\n\nKubernetes (K8s):\n\nCost: Free, but incurs costs related to the infrastructure on which it runs.\nPros: Manages containerized applications at scale, great for complex deployments.\nCons: More complex setup and management compared to Docker alone.\n\n\n\n\nPaaS solutions can reduce operational overhead and are generally cost-effective for small to medium-scale deployments.\nGoogle App Engine:\n\nCost: Offers a free tier and then pay-as-you-go pricing.\nPros: Fully managed, scalable service for web applications and APIs.\nCons: Less control over the underlying infrastructure.\n\nHeroku:\n\nCost: Free tier available, with affordable pricing plans for higher usage.\nPros: Easy to use, supports multiple programming languages, and integrates with various databases.\nCons: Might become expensive as you scale up.\n\n\n\n\nIf you need more control over your deployment environment but still want to keep costs low, consider budget-friendly cloud services:\nDigitalOcean:\n\nCost: Very competitive pricing with flexible plans starting as low as $5 per month.\nPros: Simple to set up, provides good performance for the price, supports Docker and Kubernetes.\nCons: Fewer advanced features compared to larger cloud providers.\n\nLinode:\n\nCost: Affordable plans starting at $5 per month.\nPros: Good performance, simple pricing structure, supports various deployment configurations.\nCons: Smaller ecosystem and fewer services compared to larger cloud providers.\n\nVultr:\n\nCost: Plans also starting at $5 per month.\nPros: Wide range of instances, including high-frequency compute instances.\nCons: Similar to DigitalOcean and Linode in terms of limitations.\n\n\n\n\nFor specific use cases where you need low latency and reduced data transfer costs, edge deployment can be both effective and economical:\nRaspberry Pi:\n\nCost: Low-cost hardware starting around $35.\nPros: Great for small-scale deployments, supports various machine learning frameworks.\nCons: Limited computational power, suitable for less intensive tasks.\n\nNVIDIA Jetson Nano:\n\nCost: Around $99.\nPros: Affordable, powerful enough for real-time AI tasks on edge devices.\nCons: Higher cost than Raspberry Pi but offers better performance."
  },
  {
    "objectID": "posts/budget_ml_deploy_options/index.html#key-tips-for-budget-friendly-deployment",
    "href": "posts/budget_ml_deploy_options/index.html#key-tips-for-budget-friendly-deployment",
    "title": "Budget-Friendly Options for Deploying Machine Learning Models",
    "section": "Key Tips for Budget-Friendly Deployment",
    "text": "Key Tips for Budget-Friendly Deployment\n\nOptimize Your Model: Smaller and optimized models can reduce computational requirements and, consequently, deployment costs.\nLeverage Free Tiers: Many cloud providers offer free tiers or credits for new users; take advantage of these offers.\nAuto-scaling: Use auto-scaling features to ensure you’re only paying for the resources you need at any given time.\nMonitor and Optimize: Regularly monitor resource usage and optimize configurations to avoid unnecessary costs.\nUse Spot Instances: For non-critical workloads, consider using spot instances, which are significantly cheaper than regular instances.\n\nBy carefully selecting your deployment strategy and optimizing your resources, you can deploy machine learning models effectively without exceeding your budget.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/creating_venvs/index.html",
    "href": "posts/creating_venvs/index.html",
    "title": "Python Virtual Environments",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nIn the vast ecosystem of Python programming, managing dependencies can sometimes be a daunting task. As projects grow in complexity, so does the need for a clean and isolated environment where dependencies can be installed without affecting other projects or the system-wide Python installation. This is where Python virtual environments come into play. In this guide, we’ll walk through the process of creating a virtual environment using venv and installing a package, like DataIdea, within it.\n\nWhat is a Python Virtual Environment?\nA virtual environment is a self-contained directory tree that contains a Python installation for a particular version of Python, plus a number of additional packages. It allows you to work on a Python project in isolation, ensuring that your project’s dependencies are maintained separately from other projects or the system Python.\n\n\nStep 1: Setting Up a Virtual Environment\nPython 3 comes with a built-in module called venv, which is used to create virtual environments. To create a virtual environment, open your terminal or command prompt and navigate to the directory where you want to create the environment. Then, run the following command:\n\nOn macOS and Linux:\n\npython3 -m venv myenv\n\nOn Windows:\n\npython -m venv myenv\nReplace myenv with the name you want to give to your virtual environment. This command will create a directory named myenv (or whatever name you provided) containing a Python interpreter and other necessary files.\n\n\n\n\n\n\n\nStep 2: Activating the Virtual Environment\nOnce the virtual environment is created, you need to activate it. Activation sets up the environment variables and modifies your shell prompt to indicate that you are now working within the virtual environment. Activate the virtual environment by running the appropriate command for your operating system:\n\nOn macOS and Linux:\n\nsource myenv/bin/activate\n\nOn Windows:\n\nmyenv\\Scripts\\activate\nYou’ll notice that your command prompt changes to show the name of the activated virtual environment.\n\n\n\n\n\n\n\nStep 3: Installing Packages\nWith the virtual environment activated, you can now install packages without affecting the global Python installation. Let’s install dataidea, as an example:\npip install dataidea\nyou can replace dataidea with another name of the package you want to install.\n\n\nStep 4: Using dataidea in Your Project\nOnce the package is installed, you can start using it in your Python project. Simply import it in your Python scripts as you would with any other package:\nimport dataidea\n\n\n\n\n\n\n\nStep 5: Deactivating the Virtual Environment\nWhen you’re done working on your project and want to leave the virtual environment, you can deactivate it by simply typing:\ndeactivate\n\n\nConclusion\nPython virtual environments are indispensable tools for managing dependencies and keeping project environments clean and isolated. With the venv module, creating and managing virtual environments has become easier than ever. By following the steps outlined in this guide, you can create a virtual environment, install packages like DataIdea, and develop Python projects with confidence. Happy coding!\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/data_imputation/index.html",
    "href": "posts/data_imputation/index.html",
    "title": "Handling Missing Data",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/data_imputation/index.html#handling-missing-data",
    "href": "posts/data_imputation/index.html#handling-missing-data",
    "title": "Handling Missing Data",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/data_imputation/index.html#introduction",
    "href": "posts/data_imputation/index.html#introduction",
    "title": "Handling Missing Data",
    "section": "Introduction:",
    "text": "Introduction:\nMissing data is a common hurdle in data analysis, impacting the reliability of insights drawn from datasets. Python offers a range of solutions to address this issue, some of which we discussed in the earlier weeks. In this notebook, we look into the top three missing data imputation methods in Python—SimpleImputer, KNNImputer, and IterativeImputer from scikit-learn—providing insights into their functionalities and practical considerations. We’ll explore these essential techniques, using the weather dataset.\n# install the libraries for this demonstration\n! pip install dataidea==0.2.5\nfrom dataidea.packages import *\nfrom dataidea.datasets import loadDataset\nfrom dataidea.packages import * imports for us np, pd, plt, etc. loadDataset allows us to load datasets inbuilt in the dataidea library\nweather = loadDataset('weather')\nweather\n\n\n\n\n\n\n\n\nday\n\n\ntemperature\n\n\nwindspead\n\n\nevent\n\n\n\n\n\n\n0\n\n\n01/01/2017\n\n\n32.0\n\n\n6.0\n\n\nRain\n\n\n\n\n1\n\n\n04/01/2017\n\n\nNaN\n\n\n9.0\n\n\nSunny\n\n\n\n\n2\n\n\n05/01/2017\n\n\n28.0\n\n\nNaN\n\n\nSnow\n\n\n\n\n3\n\n\n06/01/2017\n\n\nNaN\n\n\n7.0\n\n\nNaN\n\n\n\n\n4\n\n\n07/01/2017\n\n\n32.0\n\n\nNaN\n\n\nRain\n\n\n\n\n5\n\n\n08/01/2017\n\n\nNaN\n\n\nNaN\n\n\nSunny\n\n\n\n\n6\n\n\n09/01/2017\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n7\n\n\n10/01/2017\n\n\n34.0\n\n\n8.0\n\n\nCloudy\n\n\n\n\n8\n\n\n11/01/2017\n\n\n40.0\n\n\n12.0\n\n\nSunny\n\n\n\n\n\nweather.isna().sum()\nday            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64\nLet’s demonstrate how to use the top three missing data imputation methods—SimpleImputer, KNNImputer, and IterativeImputer—using the simple weather dataset.\n# select age from the data\ntemp_wind = weather[['temperature', 'windspead']].copy()\ntemp_wind_imputed = temp_wind.copy()"
  },
  {
    "objectID": "posts/data_imputation/index.html#simpleimputer-from-scikit-learn",
    "href": "posts/data_imputation/index.html#simpleimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "SimpleImputer from scikit-learn:",
    "text": "SimpleImputer from scikit-learn:\n\nUsage: SimpleImputer is a straightforward method for imputing missing values by replacing them with a constant, mean, median, or most frequent value along each column.\nPros:\n\nEasy to use and understand.\nCan handle both numerical and categorical data.\nOffers flexibility with different imputation strategies.\n\nCons:\n\nIt doesn’t consider relationships between features.\nMay not be the best choice for datasets with complex patterns of missingness.\n\nExample:\n\nfrom sklearn.impute import SimpleImputer\n\nsimple_imputer = SimpleImputer(strategy='mean')\ntemp_wind_simple_imputed = simple_imputer.fit_transform(temp_wind)\n\ntemp_wind_simple_imputed_df = pd.DataFrame(temp_wind_simple_imputed, columns=temp_wind.columns)\nLet’s have a look at the outcome\ntemp_wind_simple_imputed_df\n\n\n\n\n\n\n\n\ntemperature\n\n\nwindspead\n\n\n\n\n\n\n0\n\n\n32.0\n\n\n6.0\n\n\n\n\n1\n\n\n33.2\n\n\n9.0\n\n\n\n\n2\n\n\n28.0\n\n\n8.4\n\n\n\n\n3\n\n\n33.2\n\n\n7.0\n\n\n\n\n4\n\n\n32.0\n\n\n8.4\n\n\n\n\n5\n\n\n33.2\n\n\n8.4\n\n\n\n\n6\n\n\n33.2\n\n\n8.4\n\n\n\n\n7\n\n\n34.0\n\n\n8.0\n\n\n\n\n8\n\n\n40.0\n\n\n12.0"
  },
  {
    "objectID": "posts/data_imputation/index.html#knnimputer-from-scikit-learn",
    "href": "posts/data_imputation/index.html#knnimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "KNNImputer from scikit-learn:",
    "text": "KNNImputer from scikit-learn:\n\nUsage: KNNImputer imputes missing values using k-nearest neighbors, replacing them with the mean value of the nearest neighbors.\nPros:\n\nConsiders relationships between features, making it suitable for datasets with complex patterns of missingness.\nCan handle both numerical and categorical data.\n\nCons:\n\nComputationally expensive for large datasets.\nRequires careful selection of the number of neighbors (k).\n\nExample:\n\nfrom sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\ntemp_wind_knn_imputed = knn_imputer.fit_transform(temp_wind)\n\ntemp_wind_knn_imputed_df = pd.DataFrame(temp_wind_knn_imputed, columns=temp_wind.columns)\nIf we take a look at the outcome\ntemp_wind_knn_imputed_df\n\n\n\n\n\n\n\n\ntemperature\n\n\nwindspead\n\n\n\n\n\n\n0\n\n\n32.0\n\n\n6.0\n\n\n\n\n1\n\n\n33.0\n\n\n9.0\n\n\n\n\n2\n\n\n28.0\n\n\n7.0\n\n\n\n\n3\n\n\n33.0\n\n\n7.0\n\n\n\n\n4\n\n\n32.0\n\n\n7.0\n\n\n\n\n5\n\n\n33.2\n\n\n8.4\n\n\n\n\n6\n\n\n33.2\n\n\n8.4\n\n\n\n\n7\n\n\n34.0\n\n\n8.0\n\n\n\n\n8\n\n\n40.0\n\n\n12.0"
  },
  {
    "objectID": "posts/data_imputation/index.html#iterativeimputer-from-scikit-learn",
    "href": "posts/data_imputation/index.html#iterativeimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "IterativeImputer from scikit-learn:",
    "text": "IterativeImputer from scikit-learn:\n\nUsage: IterativeImputer models each feature with missing values as a function of other features and uses that estimate for imputation. It iteratively estimates the missing values.\nPros:\n\nTakes into account relationships between features, making it suitable for datasets with complex missing patterns.\nMore robust than SimpleImputer for handling missing data.\n\nCons:\n\nCan be computationally intensive and slower than SimpleImputer.\nRequires careful tuning of model parameters.\n\nExample:\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\niterative_imputer = IterativeImputer()\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\nLet’s take a look at the outcome\ntemp_wind_iterative_imputed_df\n\n\n\n\n\n\n\n\ntemperature\n\n\nwindspead\n\n\n\n\n\n\n0\n\n\n32.000000\n\n\n6.000000\n\n\n\n\n1\n\n\n35.773287\n\n\n9.000000\n\n\n\n\n2\n\n\n28.000000\n\n\n3.321648\n\n\n\n\n3\n\n\n33.042537\n\n\n7.000000\n\n\n\n\n4\n\n\n32.000000\n\n\n6.238915\n\n\n\n\n5\n\n\n33.545118\n\n\n7.365795\n\n\n\n\n6\n\n\n33.545118\n\n\n7.365795\n\n\n\n\n7\n\n\n34.000000\n\n\n8.000000\n\n\n\n\n8\n\n\n40.000000\n\n\n12.000000"
  },
  {
    "objectID": "posts/data_imputation/index.html#datawig",
    "href": "posts/data_imputation/index.html#datawig",
    "title": "Handling Missing Data",
    "section": "Datawig:",
    "text": "Datawig:\nDatawig is a library specifically designed for imputing missing values in tabular data using deep learning models.\n# import datawig\n\n# # Impute missing values\n# df_imputed = datawig.SimpleImputer.complete(weather)\nThese top imputation methods offer different trade-offs in terms of computational complexity, handling of missing data patterns, and ease of use. The choice between them depends on the specific characteristics of the dataset and the requirements of the analysis."
  },
  {
    "objectID": "posts/data_imputation/index.html#homework",
    "href": "posts/data_imputation/index.html#homework",
    "title": "Handling Missing Data",
    "section": "Homework",
    "text": "Homework\n\nTry out these techniques for categorical data"
  },
  {
    "objectID": "posts/data_imputation/index.html#credit",
    "href": "posts/data_imputation/index.html#credit",
    "title": "Handling Missing Data",
    "section": "Credit",
    "text": "Credit\n\nDo you seriously want to learn Programming and Data Analysis with Python?\nIf you’re serious about learning Programming, Data Analysis with Python and getting prepared for Data Science roles, I highly encourage you to enroll in my Programming for Data Science Course, which I’ve taught to hundreds of students. Don’t waste your time following disconnected, outdated tutorials\nMy Complete Programming for Data Science Course has everything you need in one place.\nThe course offers:\n\nDuration: Usually 3-4 months\nSessions: Four times a week (one on one)\nLocation: Online or/and at UMF House, Sir Apollo Kagwa Road\n\nWhat you’l learn:\n\nFundamentals of programming\nData manipulation and analysis\nVisualization techniques\nIntroduction to machine learning\nDatabase Management with SQL (optional)\nWeb Development with Django (optional)\n\nBest\nJuma Shafara\nData Scientist, Instructor\njumashafara0@gmail.com / dataideaorg@gmail.com\n+256701520768 / +256771754118\n\n\n\n\nYou may also like:\n\n\n\nHandling Missing Data in Pandas, When to Use bfill and ffill Methods"
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html",
    "href": "posts/mr_robot_softwares/index.html",
    "title": "All Software used in Mr. Robot",
    "section": "",
    "text": "Thumbnail\nHere’s a list of the most popular softwares used by the hackers in the Emmy and Golden Globe award winning drama/thriller series Mr. Robot."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#kali-linux",
    "href": "posts/mr_robot_softwares/index.html#kali-linux",
    "title": "All Software used in Mr. Robot",
    "section": "Kali Linux",
    "text": "Kali Linux\nKali Linux is a Linux distro made for security researchers for penetration testing, but is also used by hackers since it is jam packed with hacking tools. It is regularly featured in Mr. Robot since it is the hackers’ operating system of choice."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#tor-browser",
    "href": "posts/mr_robot_softwares/index.html#tor-browser",
    "title": "All Software used in Mr. Robot",
    "section": "Tor Browser",
    "text": "Tor Browser\nTor Browser is widely considered to be the best anonymizing tool out there. It will make your Internet activity very hard to trace, which fsociety takes advantage of when Trenton in season 2, episode 8 uploads a leaked FBI conference call about illegal mass surveillance to Vimeo using Tor Browser."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#raspberry-pi",
    "href": "posts/mr_robot_softwares/index.html#raspberry-pi",
    "title": "All Software used in Mr. Robot",
    "section": "Raspberry Pi",
    "text": "Raspberry Pi\nRaspberry Pi is a small, programmable computer board designed to teach children about computer science. It is also favourite among hobbyists and programmers due to its low-cost, versatility and simplicity. In season 1, episode 5 Elliot installs a Raspberry Pi into Steel Mountain’s climate control system so that fsociety at a later point in time can remotely raise the temperature in the storage room where Evil Corp’s tape backups are stored, resulting in the backups of the records of a significant portion of the US’ consumer debt being destroyed."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#proton-mail",
    "href": "posts/mr_robot_softwares/index.html#proton-mail",
    "title": "All Software used in Mr. Robot",
    "section": "Proton Mail",
    "text": "Proton Mail\nProtonMail is a secure, end-to-end encrypted e-mail service based in Switzerland that is used by Elliot in season 1, episode 8. The team behind Mr. Robot researched secure e-mail services to the extent that they actually contacted the ProtonMail developers and asked if it was possible for users to monitor their own e-mail activity in ProtonMail. The ProtonMail developers liked the idea of account access logs so much that they ended up implementing it in their v2.0 release of ProtonMail."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#pycharm",
    "href": "posts/mr_robot_softwares/index.html#pycharm",
    "title": "All Software used in Mr. Robot",
    "section": "PyCharm",
    "text": "PyCharm\nPyCharm is a Python and Django IDE (Integrated Developer Environment), which is a type of code editor software. It is used by Trenton in season 1, episode 4."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#bluesniff",
    "href": "posts/mr_robot_softwares/index.html#bluesniff",
    "title": "All Software used in Mr. Robot",
    "section": "Bluesniff",
    "text": "Bluesniff\nBluesniff is a Bluetooth device discovery tool. In season 1, episode 6 Elliot uses Bluesniff in combination with btscanner and Metasploit when he connects to the computer in a nearby police car via a MultiBlue Bluetooth USB Dongle to compromise a prison’s network in order to break a drug dealer called Vera out of prison."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#btscanner",
    "href": "posts/mr_robot_softwares/index.html#btscanner",
    "title": "All Software used in Mr. Robot",
    "section": "btscanner",
    "text": "btscanner\nbtscanner is a tool that is included in Kali Linux that extracts as much information as possible about Bluetooth devices without having to pair. In season 1, episode 6 Elliot uses btscanner in combination with Bluesniff and Metasploit when he connects to the computer in a nearby police car using a MultiBlue Bluetooth USB Dongle to compromise a prison’s network in order to break a drug dealer called Vera out of prison."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#mozilla-firefox",
    "href": "posts/mr_robot_softwares/index.html#mozilla-firefox",
    "title": "All Software used in Mr. Robot",
    "section": "Mozilla Firefox",
    "text": "Mozilla Firefox\nElliot uses Firefox as his default web browser. Trenton uses Firefox in season 2, episode 8."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#vlc-media-player",
    "href": "posts/mr_robot_softwares/index.html#vlc-media-player",
    "title": "All Software used in Mr. Robot",
    "section": "VLC Media Player",
    "text": "VLC Media Player\nVLC Media Player was used in season 2, episode 4 when Elliot and Darlene watched a VHS rip of the fake horror movie Careful Massacre of the Bourgeoisie together. VLC is also used in season 2, episode 8 when fsociety preview the video they are about to upload a leaked FBI conference call about illegal mass surveillance."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#µtorrent",
    "href": "posts/mr_robot_softwares/index.html#µtorrent",
    "title": "All Software used in Mr. Robot",
    "section": "µTorrent",
    "text": "µTorrent\nIn season 2, episode 4 Darlene was downloading a VHS rip of the fake horror movie Careful Massacre of the Bourgeoisie using µTorrent."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#flexispy",
    "href": "posts/mr_robot_softwares/index.html#flexispy",
    "title": "All Software used in Mr. Robot",
    "section": "FlexiSPY",
    "text": "FlexiSPY\nFlexiSPY is a spyware software for Android, iOS and BlackBerry that lets the user monitor all activities on the victims phone. In season 1, episode 3 Tyrell Wellick covertly installs it on a co-worker’s Android phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#kingoroot",
    "href": "posts/mr_robot_softwares/index.html#kingoroot",
    "title": "All Software used in Mr. Robot",
    "section": "KingoRoot",
    "text": "KingoRoot\nKingo Root is used by Tyrell Wellick in season 1, episode 3 to root a co-worker’s Android phone so that he can covertly install the FlexiSPY spyware on the phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#kvm-kernel-based-virtual-machine",
    "href": "posts/mr_robot_softwares/index.html#kvm-kernel-based-virtual-machine",
    "title": "All Software used in Mr. Robot",
    "section": "KVM (Kernel-based Virtual Machine)",
    "text": "KVM (Kernel-based Virtual Machine)\nKVM is a hypervisor, which is a software that can run other operating systems via virtual machines. Elliot uses KVM to virtualize Windows 7 inside of Kali Linux. In season 1, episode 6 Elliot uses KVM to run Metasploit and Metapreter and in season 1, episode 8 he uses KVM to run DeepSound."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#metasploit",
    "href": "posts/mr_robot_softwares/index.html#metasploit",
    "title": "All Software used in Mr. Robot",
    "section": "Metasploit",
    "text": "Metasploit\nMetasploit Framework is a software included in Kali Linux that makes it easier to discover vulnerabilities in networks for penetration testers. Meterpreter is one of several hundreds of payloads that can be run in the Metasploit Framework and it is used in season 1, episode 6. In season 1, episode 6 Elliot uses Metasploit Framwork and Metapreter in combination with btscanner and Bluesniff when he connects to the computer in a nearby police car via a MultiBlue Bluetooth USB Dongle to compromise a prison’s network in order to break a drug dealer called Vera out of prison."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#framaroot",
    "href": "posts/mr_robot_softwares/index.html#framaroot",
    "title": "All Software used in Mr. Robot",
    "section": "Framaroot",
    "text": "Framaroot\nFramaroot - called RooterFrame in the show - is used by Tyrell Wellick in season 1, episode 3 to root a co-worker’s Android phone so that he can covertly install the FlexiSPY spyware on the phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/mr_robot_softwares/index.html#supersu",
    "href": "posts/mr_robot_softwares/index.html#supersu",
    "title": "All Software used in Mr. Robot",
    "section": "SuperSU",
    "text": "SuperSU\nSuperSU is an app that managed superuser privileges on rooted Android phones. In season 1, episode 3 Tyrell Wellick covertly installs FlexiSPY - which uses SuperSU to give itself superuser access - on a co-worker’s Android phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/rethinking_chatgpt/index.html#introduction",
    "href": "posts/rethinking_chatgpt/index.html#introduction",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "1 Introduction",
    "text": "1 Introduction\nArtificial Intelligence (AI) has revolutionized the way we interact with technology, making many tasks more efficient and accessible. Among the AI applications, chatbots and virtual assistants have gained significant popularity, with ChatGPT being one of the leading models developed by OpenAI. While ChatGPT and similar AI tools offer numerous benefits, there are also several reasons to be cautious about their use. This blog aims to explore in detail why you might reconsider relying on ChatGPT, touching on aspects such as privacy concerns, ethical considerations, potential biases, reliability, and the broader societal implications of AI."
  },
  {
    "objectID": "posts/rethinking_chatgpt/index.html#privacy-concerns",
    "href": "posts/rethinking_chatgpt/index.html#privacy-concerns",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "2 Privacy Concerns",
    "text": "2 Privacy Concerns\n\n2.1 Data Collection and Storage\nOne of the primary concerns with using ChatGPT is related to data privacy. When you interact with ChatGPT, your conversations are often recorded and stored. Although companies like OpenAI implement stringent data security measures, there is always a risk of data breaches. The data collected from users can potentially be accessed by unauthorized individuals, leading to privacy violations.\n\n\n2.2 Misuse of Personal Information\nThe personal information you share with ChatGPT could be misused. While AI developers claim that user data is anonymized and used solely to improve AI models, there is always a risk that this data could be sold to third parties or used for targeted advertising. This not only undermines your privacy but also raises ethical questions about the exploitation of user data.\n\n\n2.3 Lack of Transparency\nMany users are unaware of how their data is being used when they interact with AI models like ChatGPT. The lack of transparency in data handling practices can lead to mistrust. Users should be informed about what data is being collected, how it is stored, and who has access to it. Without this transparency, it is difficult to trust that your data is being handled responsibly."
  },
  {
    "objectID": "posts/rethinking_chatgpt/index.html#ethical-considerations",
    "href": "posts/rethinking_chatgpt/index.html#ethical-considerations",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "3 Ethical Considerations",
    "text": "3 Ethical Considerations\n\n3.1 Manipulation and Misinformation\nAI models like ChatGPT can be used to manipulate opinions and spread misinformation. The ability of AI to generate human-like text makes it a powerful tool for creating fake news, misleading articles, and deceptive social media posts. This can have serious consequences, including influencing elections, spreading conspiracy theories, and undermining public trust in information sources.\n\n\n3.2 Dehumanization of Interaction\nRelying on AI for communication can lead to the dehumanization of interactions. While ChatGPT can simulate human conversation, it lacks the emotional intelligence and empathy that come from genuine human interaction. This can result in a diminished quality of communication, where the nuances of human emotions and connections are lost.\n\n\n3.3 Ethical Use in Sensitive Areas\nThe use of AI in sensitive areas such as mental health support, legal advice, and medical consultations raises ethical concerns. AI models may provide inaccurate or inappropriate advice, potentially causing harm to users. The reliance on AI in these critical areas should be approached with caution, ensuring that human oversight is always present to verify and validate the information provided by AI."
  },
  {
    "objectID": "posts/rethinking_chatgpt/index.html#potential-biases",
    "href": "posts/rethinking_chatgpt/index.html#potential-biases",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "4 Potential Biases",
    "text": "4 Potential Biases\n\n4.1 Inherent Biases in Training Data\nAI models like ChatGPT are trained on vast amounts of data from the internet, which inevitably includes biased and prejudiced content. These biases can be reflected in the responses generated by the AI, perpetuating stereotypes and discriminatory attitudes. For instance, gender, racial, and cultural biases can be inadvertently reinforced through AI-generated text.\n\n\n4.2 Impact on Marginalized Communities\nThe biases present in AI models can disproportionately impact marginalized communities. AI-generated content that reflects societal biases can contribute to the marginalization and discrimination of these groups. It is essential to recognize and address these biases to ensure that AI technologies do not perpetuate inequality and injustice.\n\n\n4.3 Difficulty in Mitigating Biases\nWhile efforts are being made to reduce biases in AI models, it is a challenging task. Biases are deeply ingrained in the data used to train these models, and completely eliminating them is nearly impossible. Continuous monitoring and updating of AI models are required to mitigate these biases, but the effectiveness of these measures is still a subject of ongoing research and debate."
  },
  {
    "objectID": "posts/rethinking_chatgpt/index.html#reliability-and-accuracy",
    "href": "posts/rethinking_chatgpt/index.html#reliability-and-accuracy",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "5 Reliability and Accuracy",
    "text": "5 Reliability and Accuracy\n\n5.1 Hallucinations and Errors\nAI models like ChatGPT are prone to generating incorrect or nonsensical information, often referred to as “hallucinations.” These errors can be misleading and potentially harmful, especially when users rely on the AI for accurate information. The inability to distinguish between correct and incorrect responses can undermine the reliability of AI-generated content.\n\n\n5.2 Lack of Accountability\nWhen using AI models, it can be challenging to determine who is accountable for the information provided. Unlike human professionals who can be held responsible for their advice and actions, AI lacks accountability. This lack of accountability can lead to a scenario where misinformation is spread without any consequences for those responsible.\n\n\n5.3 Dependence on AI\nOver-reliance on AI for information and decision-making can reduce critical thinking skills and the ability to independently verify information. Users may become dependent on AI-generated responses, leading to a decline in their analytical abilities and judgment. It is crucial to maintain a balance between using AI as a tool and exercising independent thought."
  },
  {
    "objectID": "posts/rethinking_chatgpt/index.html#broader-societal-implications",
    "href": "posts/rethinking_chatgpt/index.html#broader-societal-implications",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "6 Broader Societal Implications",
    "text": "6 Broader Societal Implications\n\n6.1 Job Displacement\nThe increasing use of AI technologies, including ChatGPT, has the potential to displace jobs. As AI becomes more capable of performing tasks traditionally done by humans, there is a risk of job loss in various sectors. This can lead to economic instability and exacerbate issues related to unemployment and income inequality.\n\n\n6.2 Ethical Dilemmas in AI Development\nThe development and deployment of AI technologies pose significant ethical dilemmas. Decisions about what data to use, how to handle biases, and how to ensure the responsible use of AI are complex and multifaceted. The ethical challenges associated with AI development require careful consideration and the involvement of diverse stakeholders to navigate effectively.\n\n\n6.3 Impact on Human Creativity and Innovation\nThe reliance on AI for creative tasks, such as writing, music composition, and art, raises questions about the impact on human creativity and innovation. While AI can assist in these areas, there is a concern that it may stifle human creativity by providing easy solutions and reducing the incentive to develop original ideas. It is important to find a balance where AI complements human creativity rather than replacing it.\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe."
  },
  {
    "objectID": "posts/rethinking_chatgpt/index.html#conclusion",
    "href": "posts/rethinking_chatgpt/index.html#conclusion",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nWhile ChatGPT and similar AI models offer numerous benefits, it is essential to be aware of the potential drawbacks and risks associated with their use. Privacy concerns, ethical considerations, potential biases, reliability issues, and broader societal implications are all important factors to consider. As we continue to integrate AI into our lives, it is crucial to approach its use with caution, ensuring that we prioritize ethical practices, transparency, and human oversight to mitigate the risks and maximize the benefits."
  },
  {
    "objectID": "posts/fpl_standings/index.html",
    "href": "posts/fpl_standings/index.html",
    "title": "Update on DATAIDEA Fantasy Football League Standings",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nHey Fantasy Football aficionados,\nIt’s time for another exciting update on the DATAIDEA Fantasy Football League! With each matchweek passing by, the competition has been heating up, and the leaderboard has seen its fair share of twists and turns. Let’s dive straight into the latest standings and highlights from the league.\n\nCurrent Standings:\nHere’s how the teams stack up on the leaderboard:\n\n\n\nRank\nTeam & Manager\nGW\nTOT\n\n\n\n\n1\nBARÇA ya KAHUNGYE\n47\n2089\n\n\n2\nAŁbramo FC\n61\n2053\n\n\n3\nLethal Weapon\n53\n2009\n\n\n4\nBlyckfc\n30\n2002\n\n\n5\nBrine FC\n17\n1959\n\n\n6\nJ T\n78\n1958\n\n\n7\nJumaShafara@DATAIDEA\n43\n1951\n\n\n8\nRwenzori\n59\n1949\n\n\n9\nGem FC\n59\n1911\n\n\n10\nGULU UNITED F.C\n38\n1908\n\n\n11\nAJ12\n45\n1887\n\n\n12\nLegends FC\n44\n1823\n\n\n13\nM142 HIMARS\n56\n1804\n\n\n14\nColaz guluz\n42\n1781\n\n\n15\nPaperChaser of Uganda\n34\n1732\n\n\n16\nAFC Adventurer\n39\n1669\n\n\n\n\n\nHighlights:\n\nBARCA ya KAHUNGYE continues to dominate the league, maintaining their top position with an impressive total score of 2042 points.\nAlBramo FC is trailing closely behind in second place, displaying consistent performance throughout the season.\nJumaShafara@DATAIDEA has climbed up the ranks to secure the fifth position, showcasing the competitive spirit of DATAIDEA’s own fantasy football enthusiasts.\n\n\n\nLooking Ahead:\nAs the league progresses, the competition is only expected to intensify further. With managers strategizing and making crucial transfers, every matchweek brings forth new challenges and opportunities for teams to climb the ranks.\nStay tuned for more updates as the DATAIDEA Fantasy Football League unfolds its thrilling saga of goals, assists, and tactical maneuvers.\nUntil next time, may your fantasy team flourish on the virtual pitch!\nBest regards,\nJuma Shafara\nInstructor, DATAIDEA\n+256771754118 / jumashafara0@gmail.com\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/programmer_puns/index.html",
    "href": "posts/programmer_puns/index.html",
    "title": "Puns Only Programmers Will Get",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nWhat are the best puns? Definitely the ones only a specific group of people will understand and enjoy. Especially in the era we’re living in, we need to feel like a family and share “inside” jokes with one another. So, let’s take a look at some of the best programming puns we came upon during the pre-summer times.\n\nWhy do programmers like dark mode?\n\nBecause light attracts bugs.\n\nI used to work as a programmer for autocorrect…\n\nThen they fried me for no raisin.\n\nWhat do NASA programmers do on the weekends?\n\nThey hit the space bar.\n\nWhy did the programmer quit his job?\n\nBecause he didn’t get arrays.\n\nWhat was the SNES programmers’ favorite drink?\n\nSprite\n\nWhat do programmers do when they’re hungry?\n\nThey grab a byte.\n\nWhy couldn’t the programmer dance to the song?\n\nBecause he didn’t get the… algo-rhythm…\n\nWhat you call it when computer programmers make fun of each other?\n\nCyber boolean…\n\nI am now a successful programmer…\n\nBut back in the days I was a noobgrammer.\n\nWhy do programmers always mix up Halloween and Christmas?\n\nBecause Oct 31 equals Dec 25.\n\nWhy did the programmer get a huge telephone bill?\n\nBecause his program was CALLING a lot of subroutines.\n\nWhat do Spanish programmers code in?\n\nSí ++\n\nWhich way did the programmer go?\n\nHe went data way!\n\nWhy was the programmer always running into walls?\n\nHe couldn’t see sharp.\n\nHow programmers curse?\n\nOh shift!\n\nWhy do programmers make good politicians?\n\nTheir goto is to switch statements.\n\nHow did the programmer lose weight?\n\nHe switched to a byte-sized diet.\n\nI almost bought a huge library out of old computer programming books…\n\nBut the ascii price was way too high.\n\nWhat’s a Jedi’s favorite programming language?\n\nJabbaScript…\n\n\n\nWant to learn programming and become an expert?\nIf you’re serious about learning Programming and getting a job as a Developer, I highly encourage you to enroll in my programming courses for Python, JavaScript, Data Science and Web Development.\nDon’t waste your time following disconnected, outdated tutorials. I can teach you all that you need to kickstart your career.\nContact me at +256771754118 or jumashafara@proton.me\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/top_5_operating_systems/index.html",
    "href": "posts/top_5_operating_systems/index.html",
    "title": "To 5 Operating Systems",
    "section": "",
    "text": "Thumbnail\nOperating systems are the backbone of modern computing, providing the fundamental software foundation for computers, smartphones, servers, and more. Here’s a comprehensive overview of five prominent operating systems that have made significant impacts in various domains."
  },
  {
    "objectID": "posts/top_5_operating_systems/index.html#windows",
    "href": "posts/top_5_operating_systems/index.html#windows",
    "title": "To 5 Operating Systems",
    "section": "Windows",
    "text": "Windows\n\nOverview\nDeveloped by Microsoft, Windows is one of the most widely used operating systems across PCs, servers, and embedded systems. Known for its user-friendly interface and extensive software compatibility, Windows offers a variety of versions tailored for different devices and use cases.\n\n\nFeatures\n\nGUI: Graphical User Interface for intuitive navigation.\nCompatibility: Broad support for software and hardware.\nSecurity: Regular updates and security features."
  },
  {
    "objectID": "posts/top_5_operating_systems/index.html#macos",
    "href": "posts/top_5_operating_systems/index.html#macos",
    "title": "To 5 Operating Systems",
    "section": "macOS",
    "text": "macOS\n\nOverview\nExclusive to Apple’s hardware, macOS offers a sleek and intuitive user experience. It is known for its stability, design aesthetics, and seamless integration with other Apple devices and services.\n\n\nFeatures\n\nIntegration: Seamless integration with other Apple devices.\nPerformance: Optimized for Mac hardware.\nSecurity: Built-in encryption and privacy features."
  },
  {
    "objectID": "posts/top_5_operating_systems/index.html#linux",
    "href": "posts/top_5_operating_systems/index.html#linux",
    "title": "To 5 Operating Systems",
    "section": "Linux",
    "text": "Linux\n\nOverview\nLinux is an open-source operating system available in various distributions (distros), catering to diverse user needs. Its flexibility, stability, and robustness have made it a favorite among developers, server administrators, and tech enthusiasts.\n\n\nFeatures\n\nOpen Source: Community-driven development and customization.\nVariety: Diverse distributions catering to different needs.\nStability: Known for reliability and performance."
  },
  {
    "objectID": "posts/top_5_operating_systems/index.html#android",
    "href": "posts/top_5_operating_systems/index.html#android",
    "title": "To 5 Operating Systems",
    "section": "Android",
    "text": "Android\n\nOverview\nDeveloped by Google, Android is the leading operating system for mobile devices globally. Its open-source nature, customizable interface, and extensive app ecosystem have made it a dominant force in the mobile market.\n\n\nFeatures\n\nCustomization: Ability to personalize interface and features.\nApp Ecosystem: Extensive library of applications.\nIntegration: Seamless integration with Google services."
  },
  {
    "objectID": "posts/top_5_operating_systems/index.html#iosipados",
    "href": "posts/top_5_operating_systems/index.html#iosipados",
    "title": "To 5 Operating Systems",
    "section": "iOS/iPadOS",
    "text": "iOS/iPadOS\n\nOverview\nExclusive to Apple’s mobile devices, iOS and iPadOS are renowned for their smooth performance, security features, and seamless integration within the Apple ecosystem.\n\n\nFeatures\n\nPerformance: Smooth and efficient operation on Apple devices.\nSecurity: Robust security measures and privacy controls.\nEcosystem Integration: Seamless integration with other Apple devices.\n\nIn conclusion, these operating systems showcase diversity in terms of architecture, platform support, and features, catering to the varied needs of users across different devices and industries.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/time_series_analysis/index.html",
    "href": "posts/time_series_analysis/index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 2 of our Time Series Analysis Series, Part 1 introduces you to what time series is. The link to part 1 is here\nAs name suggest its analysis of the time series data to identify the patterns in it. I will briefly explain the different techniques and test for time series data analysis.\n\n\nWe have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.\n[ad]"
  },
  {
    "objectID": "posts/time_series_analysis/index.html#time-series-analysis",
    "href": "posts/time_series_analysis/index.html#time-series-analysis",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 2 of our Time Series Analysis Series, Part 1 introduces you to what time series is. The link to part 1 is here\nAs name suggest its analysis of the time series data to identify the patterns in it. I will briefly explain the different techniques and test for time series data analysis.\n\n\nWe have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.\n[ad]"
  },
  {
    "objectID": "posts/time_series_analysis/index.html#decomposition-of-time-series",
    "href": "posts/time_series_analysis/index.html#decomposition-of-time-series",
    "title": "Time Series Analysis",
    "section": "Decomposition of Time Series",
    "text": "Decomposition of Time Series\nTime series decomposition helps to deconstruct the time series into several component like trend and seasonality for better visualization of its characteristics. Using time-series decomposition makes it easier to quickly identify a changing mean or variation in the data\n\n\n\nDecomposition_of_Time_Series"
  },
  {
    "objectID": "posts/time_series_analysis/index.html#stationary-data",
    "href": "posts/time_series_analysis/index.html#stationary-data",
    "title": "Time Series Analysis",
    "section": "Stationary Data",
    "text": "Stationary Data\nFor accurate analysis and forecasting trend and seasonality is removed from the time series and converted it into stationary series. Time series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time.\n\n\n\nStationarity"
  },
  {
    "objectID": "posts/time_series_analysis/index.html#test-for-stationarity",
    "href": "posts/time_series_analysis/index.html#test-for-stationarity",
    "title": "Time Series Analysis",
    "section": "Test for Stationarity",
    "text": "Test for Stationarity\nEasy way is to look at the plot and look for any obvious trend or seasonality. While working on real world data we can also use more sophisticated methods like rolling statistic and Augmented Dickey Fuller test to check stationarity of the data.\n\nRolling Statistics\nIn rolling statistics technique we define a size of window to calculate the mean and standard deviation throughout the series. For stationary series mean and standard deviation shouldn’t change with time.\n\n\nAugmented Dickey Fuller (ADF) Test\nI won’t go into the details of how this test works. I will concentrate more on how to interpret the result of this test to determine the stationarity of the series. ADF test will return ‘p-value’ and ‘Test Statistics’ output values.\n\np-value &gt; 0.05: non-stationary.\np-value &lt;= 0.05: stationary.\nTest statistics: More negative this value more likely we have stationary series. Also, this value should be smaller than critical values(1%, 5%, 10%). For e.g. If test statistic is smaller than the 5% critical values, then we can say with 95% confidence that this is a stationary series\n\n[ad]"
  },
  {
    "objectID": "posts/time_series_analysis/index.html#convert-non-stationary-data-to-stationary-data",
    "href": "posts/time_series_analysis/index.html#convert-non-stationary-data-to-stationary-data",
    "title": "Time Series Analysis",
    "section": "Convert Non-Stationary Data to Stationary Data",
    "text": "Convert Non-Stationary Data to Stationary Data\nAccounting for the time series data characteristics like trend and seasonality is called as making data stationary. So by making the mean and variance of the time series constant, we will get the stationary data. Below are the few technique used for the same…\n\nDifferencing\nDifferencing technique helps to remove the trend and seasonality from time series data. Differencing is performed by subtracting the previous observation from the current observation. The differenced data will contain one less data point than original data. So differencing actually reduces the number of observations and stabilize the mean of a time series.\n\\[d = t - t0\\]\nAfter performing the differencing it’s recommended to plot the data and visualize the change. In case there is not sufficient improvement you can perform second order or even third order differencing.\n\n\nTransformation\nA simple but often effective way to stabilize the variance across time is to apply a power transformation to the time series. Log, square root, cube root are most commonly used transformation techniques. Most of the time you can pick the type of growth of the time series and accordingly choose the transformation method. For. e.g. A time series that has a quadratic growth trend can be made linear by taking the square root. In case differencing don’t work, you may first want to use one of above transformation technique to remove the variation from the series.\n\n\n\nLog_Transformation\n\n\n\n\nMoving Average\nIn moving averages technique, a new series is created by taking the averages of data points from original series. In this technique we can use two or more raw data points to calculate the average. This is also called as ‘window width (w)’. Once window width is decided, averages are calculated from start to the end for each set of w consecutive values, hence the name moving averages. It can also be used for time series forecasting.\n\n\n\nMoving_Average\n\n\n\nWeighted Moving Averages(WMA)\nWMA is a technical indicator that assigns a greater weighting to the most recent data points, and less weighting to data points in the distant past. The WMA is obtained by multiplying each number in the data set by a predetermined weight and summing up the resulting values. There can be many techniques for assigning weights. A popular one is exponentially weighted moving average where weights are assigned to all the previous values with a decay factor.\n\n\nCentered Moving Averages(CMS)\nIn a centered moving average, the value of the moving average at time t is computed by centering the window around time t and averaging across the w values within the window. For example, a center moving average with a window of 3 would be calculated as\n\\[CMA(t) = mean(t-1, t, t+1)\\]\nCMA is very useful for visualizing the time series data\n\n\nTrailing Moving Averages(TMA)\nIn trailing moving average, instead of averaging over a window that is centered around a time period of interest, it simply takes the average of the last w values. For example, a trailing moving average with a window of 3 would be calculated as:\n\\[TMA(t) = mean(t-2, t-1, t)\\]\nTMA are useful for forecasting.\n[ad]"
  },
  {
    "objectID": "posts/time_series_analysis/index.html#correlation",
    "href": "posts/time_series_analysis/index.html#correlation",
    "title": "Time Series Analysis",
    "section": "Correlation",
    "text": "Correlation\n\nMost important point about values in time series is its dependence on the previous values.\nWe can calculate the correlation for time series observations with previous time steps, called as lags.\nBecause the correlation of the time series observations is calculated with values of the same series at previous times, this is called an autocorrelation or serial correlation.\nTo understand it better lets consider the example of fish prices. We will use below notation to represent the fish prices.\n\n\\(P(t)\\)= Fish price of today\n\\(P(t-1)\\) = Fish price of last month\n\\(P(t-2)\\) =Fish price of last to last month\n\nTime series of fish prices can be represented as \\(P(t-n),..... P(t-3), P(t-2),P(t-1), P(t)\\)\nSo if we have fish prices for last few months then it will be easy for us to predict the fish price for today (Here we are ignoring all other external factors that may affect the fish prices)\n\nAll the past and future data points are related in time series and ACF and PACF functions help us to determine correlation in it.\n\nAuto Correlation Function (ACF)\n\nACF tells you how correlated points are with each other, based on how many time steps they are separated by.\nNow to understand it better lets consider above example of fish prices. Let’s try to find the correlation between fish price for current month P(t) and two months ago P(t-2). Important thing to note that, fish price of two months ago can directly affect the today’s fish price or it can indirectly affect the fish price through last months price P(t-1)\nSo ACF consider the direct as well indirect effect between the points while determining the correlation\n\n\n\nPartial Auto Correlation Function (PACF)\n\nUnlike ACF, PACF only consider the direct effect between the points while determining the correlation\nIn case of above fish price example PACF will determine the correlation between fish price for current month P(t) and two months ago P(t-2) by considering only P(t) and P(t-2) and ignoring P(t-1)\n\n[ad]"
  },
  {
    "objectID": "posts/time_series_forecasting/index.html",
    "href": "posts/time_series_forecasting/index.html",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 3 of our Time Series Analysis Series, Part 2 introduces you to doing time series analysis. The link to part 2 is here\nForecasting refers to the future predictions based on the time series data analysis. Below are the steps performed during time series forecasting\n\nStep 1: Understand the time series characteristics like trend, seasonality etc\nStep 2: Do the analysis and identify the best method to make the time series stationary\nStep 3: Note down the transformation steps performed to make the time series stationary and make sure that the reverse transformation of data is possible to get the original scale back\nStep 4: Based on data analysis choose the appropriate model for time series forecasting\nStep 5: We can assess the performance of a model by applying simple metrics such as residual sum of squares(RSS). Make sure to use whole data for prediction.\nStep 6: Now we will have an array of predictions which are in transformed scale. We just need to apply the reverse transformation to get the prediction values in original scale.\nStep 7: At the end we can do the future forecasting and get the future forecasted values in original scale."
  },
  {
    "objectID": "posts/time_series_forecasting/index.html#time-series-forecasting",
    "href": "posts/time_series_forecasting/index.html#time-series-forecasting",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 3 of our Time Series Analysis Series, Part 2 introduces you to doing time series analysis. The link to part 2 is here\nForecasting refers to the future predictions based on the time series data analysis. Below are the steps performed during time series forecasting\n\nStep 1: Understand the time series characteristics like trend, seasonality etc\nStep 2: Do the analysis and identify the best method to make the time series stationary\nStep 3: Note down the transformation steps performed to make the time series stationary and make sure that the reverse transformation of data is possible to get the original scale back\nStep 4: Based on data analysis choose the appropriate model for time series forecasting\nStep 5: We can assess the performance of a model by applying simple metrics such as residual sum of squares(RSS). Make sure to use whole data for prediction.\nStep 6: Now we will have an array of predictions which are in transformed scale. We just need to apply the reverse transformation to get the prediction values in original scale.\nStep 7: At the end we can do the future forecasting and get the future forecasted values in original scale."
  },
  {
    "objectID": "posts/time_series_forecasting/index.html#models-used-for-time-series-forecasting",
    "href": "posts/time_series_forecasting/index.html#models-used-for-time-series-forecasting",
    "title": "Time Series Forecasting",
    "section": "Models Used For Time Series Forecasting",
    "text": "Models Used For Time Series Forecasting\n\nAutoregression (AR)\nMoving Average (MA)\nAutoregressive Moving Average (ARMA)\nAutoregressive Integrated Moving Average (ARIMA)\nSeasonal Autoregressive Integrated Moving-Average (SARIMA)\nSeasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)\nVector Autoregression (VAR)\nVector Autoregression Moving-Average (VARMA)\nVector Autoregression Moving-Average with Exogenous Regressors (VARMAX)\nSimple Exponential Smoothing (SES)\nHolt Winter’s Exponential Smoothing (HWES)\n\nNext part of this article we are going to analyze and forecast air passengers time series data using ARIMA model. Brief introduction of ARIMA model is as below\n[ad]"
  },
  {
    "objectID": "posts/time_series_forecasting/index.html#arima",
    "href": "posts/time_series_forecasting/index.html#arima",
    "title": "Time Series Forecasting",
    "section": "ARIMA",
    "text": "ARIMA\n\nARIMA stands for Auto-Regressive Integrated Moving Averages. It is actually a combination of AR and MA model.\nARIMA has three parameters ‘p’ for the order of Auto-Regressive (AR) part, ‘q’ for the order of Moving Average (MA) part and ‘d’ for the order of integrated part.\n\n\nAuto-Regressive (AR) Model:\n\nAs the name indicates, its the regression of the variables against itself. In this model linear combination of the past values are used to forecast the future values.\nTo figure out the order of AR model we will use PACF function\n\n\n\nIntegration(I):\n\nUses differencing of observations (subtracting an observation from observation at the previous time step) in order to make the time series stationary. Differencing involves the subtraction of the current values of a series with its previous values \\(d\\) number of times.\nMost of the time value of \\(d = 1\\), means first order of difference.\n\n\n\nMoving Average (MA) Model:\n\nRather than using past values of the forecast variable in a regression, a moving average model uses linear combination of past forecast errors\nTo figure out the order of MA model we will use ACF function\n\n[ad]"
  },
  {
    "objectID": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html",
    "href": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html",
    "title": "Handling Imbalanced Datasets",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nHandling imbalanced datasets is a common challenge in machine learning, especially in classification tasks where one class significantly outnumbers the other(s). Let’s go through a simple example using the popular Iris dataset, which we’ll artificially imbalance for demonstration purposes.\nThe Iris dataset consists of 150 samples, each belonging to one of three classes: Iris Setosa, Iris Versicolour, and Iris Virginica. We’ll create an imbalanced version of this dataset where one class is underrepresented.\nFirst, let’s load the dataset and create the imbalance:\n# !pip install imbalanced-learn\n# !pip install --upgrade dataidea\nfrom dataidea.packages import pd, plt\nfrom sklearn.datasets import load_iris\n# Load Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Convert to DataFrame for manipulation\ndf = pd.DataFrame(data=np.c_[X, y],\n                  columns=iris.feature_names + ['target'])\n\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\ntarget\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n0.0\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n0.0\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n\n\n\n# Introduce imbalance by removing samples from one class\nclass_to_remove = 2  # Iris Virginica\nimbalance_ratio = 0.5  # Ratio of samples to be removed\nindices_to_remove = np.random.choice(df[df['target'] == class_to_remove].index,\n                                     size=int(imbalance_ratio * len(df[df['target'] == class_to_remove])),\n                                     replace=False)\ndf_imbalanced = df.drop(indices_to_remove)\n\n# Check the class distribution\nvalue_counts = df_imbalanced['target'].value_counts()\nprint(value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    25\nName: count, dtype: int64\nplt.bar(value_counts.index,\n        value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\nNow, df_imbalanced contains the imbalanced dataset. Next, we’ll demonstrate a few techniques to handle this imbalance:\n\nResampling Methods:\n\nOversampling: Randomly duplicate samples from the minority class.\nUndersampling: Randomly remove samples from the majority class.\n\nSynthetic Sampling Methods:\n\nSMOTE (Synthetic Minority Over-sampling Technique): Generates synthetic samples for the minority class.\n\nAlgorithmic Techniques:\n\nAlgorithm Tuning: Adjusting class weights in the algorithm.\nEnsemble Methods: Using ensemble techniques like bagging or boosting."
  },
  {
    "objectID": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#handling-imbalanced-dataset",
    "href": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#handling-imbalanced-dataset",
    "title": "Handling Imbalanced Datasets",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nHandling imbalanced datasets is a common challenge in machine learning, especially in classification tasks where one class significantly outnumbers the other(s). Let’s go through a simple example using the popular Iris dataset, which we’ll artificially imbalance for demonstration purposes.\nThe Iris dataset consists of 150 samples, each belonging to one of three classes: Iris Setosa, Iris Versicolour, and Iris Virginica. We’ll create an imbalanced version of this dataset where one class is underrepresented.\nFirst, let’s load the dataset and create the imbalance:\n# !pip install imbalanced-learn\n# !pip install --upgrade dataidea\nfrom dataidea.packages import pd, plt\nfrom sklearn.datasets import load_iris\n# Load Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Convert to DataFrame for manipulation\ndf = pd.DataFrame(data=np.c_[X, y],\n                  columns=iris.feature_names + ['target'])\n\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\ntarget\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n0.0\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n0.0\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n\n\n\n# Introduce imbalance by removing samples from one class\nclass_to_remove = 2  # Iris Virginica\nimbalance_ratio = 0.5  # Ratio of samples to be removed\nindices_to_remove = np.random.choice(df[df['target'] == class_to_remove].index,\n                                     size=int(imbalance_ratio * len(df[df['target'] == class_to_remove])),\n                                     replace=False)\ndf_imbalanced = df.drop(indices_to_remove)\n\n# Check the class distribution\nvalue_counts = df_imbalanced['target'].value_counts()\nprint(value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    25\nName: count, dtype: int64\nplt.bar(value_counts.index,\n        value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\nNow, df_imbalanced contains the imbalanced dataset. Next, we’ll demonstrate a few techniques to handle this imbalance:\n\nResampling Methods:\n\nOversampling: Randomly duplicate samples from the minority class.\nUndersampling: Randomly remove samples from the majority class.\n\nSynthetic Sampling Methods:\n\nSMOTE (Synthetic Minority Over-sampling Technique): Generates synthetic samples for the minority class.\n\nAlgorithmic Techniques:\n\nAlgorithm Tuning: Adjusting class weights in the algorithm.\nEnsemble Methods: Using ensemble techniques like bagging or boosting."
  },
  {
    "objectID": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#resampling",
    "href": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#resampling",
    "title": "Handling Imbalanced Datasets",
    "section": "Resampling",
    "text": "Resampling\n\nOversampling\nLet’s implement oversampling using the imbalanced-learn library:\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Separate features and target\nX_imbalanced = df_imbalanced.drop('target', axis=1)\ny_imbalanced = df_imbalanced['target']\n\n# Apply Random Over-Sampling\noversample = RandomOverSampler(sampling_strategy='auto',\n                               random_state=42)\nX_resampled, y_resampled = oversample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after oversampling\noversampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(oversampled_data_value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\nplt.bar(oversampled_data_value_counts.index,\n        oversampled_data_value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\n\n\nUndersampling:\nUndersampling involves reducing the number of samples in the majority class to balance the dataset. Here’s how you can apply random undersampling using the imbalanced-learn library:\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Apply Random Under-Sampling\nundersample = RandomUnderSampler(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = undersample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after undersampling\nundersampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(undersampled_data_value_counts)\ntarget\n0.0    25\n1.0    25\n2.0    25\nName: count, dtype: int64\nplt.bar(undersampled_data_value_counts.index,\n        undersampled_data_value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#smote-synthetic-minority-over-sampling-technique",
    "href": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#smote-synthetic-minority-over-sampling-technique",
    "title": "Handling Imbalanced Datasets",
    "section": "SMOTE (Synthetic Minority Over-sampling Technique)",
    "text": "SMOTE (Synthetic Minority Over-sampling Technique)\nSMOTE generates synthetic samples for the minority class by interpolating between existing minority class samples.\nHere’s how you can apply SMOTE using the imbalanced-learn library:\nfrom imblearn.over_sampling import SMOTE\n\n# Apply SMOTE\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after SMOTE\nsmoted_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(smoted_data_value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\nplt.bar(smoted_data_value_counts.index,\n        smoted_data_value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\nBy using SMOTE, you can generate synthetic samples for the minority class, effectively increasing its representation in the dataset. This can help to mitigate the class imbalance issue and improve the performance of your machine learning model."
  },
  {
    "objectID": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#algorithmic-techniques",
    "href": "posts/handling_imbalanced_datasets/handling_imbalanced_data.html#algorithmic-techniques",
    "title": "Handling Imbalanced Datasets",
    "section": "Algorithmic Techniques",
    "text": "Algorithmic Techniques\n\nAlgorithm Tuning:\nMany algorithms allow you to specify class weights to penalize misclassifications of the minority class more heavily. Here’s an example using the class_weight parameter in a logistic regression classifier:\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_imbalanced, y_imbalanced,\n                                                    test_size=0.2, random_state=42)\n\n# Define the logistic regression classifier with class weights\nclass_weights = {0: 1, 1: 1, 2: 20}  # Penalize the minority class more heavily\nlog_reg = LogisticRegression(class_weight=class_weights)\n\n# Train the model\nlog_reg.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = log_reg.predict(X_test)\n\n# display classification report\npd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n\n\n\n\n\n\n\n\nprecision\n\n\nrecall\n\n\nf1-score\n\n\nsupport\n\n\n\n\n\n\n0.0\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n13.00\n\n\n\n\n1.0\n\n\n1.000000\n\n\n0.750000\n\n\n0.857143\n\n\n8.00\n\n\n\n\n2.0\n\n\n0.666667\n\n\n1.000000\n\n\n0.800000\n\n\n4.00\n\n\n\n\naccuracy\n\n\n0.920000\n\n\n0.920000\n\n\n0.920000\n\n\n0.92\n\n\n\n\nmacro avg\n\n\n0.888889\n\n\n0.916667\n\n\n0.885714\n\n\n25.00\n\n\n\n\nweighted avg\n\n\n0.946667\n\n\n0.920000\n\n\n0.922286\n\n\n25.00\n\n\n\n\n\nIn this example, the class weight for the minority class is increased to penalize misclassifications more heavily.\n\n\nEnsemble Methods\nEnsemble methods can also be effective for handling imbalanced datasets. Techniques such as bagging and boosting can improve the performance of classifiers, especially when dealing with imbalanced classes.\nHere’s an example of using ensemble methods like Random Forest, a popular bagging algorithm, with an imbalanced dataset:\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Define and train Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_rf = rf_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"Random Forest Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_rf, output_dict=True)).transpose()\nRandom Forest Classifier:\n\n\n\n\n\n\n\n\nprecision\n\n\nrecall\n\n\nf1-score\n\n\nsupport\n\n\n\n\n\n\n0.0\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n13.00\n\n\n\n\n1.0\n\n\n1.000000\n\n\n0.875000\n\n\n0.933333\n\n\n8.00\n\n\n\n\n2.0\n\n\n0.800000\n\n\n1.000000\n\n\n0.888889\n\n\n4.00\n\n\n\n\naccuracy\n\n\n0.960000\n\n\n0.960000\n\n\n0.960000\n\n\n0.96\n\n\n\n\nmacro avg\n\n\n0.933333\n\n\n0.958333\n\n\n0.940741\n\n\n25.00\n\n\n\n\nweighted avg\n\n\n0.968000\n\n\n0.960000\n\n\n0.960889\n\n\n25.00\n\n\n\n\n\nEnsemble methods like Random Forest build multiple decision trees and combine their predictions to make a final prediction. This can often lead to better generalization and performance, even in the presence of imbalanced classes.\n\n\nAdaBoost Classifier\nAnother ensemble method that specifically addresses class imbalance is AdaBoost (Adaptive Boosting). AdaBoost focuses more on those training instances that were previously misclassified, thus giving higher weight to the minority class instances.\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Define and train AdaBoost classifier\nada_classifier = AdaBoostClassifier(n_estimators=100, random_state=42)\nada_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_ada = ada_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"AdaBoost Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_ada, output_dict=True)).transpose()\nAdaBoost Classifier:\n\n\n/home/jumashafara/venvs/dataidea/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n  warnings.warn(\n\n\n\n\n\n\n\n\nprecision\n\n\nrecall\n\n\nf1-score\n\n\nsupport\n\n\n\n\n\n\n0.0\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n13.00\n\n\n\n\n1.0\n\n\n1.000000\n\n\n0.875000\n\n\n0.933333\n\n\n8.00\n\n\n\n\n2.0\n\n\n0.800000\n\n\n1.000000\n\n\n0.888889\n\n\n4.00\n\n\n\n\naccuracy\n\n\n0.960000\n\n\n0.960000\n\n\n0.960000\n\n\n0.96\n\n\n\n\nmacro avg\n\n\n0.933333\n\n\n0.958333\n\n\n0.940741\n\n\n25.00\n\n\n\n\nweighted avg\n\n\n0.968000\n\n\n0.960000\n\n\n0.960889\n\n\n25.00\n\n\n\n\n\nBy utilizing ensemble methods like Random Forest and AdaBoost, you can often achieve better performance on imbalanced datasets compared to individual classifiers, as these methods inherently mitigate the effects of class imbalance through their construction.\nThese are just a few techniques for handling imbalanced datasets. It’s crucial to experiment with different methods and evaluate their performance using appropriate evaluation metrics to find the best approach for your specific problem.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/what-is-demographic-data/index.html",
    "href": "posts/what-is-demographic-data/index.html",
    "title": "What is Demographic Data, Understanding and Utilizing It",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDemographic data helps us to deepen our knowledge of the target audience and to create buyer personas. It is primarily used to strategically tailor offerings to specific target groups and can serve as the basis for business analysis and performance reporting. Practical business intelligence relies on the synergy between analytics and reporting, where analytics uncovers valuable insights, and reporting communicates these findings to stakeholders.\n\nWhat is Demographic Data?\nDemographic data is information about groups of people according to certain attributes such as age, sex, and place of residence. It can include socioeconomic factors such as occupation, family status, or income. Demographics and interests are among the most important web analytics and consumer buying behavior analytics statistics. In marketing, the demographics approach focuses more on age, gender, and interests rather than fertility and mortality data.\n\n\nImportance of Demographics\nIn web analytics and online marketing, demographic data is collected to gain deeper insights into the target group of a web page or to create buyer personas based on this information. It is primarily used for strategic supply targeting and can also be used for business analysis and performance reporting.\n\n\nExamples of Demographic Data\nHere are some examples of data you can request in a demographic survey:\n\n\n\n\n\nAge: One of the most important demographic factors, age is a good indicator of the groups of users that visit a web page and the age groups that buy the most. It provides information about content that is interesting to a particular age group and where potential can be identified.\nGender: Gender information shows which parts of a website or which products are more suitable for men or women. Classifying visits according to gender survey questions can serve as the basis for planning campaigns targeting men or women.\nEducation: Data on education can indicate whether users have attended university.\nIncome: Income information helps target high-income individuals, for example, to buy a high-end product.\nInterests: Data on user interests shows what visitors of a web page are interested in and allows marketers to draw conclusions about consumer behavior. For example, if users have an affinity for certain product categories, marketers can create ads focused on these interests.\nLanguage: For online marketing and website design, the language of the target group is important, especially for internationally oriented online stores. Advertising and content should be geared towards the language spoken by the target group.\nCountries: Knowing the region, city, or country users come from is important for targeting advertising measures specifically to these geographic segments.\n\nAdditionally, the use of demographic data allows for segmenting user groups, for example, to establish a connection between people aged 18 to 24 with certain keywords and interests. This type of targeting is especially useful for remarketing campaigns.\n\n\n\n\n\n\nWhat Does Demographic Data Tell Us?\nDemographic data can provide answers to the following questions:\n\nWhat groups of users visit the website? Young users have different interests than older users.\nWhich of these groups provides the most income? The most profitable clientele usually belongs to a certain age group.\nWhere should content be placed to increase sales? Relevant content can be tailored to age, gender, and interests.\nHow can ads be better targeted? Young female users want to see different types of ads compared to older male users.\nWhat factors improve remarketing? Thanks to segmentation, subsequent actions can be tailored to the target group and corresponding interests.\nHow can email campaigns be more effective and directly target specific groups? Emails can be sent to specific groups based on demographic data.\n\n\n\n\n\nDemographic data provides much deeper insight into user behavior. Information about user groups can be used to improve the effectiveness of advertising campaigns, optimize website offerings, and drive sales. At the same time, the legal use of this data should not be ignored: it should be anonymous, and users should be informed about the collection of data, as well as the use of cookies. Users must also have the opportunity to object to data collection.\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/overview-of-machine-learning/index.html",
    "href": "posts/overview-of-machine-learning/index.html",
    "title": "Overview of Machine Learning",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/overview-of-machine-learning/index.html#references",
    "href": "posts/overview-of-machine-learning/index.html#references",
    "title": "Overview of Machine Learning",
    "section": "References",
    "text": "References\n\nDATAIDEA - What is Demographic Data\nIBM - What is Machine Learning\nData Camp - What is Machine Learning\n\n\n\nYou may also like:\n\n\n\nWhat is Demographic Data"
  },
  {
    "objectID": "posts/who-will-win-the-euros-2024/index.html",
    "href": "posts/who-will-win-the-euros-2024/index.html",
    "title": "Who Will Win Euro 2024? The Opta Predictions",
    "section": "",
    "text": "Image by Opta\n\n\nEuro 2024 is set to be a thrilling tournament, and the Opta Supercomputer’s pre-tournament predictions give us a fascinating insight into the likely outcomes. Here’s a summary of the key predictions and contenders:\n\nFavorites to Win\n\nEngland (19.9%)\n\nKey Players: Harry Kane, Jude Bellingham, Phil Foden\nGroup: Denmark, Serbia, Slovenia\nChances: England are favorites to win, with strong attacking talent. They have a high probability (95.4%) of reaching the last 16, and a significant chance of progressing to the quarter-finals (70.0%) and semi-finals (48.2%). They have a 19.9% chance of winning the tournament.\n\nFrance (19.1%)\n\nKey Players: Kylian Mbappé, Antoine Griezmann\nGroup: Netherlands, other teams\nChances: France are close behind England as favorites. They have a 69.2% chance of reaching the quarter-finals and a 48.1% chance of making it to the semi-finals. Their probability of winning the tournament stands at 19.1%.\n\nGermany (12.4%)\n\nKey Players: Manuel Neuer, Toni Kroos, Thomas Müller, Kai Havertz\nGroup: Other teams\nChances: Despite recent struggles, Germany are strong contenders with a home advantage. They have a 36.5% chance of reaching the semi-finals and a 12.4% chance of winning the tournament.\n\nSpain (9.6%)\n\nKey Players: Álvaro Morata, other key players\nGroup: Italy, Croatia, Albania\nChances: Spain are seen as strong contenders with a 59.1% chance of reaching the quarter-finals. They have a 9.6% chance of winning the tournament.\n\nPortugal (9.2%)\n\nKey Players: Cristiano Ronaldo, Bruno Fernandes\nGroup: Czech Republic, Turkey, Georgia\nChances: Portugal have a high likelihood (93.6%) of progressing from the group stage and a 33.6% chance of reaching the semi-finals. Their probability of winning is 9.2%.\n\n\n\n\nOther Contenders\n\nNetherlands (5.1%)\nItaly (5.0%)\nBelgium (4.7%)\n\n\n\nSummary\nThe Opta Supercomputer simulations highlight England and France as the top favorites to win Euro 2024, with Germany, Spain, and Portugal also having strong chances. The tournament promises intense competition with key matchups and potential surprises. Fans can look forward to an exciting summer of football as these top nations vie for European glory.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/cost-function-in-machine-learning/index.html",
    "href": "posts/cost-function-in-machine-learning/index.html",
    "title": "Understanding the Cost Function in Linear Regression",
    "section": "",
    "text": "Image by DATAIDEA\n\n\nLinear regression is a fundamental algorithm in machine learning and statistics, used to model the relationship between a dependent variable and one or more independent variables. At the heart of linear regression lies the concept of the cost function, a crucial element that helps the model learn and make accurate predictions. In this article, I’ll get into what a cost function is, why it’s important, and how it works in the context of linear regression.\n\nWhat is a Cost Function?\nA cost function, also known as a loss function or error function, quantifies the error between predicted values and actual values. It is a mathematical function that the model aims to minimize during the training process. By minimizing the cost function, the model adjusts its parameters (weights and biases) to produce the most accurate predictions possible.\n\n\n\n\n\n\nLinear Regression Recap\nBefore I go deep into the cost function, let me briefly revisit the basics of linear regression. The goal of linear regression is to find the best-fitting straight line through the data points, which can be represented by the equation:\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon\\]\nWhere:\n\n\\(y\\) is the dependent variable (the outcome we’re trying to predict).\n\\(x\\) is the independent variable (the feature or input).\n\\(\\beta_0\\) is the y-intercept (the value of \\(y\\) when \\(x\\) is zero).\n\\(\\beta_1\\) is the slope of the line (the change in \\(y\\) for a unit change in \\(x\\)).\n\\(\\epsilon\\) is the error term (the difference between the observed and predicted values).\n\n\n\nThe Role of the Cost Function\nThe cost function in linear regression measures how well the model’s predictions match the actual data. One of the most commonly used cost functions is the Mean Squared Error (MSE), which is defined as:\n\\[\\text{MSE}  = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2\\]\nWhere:\n\n\\(\\text{MSE}\\) is the cost function.\n\\(n\\) is the number of training examples.\n\\(\\hat{y}^{(i)}\\) is the predicted value for the \\(i\\)-th training example, given by the hypothesis function \\(\\hat{y}\\).\n\\(y^{(i)}\\) is the actual value for the \\(i\\)-th training example.\n\\(\\theta\\) represents the parameters of the hypothesis.\n\nThe MSE calculates the average of the squares of the errors (the differences between actual and predicted values). Squaring the errors ensures that both positive and negative errors are treated equally and magnifies larger errors, making them more impactful on the cost function.\n\n\nWhy Minimize the Cost Function?\nMinimizing the cost function is essential because it directly translates to improving the model’s accuracy. When the cost function is minimized, the predicted values are as close as possible to the actual values, indicating a well-fitting model. This process involves finding the optimal values for the model parameters (\\(\\beta_0\\) and \\(\\beta_1\\)).\n\n\n\n\n\n\nGradient Descent: An Optimization Technique\nTo minimize the cost function, linear regression often employs an optimization technique called gradient descent. Gradient descent iteratively adjusts the model parameters in the direction that reduces the cost function. The update rules for the parameters are:\n\\[\\beta_0 = \\beta_0 - \\alpha \\frac{\\partial}{\\partial \\beta_0} \\text{MSE}\\] \\[\\beta_1 = \\beta_1 - \\alpha \\frac{\\partial}{\\partial \\beta_1} \\text{MSE}\\]\nHere:\n\n\\(\\alpha\\) is the learning rate, a hyperparameter that controls the step size of each update.\n\\(\\frac{\\partial}{\\partial \\beta_0} \\text{MSE}\\) and \\(\\frac{\\partial}{\\partial \\beta_1} \\text{MSE}\\) are the partial derivatives of the MSE with respect to \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\n\nThese partial derivatives (also called gradients) indicate the direction and magnitude of the steepest increase in the cost function. By moving in the opposite direction of the gradients, gradient descent reduces the cost function, gradually leading to the optimal parameter values.\n\n\nConclusion\nThe cost function is a fundamental concept in linear regression, serving as the guiding metric for model optimization. By quantifying the difference between predicted and actual values, the cost function enables the model to learn and improve its predictions through techniques like gradient descent. Understanding and minimizing the cost function is crucial for building accurate and reliable linear regression models, making it a cornerstone of predictive analytics and machine learning.\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/new-programming-for-data-science-app/index.html",
    "href": "posts/new-programming-for-data-science-app/index.html",
    "title": "Introducing Our New Programming for Data Science App!",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nWe are thrilled to announce the launch of our brand-new app designed specifically for learning “Programming for Data Science.” Whether you’re diving into Python, exploring data analysis techniques or machine learning concepts, our app is tailored to support your learning journey every step of the way.\n\nHow to Access the App:\nAs a valued subscriber of our newsletter, you have exclusive early access to our app. Simply click here (download link) to download it now, install and start exploring the exciting world of data science programming!\n\n\nWe Want Your Feedback:\nYour feedback is invaluable to us as we continue to improve and expand our app. Please feel free to share your thoughts, suggestions, or any issues you encounter. Your input will help us tailor the app to meet your learning needs better.\nThank you for being a part of our community and for your continued support. We’re excited to embark on this learning journey with you through our new app!\nWarm regards,\nJuma Shafara\nData Scientist, Instructor, CoFounder\nDATAIDEA\nhttps://www.dataidea.org\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/new-programming-for-data-science-app/index.html#how-to-access-the-app",
    "href": "posts/new-programming-for-data-science-app/index.html#how-to-access-the-app",
    "title": "Introducing Our New Programming for Data Science App!",
    "section": "How to Access the App:",
    "text": "How to Access the App:\nAs a valued subscriber of our newsletter, you have exclusive early access to our app. Simply click here (download link) to download it now and start exploring the exciting world of data science programming!"
  },
  {
    "objectID": "posts/new-programming-for-data-science-app/index.html#we-want-your-feedback",
    "href": "posts/new-programming-for-data-science-app/index.html#we-want-your-feedback",
    "title": "Introducing Our New Programming for Data Science App!",
    "section": "We Want Your Feedback:",
    "text": "We Want Your Feedback:\nYour feedback is invaluable to us as we continue to improve and expand our app. Please feel free to share your thoughts, suggestions, or any issues you encounter. Your input will help us tailor the app to meet your learning needs better.\nThank you for being a part of our community and for your continued support. We’re excited to embark on this learning journey with you through our new app!\nWarm regards,\nJuma Shafara\nData Scientist, Instructor, CoFounder\nDATAIDEA\nwww.dataidea.org"
  },
  {
    "objectID": "posts/what-is-supervised-machine-learning/index.html",
    "href": "posts/what-is-supervised-machine-learning/index.html",
    "title": "What is Supervised Machine Learning",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/what-is-supervised-machine-learning/index.html#understanding-supervised-machine-learning",
    "href": "posts/what-is-supervised-machine-learning/index.html#understanding-supervised-machine-learning",
    "title": "What is Supervised Machine Learning",
    "section": "Understanding Supervised Machine Learning",
    "text": "Understanding Supervised Machine Learning\nSupervised machine learning is a subfield of machine learning where an algorithm learns from labeled training data to make predictions or decisions without being explicitly programmed to perform the task. The process involves two primary phases: training and testing. Here, we’ll delve into the details of how supervised learning works, using formulas and tables to illustrate key concepts.\n\nKey Concepts in Supervised Learning\n\nLabeled Data: The foundational element in supervised learning is labeled data, which consists of input-output pairs. Each input (feature vector \\(\\mathbf{x}\\)) has a corresponding output (label \\(y\\)). For instance, in a housing price prediction model, the features could include the size of the house, number of bedrooms, etc., and the label would be the price of the house.\nTraining Phase: During the training phase, the algorithm learns a mapping from inputs to outputs using the labeled dataset. This involves optimizing a model to minimize the difference between the predicted outputs and the actual outputs.\nTesting Phase: After training, the model’s performance is evaluated on a separate testing set, which was not seen during training. This helps assess the model’s generalization ability to new, unseen data.\n\n\n\nSupervised Learning Algorithms\nThere are two main types of supervised learning tasks: regression and classification.\n\nRegression: Used when the output variable is continuous. Example: Predicting house prices.\nClassification: Used when the output variable is categorical. Example: Spam email detection.\n\n\n\nMathematical Formulation\n\nRegression\nFor regression tasks, a common algorithm is Linear Regression. The goal is to find the best-fitting line through the data points.\nThe model is defined as: \\[y = \\mathbf{w}^T \\mathbf{x} + b\\]\nwhere: - \\(y\\) is the predicted output. - \\(\\mathbf{x}\\) is the feature vector. - \\(\\mathbf{w}\\) is the weight vector. - \\(b\\) is the bias term.\nThe training process involves minimizing the loss function, typically the Mean Squared Error (MSE): \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2\\]\nwhere: - \\(n\\) is the number of training examples. - \\(y_i\\) is the actual output for the \\(i\\)-th example. - \\(\\hat{y_i}\\) is the predicted output for the \\(i\\)-th example.\n\n\nClassification\nFor classification tasks, Logistic Regression is a commonly used algorithm. The model estimates the probability that a given input belongs to a certain class.\nThe logistic model is: \\[P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)\\]\nwhere \\(\\sigma\\) is the sigmoid function: \\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\]\nThe loss function for logistic regression is the Binary Cross-Entropy: \\[\\text{BCE} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}) \\right]\\]\n\n\n\nExample: Housing Price Prediction\n\nTraining Data\n\n\n\nSize (sq ft)\nBedrooms\nPrice ($)\n\n\n\n\n1500\n3\n300,000\n\n\n1800\n4\n350,000\n\n\n2000\n4\n400,000\n\n\n2200\n5\n450,000\n\n\n\n\n\nLinear Regression Model\nAssume we have a feature vector \\(\\mathbf{x} = [\\text{Size, Bedrooms}]\\) and we want to predict the price \\(y\\).\n\nInitialize weights \\(\\mathbf{w}\\) and bias \\(b\\).\nUse the training data to find the best \\(\\mathbf{w}\\) and \\(b\\) that minimize the MSE.\nPredict the price for a new house with size 2100 sq ft and 4 bedrooms.\n\n\n\n\nExample: Email Spam Detection\n\nTraining Data\n\n\n\nEmail Content\nSpam (1) / Not Spam (0)\n\n\n\n\n“Win a free vacation now!”\n1\n\n\n“Meeting at 10 AM tomorrow”\n0\n\n\n“Limited time offer, click now!”\n1\n\n\n“Your report is due by Friday”\n0\n\n\n\n\n\nLogistic Regression Model\n\nConvert email content to feature vectors (e.g., word counts, presence of certain keywords).\nInitialize weights \\(\\mathbf{w}\\) and bias \\(b\\).\nUse the training data to find the best \\(\\mathbf{w}\\) and \\(b\\) that minimize the BCE.\nPredict whether a new email “Congratulations, you’ve won!” is spam.\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\nYou may also like:\n\n\n\nOverview of Machine Learning"
  },
  {
    "objectID": "posts/how-the-decision-tree-classifier-works/index.html",
    "href": "posts/how-the-decision-tree-classifier-works/index.html",
    "title": "How The Decision Tree Classifiers Works",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\n\nIntroduction\nDecision tree classifiers are a type of supervised machine learning algorithm used for classification tasks. They are popular due to their simplicity and interpretability. This article will explain how decision tree classifiers work in simple terms, breaking down the key concepts and processes involved.\n\n\nWhat is a Decision Tree?\nA decision tree is a flowchart-like structure where:\n\nEach internal node represents a “decision” based on the value of a feature.\nEach branch represents the outcome of a decision.\nEach leaf node represents a class label (the decision made after all features are considered).\n\n\n\nHow Does a Decision Tree Classifier Work?\n\nStarting at the Root Node:\nThe process begins at the root node, which contains the entire dataset. The goal is to split this dataset into subsets that are more homogenous in terms of the target variable (class label).\nChoosing the Best Feature to Split On:\nAt each step, the algorithm selects the feature that best separates the classes. This is done using a metric like Gini impurity or Information Gain.\n\nGini Impurity: Measures the frequency at which any element of the dataset would be misclassified. It’s calculated as: \\[\nGini = 1 - \\sum_{i=1}^{n} (p_i)^2\n\\] where \\(p_i\\) is the probability of an element being classified into a particular class.\nInformation Gain: Measures the reduction in entropy or impurity before and after a split. It’s calculated as: \\[\n\\text{Information Gain} = \\text{Entropy(before split)} - \\sum_{i=1}^{k} \\frac{n_i}{n} \\times \\text{Entropy}(i)\n\\] where \\(n_i\\) is the number of instances in the \\(i\\)-th subset.\n\nSplitting the Node:\nOnce the best feature is chosen, the dataset is split into subsets based on the feature’s values. Each subset forms a new node.\nRepeating the Process:\nThe algorithm recursively repeats the process for each new node, selecting the best feature to split on and creating further branches, until one of the stopping criteria is met:\n\nAll instances in a node belong to the same class.\nNo more features are left to split on.\nA pre-defined maximum tree depth is reached.\n\nMaking Predictions:\nAfter the tree is built, it can be used to classify new instances. Starting from the root, the instance is evaluated against the decision rules at each node, following the branches until it reaches a leaf node, which gives the predicted class.\n\n\n\nExample of a Decision Tree Classifier\nConsider a simple example where we want to classify whether a person will buy a computer based on their age and income.\n\nTraining Data\n\n\n\nAge\nIncome\nBuys Computer\n\n\n\n\n&lt;30\nHigh\nNo\n\n\n&lt;30\nHigh\nNo\n\n\n31-40\nHigh\nYes\n\n\n&gt;40\nMedium\nYes\n\n\n&gt;40\nLow\nNo\n\n\n&gt;40\nLow\nYes\n\n\n31-40\nLow\nYes\n\n\n&lt;30\nMedium\nNo\n\n\n&lt;30\nLow\nYes\n\n\n&gt;40\nMedium\nYes\n\n\n&lt;30\nMedium\nYes\n\n\n31-40\nMedium\nYes\n\n\n\n\n\nBuilding the Tree\n\nRoot Node:\n\nCalculate the Gini impurity for the entire dataset.\nSelect the feature (Age or Income) that provides the best split based on Gini impurity or Information Gain.\n\nFirst Split:\n\nSuppose Age is selected. The data is split into three groups: &lt;30, 31-40, and &gt;40.\n\nFurther Splits:\n\nFor each age group, calculate the Gini impurity or Information Gain again and split further based on Income.\n\n\n\n\nResulting Tree\n          [Age]\n         /  |   \\\n      &lt;30 31-40 &gt;40\n      /     |     \\\n  [Income]  Yes  [Income]\n    /  \\          /   \\\n  High Medium    Medium Low\n   No   Yes       Yes   No\n\n\n\nAdvantages of Decision Trees\n\nSimple to Understand: They are easy to visualize and interpret.\nNon-linear Relationships: Can capture non-linear relationships between features and the target variable.\nLittle Data Preparation: Requires little data preprocessing compared to other algorithms.\n\n\n\nDisadvantages of Decision Trees\n\nOverfitting: Trees can become very complex and overfit the training data, especially if not pruned.\nUnstable: Small changes in the data can lead to different splits and thus different trees.\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\nYou may also like:\n\n\n\nWhat is Supervised Machine Learning\n\n \n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/how-the-decision-tree-classifier-works/index.html#understanding-how-decision-tree-classifiers-work",
    "href": "posts/how-the-decision-tree-classifier-works/index.html#understanding-how-decision-tree-classifiers-work",
    "title": "How The Decision Tree Classifiers Works",
    "section": "Understanding How Decision Tree Classifiers Work",
    "text": "Understanding How Decision Tree Classifiers Work\n\nIntroduction\nDecision tree classifiers are a type of supervised machine learning algorithm used for classification tasks. They are popular due to their simplicity and interpretability. This article will explain how decision tree classifiers work in simple terms, breaking down the key concepts and processes involved.\n\n\nWhat is a Decision Tree?\nA decision tree is a flowchart-like structure where:\n\nEach internal node represents a “decision” based on the value of a feature.\nEach branch represents the outcome of a decision.\nEach leaf node represents a class label (the decision made after all features are considered).\n\n\n\nHow Does a Decision Tree Classifier Work?\n\nStarting at the Root Node:\nThe process begins at the root node, which contains the entire dataset. The goal is to split this dataset into subsets that are more homogenous in terms of the target variable (class label).\nChoosing the Best Feature to Split On:\nAt each step, the algorithm selects the feature that best separates the classes. This is done using a metric like Gini impurity or Information Gain.\n\nGini Impurity: Measures the frequency at which any element of the dataset would be misclassified. It’s calculated as: \\[\nGini = 1 - \\sum_{i=1}^{n} (p_i)^2\n\\] where \\(p_i\\) is the probability of an element being classified into a particular class.\nInformation Gain: Measures the reduction in entropy or impurity before and after a split. It’s calculated as: \\[\n\\text{Information Gain} = \\text{Entropy(before split)} - \\sum_{i=1}^{k} \\frac{n_i}{n} \\times \\text{Entropy}(i)\n\\] where \\(n_i\\) is the number of instances in the \\(i\\)-th subset.\n\nSplitting the Node:\nOnce the best feature is chosen, the dataset is split into subsets based on the feature’s values. Each subset forms a new node.\nRepeating the Process:\nThe algorithm recursively repeats the process for each new node, selecting the best feature to split on and creating further branches, until one of the stopping criteria is met:\n\nAll instances in a node belong to the same class.\nNo more features are left to split on.\nA pre-defined maximum tree depth is reached.\n\nMaking Predictions:\nAfter the tree is built, it can be used to classify new instances. Starting from the root, the instance is evaluated against the decision rules at each node, following the branches until it reaches a leaf node, which gives the predicted class.\n\n\n\nExample of a Decision Tree Classifier\nConsider a simple example where we want to classify whether a person will buy a computer based on their age and income.\n\nTraining Data\n\n\n\nAge\nIncome\nBuys Computer\n\n\n\n\n&lt;30\nHigh\nNo\n\n\n&lt;30\nHigh\nNo\n\n\n31-40\nHigh\nYes\n\n\n&gt;40\nMedium\nYes\n\n\n&gt;40\nLow\nNo\n\n\n&gt;40\nLow\nYes\n\n\n31-40\nLow\nYes\n\n\n&lt;30\nMedium\nNo\n\n\n&lt;30\nLow\nYes\n\n\n&gt;40\nMedium\nYes\n\n\n&lt;30\nMedium\nYes\n\n\n31-40\nMedium\nYes\n\n\n\n\n\nBuilding the Tree\n\nRoot Node:\n\nCalculate the Gini impurity for the entire dataset.\nSelect the feature (Age or Income) that provides the best split based on Gini impurity or Information Gain.\n\nFirst Split:\n\nSuppose Age is selected. The data is split into three groups: &lt;30, 31-40, and &gt;40.\n\nFurther Splits:\n\nFor each age group, calculate the Gini impurity or Information Gain again and split further based on Income.\n\n\n\n\nResulting Tree\n          [Age]\n         /  |   \\\n      &lt;30 31-40 &gt;40\n      /     |     \\\n  [Income]  Yes  [Income]\n    /  \\          /   \\\n  High Medium    Medium Low\n   No   Yes       Yes   No\n\n\n\nAdvantages of Decision Trees\n\nSimple to Understand: They are easy to visualize and interpret.\nNon-linear Relationships: Can capture non-linear relationships between features and the target variable.\nLittle Data Preparation: Requires little data preprocessing compared to other algorithms.\n\n\n\nDisadvantages of Decision Trees\n\nOverfitting: Trees can become very complex and overfit the training data, especially if not pruned.\nUnstable: Small changes in the data can lead to different splits and thus different trees.\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\nYou may also like:\n\n\n\nWhat is Supervised Machine Learning"
  },
  {
    "objectID": "posts/time-series-analysis/index.html",
    "href": "posts/time-series-analysis/index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 2 of our Time Series Analysis Series, Part 1 introduces you to what time series is. The link to part 1 is here\nAs name suggest its analysis of the time series data to identify the patterns in it. I will briefly explain the different techniques and test for time series data analysis.\n\n\nWe have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.\n[ad]"
  },
  {
    "objectID": "posts/time-series-analysis/index.html#time-series-analysis",
    "href": "posts/time-series-analysis/index.html#time-series-analysis",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 2 of our Time Series Analysis Series, Part 1 introduces you to what time series is. The link to part 1 is here\nAs name suggest its analysis of the time series data to identify the patterns in it. I will briefly explain the different techniques and test for time series data analysis.\n\n\nWe have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.\n[ad]"
  },
  {
    "objectID": "posts/time-series-analysis/index.html#decomposition-of-time-series",
    "href": "posts/time-series-analysis/index.html#decomposition-of-time-series",
    "title": "Time Series Analysis",
    "section": "Decomposition of Time Series",
    "text": "Decomposition of Time Series\nTime series decomposition helps to deconstruct the time series into several component like trend and seasonality for better visualization of its characteristics. Using time-series decomposition makes it easier to quickly identify a changing mean or variation in the data\n\n\n\nDecomposition_of_Time_Series"
  },
  {
    "objectID": "posts/time-series-analysis/index.html#stationary-data",
    "href": "posts/time-series-analysis/index.html#stationary-data",
    "title": "Time Series Analysis",
    "section": "Stationary Data",
    "text": "Stationary Data\nFor accurate analysis and forecasting trend and seasonality is removed from the time series and converted it into stationary series. Time series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time.\n\n\n\nStationarity"
  },
  {
    "objectID": "posts/time-series-analysis/index.html#test-for-stationarity",
    "href": "posts/time-series-analysis/index.html#test-for-stationarity",
    "title": "Time Series Analysis",
    "section": "Test for Stationarity",
    "text": "Test for Stationarity\nEasy way is to look at the plot and look for any obvious trend or seasonality. While working on real world data we can also use more sophisticated methods like rolling statistic and Augmented Dickey Fuller test to check stationarity of the data.\n\nRolling Statistics\nIn rolling statistics technique we define a size of window to calculate the mean and standard deviation throughout the series. For stationary series mean and standard deviation shouldn’t change with time.\n\n\nAugmented Dickey Fuller (ADF) Test\nI won’t go into the details of how this test works. I will concentrate more on how to interpret the result of this test to determine the stationarity of the series. ADF test will return ‘p-value’ and ‘Test Statistics’ output values.\n\np-value &gt; 0.05: non-stationary.\np-value &lt;= 0.05: stationary.\nTest statistics: More negative this value more likely we have stationary series. Also, this value should be smaller than critical values(1%, 5%, 10%). For e.g. If test statistic is smaller than the 5% critical values, then we can say with 95% confidence that this is a stationary series\n\n[ad]"
  },
  {
    "objectID": "posts/time-series-analysis/index.html#convert-non-stationary-data-to-stationary-data",
    "href": "posts/time-series-analysis/index.html#convert-non-stationary-data-to-stationary-data",
    "title": "Time Series Analysis",
    "section": "Convert Non-Stationary Data to Stationary Data",
    "text": "Convert Non-Stationary Data to Stationary Data\nAccounting for the time series data characteristics like trend and seasonality is called as making data stationary. So by making the mean and variance of the time series constant, we will get the stationary data. Below are the few technique used for the same…\n\nDifferencing\nDifferencing technique helps to remove the trend and seasonality from time series data. Differencing is performed by subtracting the previous observation from the current observation. The differenced data will contain one less data point than original data. So differencing actually reduces the number of observations and stabilize the mean of a time series.\n\\[d = t - t0\\]\nAfter performing the differencing it’s recommended to plot the data and visualize the change. In case there is not sufficient improvement you can perform second order or even third order differencing.\n\n\nTransformation\nA simple but often effective way to stabilize the variance across time is to apply a power transformation to the time series. Log, square root, cube root are most commonly used transformation techniques. Most of the time you can pick the type of growth of the time series and accordingly choose the transformation method. For. e.g. A time series that has a quadratic growth trend can be made linear by taking the square root. In case differencing don’t work, you may first want to use one of above transformation technique to remove the variation from the series.\n\n\n\nLog_Transformation\n\n\n\n\nMoving Average\nIn moving averages technique, a new series is created by taking the averages of data points from original series. In this technique we can use two or more raw data points to calculate the average. This is also called as ‘window width (w)’. Once window width is decided, averages are calculated from start to the end for each set of w consecutive values, hence the name moving averages. It can also be used for time series forecasting.\n\n\n\nMoving_Average\n\n\n\nWeighted Moving Averages(WMA)\nWMA is a technical indicator that assigns a greater weighting to the most recent data points, and less weighting to data points in the distant past. The WMA is obtained by multiplying each number in the data set by a predetermined weight and summing up the resulting values. There can be many techniques for assigning weights. A popular one is exponentially weighted moving average where weights are assigned to all the previous values with a decay factor.\n\n\nCentered Moving Averages(CMS)\nIn a centered moving average, the value of the moving average at time t is computed by centering the window around time t and averaging across the w values within the window. For example, a center moving average with a window of 3 would be calculated as\n\\[CMA(t) = mean(t-1, t, t+1)\\]\nCMA is very useful for visualizing the time series data\n\n\nTrailing Moving Averages(TMA)\nIn trailing moving average, instead of averaging over a window that is centered around a time period of interest, it simply takes the average of the last w values. For example, a trailing moving average with a window of 3 would be calculated as:\n\\[TMA(t) = mean(t-2, t-1, t)\\]\nTMA are useful for forecasting.\n[ad]"
  },
  {
    "objectID": "posts/time-series-analysis/index.html#correlation",
    "href": "posts/time-series-analysis/index.html#correlation",
    "title": "Time Series Analysis",
    "section": "Correlation",
    "text": "Correlation\n\nMost important point about values in time series is its dependence on the previous values.\nWe can calculate the correlation for time series observations with previous time steps, called as lags.\nBecause the correlation of the time series observations is calculated with values of the same series at previous times, this is called an autocorrelation or serial correlation.\nTo understand it better lets consider the example of fish prices. We will use below notation to represent the fish prices.\n\n\\(P(t)\\)= Fish price of today\n\\(P(t-1)\\) = Fish price of last month\n\\(P(t-2)\\) =Fish price of last to last month\n\nTime series of fish prices can be represented as \\(P(t-n),..... P(t-3), P(t-2),P(t-1), P(t)\\)\nSo if we have fish prices for last few months then it will be easy for us to predict the fish price for today (Here we are ignoring all other external factors that may affect the fish prices)\n\nAll the past and future data points are related in time series and ACF and PACF functions help us to determine correlation in it.\n\nAuto Correlation Function (ACF)\n\nACF tells you how correlated points are with each other, based on how many time steps they are separated by.\nNow to understand it better lets consider above example of fish prices. Let’s try to find the correlation between fish price for current month P(t) and two months ago P(t-2). Important thing to note that, fish price of two months ago can directly affect the today’s fish price or it can indirectly affect the fish price through last months price P(t-1)\nSo ACF consider the direct as well indirect effect between the points while determining the correlation\n\n\n\nPartial Auto Correlation Function (PACF)\n\nUnlike ACF, PACF only consider the direct effect between the points while determining the correlation\nIn case of above fish price example PACF will determine the correlation between fish price for current month P(t) and two months ago P(t-2) by considering only P(t) and P(t-2) and ignoring P(t-1)\n\n[ad]"
  },
  {
    "objectID": "posts/bffill-and-ffill-for-handling-missing-data/index.html",
    "href": "posts/bffill-and-ffill-for-handling-missing-data/index.html",
    "title": "Handling Missing Data in Pandas, When to Use bfill and ffill Methods",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThe bfill (backward fill) and ffill (forward fill) methods are used in data analysis and manipulation, particularly for handling missing data in pandas DataFrames or Series. Here’s a detailed explanation of when and how to use each method:\n\nffill (Forward Fill)\nDescription: ffill propagates the last valid observation forward to the next valid one.\nUse Cases:\n\nTime Series Data: When dealing with time series data, if a certain value is missing, it might be logical to assume that the last observed value remains in effect until a new value is observed. For example, if you have daily stock prices and some days are missing, you might want to fill those missing days with the last known stock price.\nData Smoothing: In some cases, for smoothing purposes, forward filling can help maintain a continuous data series without introducing significant bias.\nSurvey Data: In survey data, if a respondent skips a question but answers the following ones, you might assume their previous answer holds until they provide a new one.\n\nExample:\nimport pandas as pd\n\n# Sample data with missing values\ndata = pd.Series([1, 2, None, 4, None, None, 7])\nfilled_data = data.ffill()\nprint(filled_data)\nOutput:\n0    1.0\n1    2.0\n2    2.0\n3    4.0\n4    4.0\n5    4.0\n6    7.0\ndtype: float64\n\n\n\n\n\n\nbfill (Backward Fill)\nDescription: bfill propagates the next valid observation backward to fill the missing values.\nUse Cases:\n\nPredictive Models: In certain predictive models, you might want to assume the next known value affects the previous unknown period. This can be useful when the data naturally reflects future events impacting current states.\nData Preparation: In data preparation, backward filling can sometimes be used to prepare data for algorithms that require no missing values, ensuring that any gap is filled with the next available data point.\nInterim Reporting: When generating interim reports, you might use backward fill to assume that future values (like projected sales or stock levels) should be filled backward to reflect estimates.\n\nExample:\nimport pandas as pd\n\n# Sample data with missing values\ndata = pd.Series([1, 2, None, 4, None, None, 7])\nfilled_data = data.bfill()\nprint(filled_data)\nOutput:\n0    1.0\n1    2.0\n2    4.0\n3    4.0\n4    7.0\n5    7.0\n6    7.0\ndtype: float64\n\n\n\n\n\n\nChoosing Between ffill and bfill\n\nContext: Choose based on the context and the logical assumption that fits the nature of your data. If the past influences the present, use ffill. If the future influences the present, use bfill.\nData Patterns: Consider the patterns in your data and what makes sense for your specific analysis or model. Ensure that the method you choose maintains the integrity and meaning of your data.\n\nIn summary, use ffill to carry forward the last known value to fill missing data points and bfill to use the next known value to fill previous missing data points. Both methods are useful for maintaining data continuity and preparing datasets for analysis.\n\n\n\n\n\n\nYou may also like:\n\n\n\nHandling Missing Data\n\n \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/anova-for-feature-selection/75_bonus.html",
    "href": "posts/anova-for-feature-selection/75_bonus.html",
    "title": "ANOVA for Feature Selection",
    "section": "",
    "text": "Photo By DATAIDEA"
  },
  {
    "objectID": "posts/anova-for-feature-selection/75_bonus.html#anova-for-feature-selection",
    "href": "posts/anova-for-feature-selection/75_bonus.html#anova-for-feature-selection",
    "title": "ANOVA for Feature Selection",
    "section": "ANOVA for Feature Selection",
    "text": "ANOVA for Feature Selection\nIn this notebook, we demonstrate how ANOVA (Analysis of Variance) can be used to identify better features for machine learning models. We’ll use the Fantasy Premier League (FPL) dataset to show how ANOVA helps in selecting features that best differentiate categories.\n# Uncomment the line below if you need to install the dataidea package\n# !pip install -U dataidea\nFirst, we’ll import the necessary packages: scipy for performing ANOVA, dataidea for loading the FPL dataset, and SelectKBest from scikit-learn for univariate feature selection based on statistical tests.\nimport scipy as sp\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport dataidea as di\nLet’s load the FPL dataset and preview the top 5 rows.\n# Load FPL dataset\nfpl = di.loadDataset('fpl') \n\n# Preview the top 5 rows\nfpl.head(n=5)\n\n\n\n\n\n\n\n\nFirst_Name\n\n\nSecond_Name\n\n\nClub\n\n\nGoals_Scored\n\n\nAssists\n\n\nTotal_Points\n\n\nMinutes\n\n\nSaves\n\n\nGoals_Conceded\n\n\nCreativity\n\n\nInfluence\n\n\nThreat\n\n\nBonus\n\n\nBPS\n\n\nICT_Index\n\n\nClean_Sheets\n\n\nRed_Cards\n\n\nYellow_Cards\n\n\nPosition\n\n\n\n\n\n\n0\n\n\nBruno\n\n\nFernandes\n\n\nMUN\n\n\n18\n\n\n14\n\n\n244\n\n\n3101\n\n\n0\n\n\n36\n\n\n1414.9\n\n\n1292.6\n\n\n1253\n\n\n36\n\n\n870\n\n\n396.2\n\n\n13\n\n\n0\n\n\n6\n\n\nMID\n\n\n\n\n1\n\n\nHarry\n\n\nKane\n\n\nTOT\n\n\n23\n\n\n14\n\n\n242\n\n\n3083\n\n\n0\n\n\n39\n\n\n659.1\n\n\n1318.2\n\n\n1585\n\n\n40\n\n\n880\n\n\n355.9\n\n\n12\n\n\n0\n\n\n1\n\n\nFWD\n\n\n\n\n2\n\n\nMohamed\n\n\nSalah\n\n\nLIV\n\n\n22\n\n\n6\n\n\n231\n\n\n3077\n\n\n0\n\n\n41\n\n\n825.7\n\n\n1056.0\n\n\n1980\n\n\n21\n\n\n657\n\n\n385.8\n\n\n11\n\n\n0\n\n\n0\n\n\nMID\n\n\n\n\n3\n\n\nHeung-Min\n\n\nSon\n\n\nTOT\n\n\n17\n\n\n11\n\n\n228\n\n\n3119\n\n\n0\n\n\n36\n\n\n1049.9\n\n\n1052.2\n\n\n1046\n\n\n26\n\n\n777\n\n\n315.2\n\n\n13\n\n\n0\n\n\n0\n\n\nMID\n\n\n\n\n4\n\n\nPatrick\n\n\nBamford\n\n\nLEE\n\n\n17\n\n\n11\n\n\n194\n\n\n3052\n\n\n0\n\n\n50\n\n\n371.0\n\n\n867.2\n\n\n1512\n\n\n26\n\n\n631\n\n\n274.6\n\n\n10\n\n\n0\n\n\n3\n\n\nFWD\n\n\n\n\n\nANOVA helps us determine if there’s a significant difference between the means of different groups. We use it to select features that best show the difference between categories. Features with higher F-statistics are preferred.\n\nANOVA for Goals Scored\nWe will create groups of goals scored by each player position (forwards, midfielders, defenders, and goalkeepers) and run an ANOVA test.\n# Create groups of goals scored for each player position\nforwards_goals = fpl[fpl.Position == 'FWD']['Goals_Scored']\nmidfielders_goals = fpl[fpl.Position == 'MID']['Goals_Scored']\ndefenders_goals = fpl[fpl.Position == 'DEF']['Goals_Scored']\ngoalkeepers_goals = fpl[fpl.Position == 'GK']['Goals_Scored']\n\n# Perform the ANOVA test for the groups\nf_statistic, p_value = sp.stats.f_oneway(forwards_goals, midfielders_goals, defenders_goals, goalkeepers_goals)\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\nF-statistic: 33.281034594400445\np-value: 3.9257634156019246e-20\nWe observe an F-statistic of 33.281 and a p-value of 3.926e-20, indicating a significant difference at multiple confidence levels.\n\n\nANOVA for Assists\nNext, we’ll create groups for assists and run an ANOVA test.\n# Create groups of assists for each player position\nforwards_assists = fpl[fpl.Position == 'FWD']['Assists']\nmidfielders_assists = fpl[fpl.Position == 'MID']['Assists']\ndefenders_assists = fpl[fpl.Position == 'DEF']['Assists']\ngoalkeepers_assists = fpl[fpl.Position == 'GK']['Assists']\n\n# Perform the ANOVA test for the groups\nf_statistic, p_value = sp.stats.f_oneway(forwards_assists, midfielders_assists, defenders_assists, goalkeepers_assists)\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\nF-statistic: 19.263717036430815\np-value: 5.124889288362087e-12\nWe observe an F-statistic of 19.264 and a p-value of 5.125e-12, again indicating significance.\n\n\nComparing Results\nBoth features show significant F-statistics, but goals scored has a higher value, indicating it is a better feature for differentiating player positions.\n\n\nUsing SelectKBest for Feature Selection\nWe can also use SelectKBest from scikit-learn to automate this process.\n# Use scikit-learn's SelectKBest (with f_classif)\ntest = SelectKBest(score_func=f_classif, k=1)\n\n# Fit the model to the data\nfit = test.fit(fpl[['Goals_Scored', 'Assists']], fpl.Position)\n\n# Get the F-statistics\nscores = fit.scores_\n\n# Select the best feature\nfeatures = fit.transform(fpl[['Goals_Scored', 'Assists']])\n\n# Get the indices of the selected features (optional)\nselected_indices = test.get_support(indices=True)\n\n# Print indices and scores\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\nFeature Scores:  [33.28103459 19.26371704]\nSelected Features Indices:  [0]\nThe 0th feature (Goals Scored) is selected as the best feature based on the F-statistics.\n\n\n\n\n\n\n\nDon’t Miss Any Updates!\n\n\n\nTo be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\n\nSummary\nIn this notebook, we demonstrated how to use ANOVA for feature selection in the Fantasy Premier League dataset. By comparing the F-statistics of different features, we identified that ‘Goals Scored’ is a more significant feature than ‘Assists’ for differentiating player positions. Using SelectKBest from scikit-learn, we confirmed that ‘Goals Scored’ is the best feature among the two. This method can be applied to other datasets and features to enhance the performance of machine learning models."
  },
  {
    "objectID": "posts/sklearn-unsupervised-learning/index.html",
    "href": "posts/sklearn-unsupervised-learning/index.html",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "",
    "text": "Photo by DATAIDEA\nThe following topics are covered in this tutorial:\nLet’s install the required libraries."
  },
  {
    "objectID": "posts/sklearn-unsupervised-learning/index.html#introduction-to-unsupervised-learning",
    "href": "posts/sklearn-unsupervised-learning/index.html#introduction-to-unsupervised-learning",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "Introduction to Unsupervised Learning",
    "text": "Introduction to Unsupervised Learning\nUnsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels. Unsupervised learning is generally use to discover patterns in data and reduce high-dimensional data to fewer dimensions. Here’s how unsupervised learning fits into the landscape of machine learning algorithms(source):\n\nHere are the topics in machine learning that we’re studying in this course (source):\n\nHere’s a cheatsheet to help you decide which model to pick for a given problem. Can you identify the unsupervised learning algorithms?\n\nHere is a full list of unsupervised learning algorithms available in Scikit-learn: https://scikit-learn.org/stable/unsupervised_learning.html"
  },
  {
    "objectID": "posts/sklearn-unsupervised-learning/index.html#clustering",
    "href": "posts/sklearn-unsupervised-learning/index.html#clustering",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "Clustering",
    "text": "Clustering\nClustering is the process of grouping objects from a dataset such that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (Wikipedia). Scikit-learn offers several clustering algorithms. You can learn more about them here: https://scikit-learn.org/stable/modules/clustering.html\nHere is a visual representation of clustering:\n\nHere are some real-world applications of clustering:\n\nCustomer segmentation\nProduct recommendation\nFeature engineering\nAnomaly/fraud detection\nTaxonomy creation\n\nWe’ll use the Iris flower dataset to study some of the clustering algorithms available in scikit-learn. It contains various measurements for 150 flowers belonging to 3 different species.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nsns.set_style('darkgrid')\n%matplotlib inline\nLet’s load the popular iris and penguin datasets. These datasets are already built in seaborn\n# load the iris dataset\niris_df = sns.load_dataset('iris')\niris_df.head()\n\n\n\n\n\n\n\n\nsepal_length\n\n\nsepal_width\n\n\npetal_length\n\n\npetal_width\n\n\nspecies\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\nsetosa\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\nsetosa\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n\n# load the penguin dataset\nsns.get_dataset_names()\nping_df = sns.load_dataset('penguins')\nping_df.head()\n\n\n\n\n\n\n\n\nspecies\n\n\nisland\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nflipper_length_mm\n\n\nbody_mass_g\n\n\nsex\n\n\n\n\n\n\n0\n\n\nAdelie\n\n\nTorgersen\n\n\n39.1\n\n\n18.7\n\n\n181.0\n\n\n3750.0\n\n\nMale\n\n\n\n\n1\n\n\nAdelie\n\n\nTorgersen\n\n\n39.5\n\n\n17.4\n\n\n186.0\n\n\n3800.0\n\n\nFemale\n\n\n\n\n2\n\n\nAdelie\n\n\nTorgersen\n\n\n40.3\n\n\n18.0\n\n\n195.0\n\n\n3250.0\n\n\nFemale\n\n\n\n\n3\n\n\nAdelie\n\n\nTorgersen\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n4\n\n\nAdelie\n\n\nTorgersen\n\n\n36.7\n\n\n19.3\n\n\n193.0\n\n\n3450.0\n\n\nFemale\n\n\n\n\n\nsns.scatterplot(data=ping_df, x='bill_length_mm', y='bill_depth_mm', hue='species')\nplt.title('Penguin Bill Depth against Bill Length per Species')\nplt.ylabel('Bill Depth')\nplt.xlabel('Bill Length')\nplt.show()\n\n\n\npng\n\n\nsns.scatterplot(data=iris_df, x='sepal_length', y='petal_length', hue='species')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\npng\n\n\nWe’ll attempt to cluster observations using numeric columns in the data.\nnumeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nX = iris_df[numeric_cols]\nX.head()\n\n\n\n\n\n\n\n\nsepal_length\n\n\nsepal_width\n\n\npetal_length\n\n\npetal_width\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n\n\n\n\n\n\n\n\n\nK Means Clustering\nThe K-means algorithm attempts to classify objects into a pre-determined number of clusters by finding optimal central points (called centroids) for each cluster. Each object is classifed as belonging the cluster represented by the closest centroid.\n\nHere’s how the K-means algorithm works:\n\nPick K random objects as the initial cluster centers.\nClassify each object into the cluster whose center is closest to the point.\nFor each cluster of classified objects, compute the centroid (mean).\nNow reclassify each object using the centroids as cluster centers.\nCalculate the total variance of the clusters (this is the measure of goodness).\nRepeat steps 1 to 6 a few more times and pick the cluster centers with the lowest total variance.\n\nHere’s a video showing the above steps:\n\n \n\nLet’s apply K-means clustering to the Iris dataset.\nfrom sklearn.cluster import KMeans\nmodel = KMeans(n_clusters=3, random_state=42)\nmodel.fit(X)\n\n\n\nKMeans(n_clusters=3, random_state=42)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  KMeans?Documentation for KMeansiFitted\n\nKMeans(n_clusters=3, random_state=42)\n\n\n\n\n\nWe can check the cluster centers for each cluster.\nmodel.cluster_centers_\narray([[ 6.85384615e+00,  3.07692308e+00,  5.71538462e+00,\n         2.05384615e+00, -8.88178420e-16],\n       [ 5.00600000e+00,  3.42800000e+00,  1.46200000e+00,\n         2.46000000e-01,  1.00000000e+00],\n       [ 5.88360656e+00,  2.74098361e+00,  4.38852459e+00,\n         1.43442623e+00,  2.00000000e+00]])\nWe can now classify points using the model.\n# making predictions on X (clustering)\npreds = model.predict(X)\n# assign each row to their cluster\nX['clusters'] = preds\n# looking at some samples\nX.sample(n=5)\n\n\n\n\n\n\n\n\nsepal_length\n\n\nsepal_width\n\n\npetal_length\n\n\npetal_width\n\n\nclusters\n\n\n\n\n\n\n88\n\n\n5.6\n\n\n3.0\n\n\n4.1\n\n\n1.3\n\n\n2\n\n\n\n\n24\n\n\n4.8\n\n\n3.4\n\n\n1.9\n\n\n0.2\n\n\n1\n\n\n\n\n131\n\n\n7.9\n\n\n3.8\n\n\n6.4\n\n\n2.0\n\n\n0\n\n\n\n\n81\n\n\n5.5\n\n\n2.4\n\n\n3.7\n\n\n1.0\n\n\n2\n\n\n\n\n132\n\n\n6.4\n\n\n2.8\n\n\n5.6\n\n\n2.2\n\n\n0\n\n\n\n\n\nLet’s use seaborn and pyplot to visualize the clusters\nsns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds)\ncenters_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\nplt.plot(centers_x, centers_y, 'xb')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\npng\n\n\nAs you can see, K-means algorithm was able to classify (for the most part) different specifies of flowers into separate clusters. Note that we did not provide the “species” column as an input to KMeans.\nWe can check the “goodness” of the fit by looking at model.inertia_, which contains the sum of squared distances of samples to their closest cluster center. Lower the inertia, better the fit.\nmodel.inertia_\n78.8556658259773\n\n\n\n\n\nLet’s try creating 6 clusters.\nmodel = KMeans(n_clusters=6, random_state=42)\n# fitting the model\nmodel.fit(X)\n# making predictions on X (clustering)\npreds = model.predict(X)\n# assign each row to their cluster\nX['clusters'] = preds\n# looking at some samples\nX.sample(n=5)\n\n\n\n\n\n\n\n\nsepal_length\n\n\nsepal_width\n\n\npetal_length\n\n\npetal_width\n\n\nclusters\n\n\n\n\n\n\n13\n\n\n4.3\n\n\n3.0\n\n\n1.1\n\n\n0.1\n\n\n1\n\n\n\n\n20\n\n\n5.4\n\n\n3.4\n\n\n1.7\n\n\n0.2\n\n\n5\n\n\n\n\n127\n\n\n6.1\n\n\n3.0\n\n\n4.9\n\n\n1.8\n\n\n0\n\n\n\n\n40\n\n\n5.0\n\n\n3.5\n\n\n1.3\n\n\n0.3\n\n\n5\n\n\n\n\n70\n\n\n5.9\n\n\n3.2\n\n\n4.8\n\n\n1.8\n\n\n0\n\n\n\n\n\nLet’s visualize the clusters\nsns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds)\ncenters_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\nplt.plot(centers_x, centers_y, 'xb')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\npng\n\n\n# Let's calculate the new model inertia\nmodel.inertia_\n50.560990643274856\n\n\n\n\n\n\n\nSo, what number of clusters is good enough?\nIn most real-world scenarios, there’s no predetermined number of clusters. In such a case, you can create a plot of “No. of clusters” vs “Inertia” to pick the right number of clusters.\noptions = range(2, 11)\ninertias = []\n\nfor n_clusters in options:\n    model = KMeans(n_clusters, random_state=42).fit(X)\n    inertias.append(model.inertia_)\n\nplt.title(\"No. of clusters vs. Inertia\")\nplt.plot(options, inertias, '-o')\nplt.xlabel('No. of clusters (K)')\nplt.ylabel('Inertia')\nText(0, 0.5, 'Inertia')\n\n\n\npng\n\n\nThe chart is creates an “elbow” plot, and you can pick the number of clusters beyond which the reduction in inertia decreases sharply.\nMini Batch K Means: The K-means algorithm can be quite slow for really large dataset. Mini-batch K-means is an iterative alternative to K-means that works well for large datasets. Learn more about it here: https://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans\n\nEXERCISE: Perform clustering on the Mall customers dataset on Kaggle. Study the segments carefully and report your observations."
  },
  {
    "objectID": "posts/sklearn-unsupervised-learning/index.html#summary-and-references",
    "href": "posts/sklearn-unsupervised-learning/index.html#summary-and-references",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "Summary and References",
    "text": "Summary and References\n\nThe following topics were covered in this tutorial:\n\nOverview of unsupervised learning algorithms in Scikit-learn\nClustering algorithms: K Means, DBScan, Hierarchical clustering etc.\nDimensionality reduction (PCA) and manifold learning (t-SNE)\n\nCheck out these resources to learn more:\n\nhttps://www.coursera.org/learn/machine-learning\nhttps://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\nhttps://scikit-learn.org/stable/unsupervised_learning.html\nhttps://scikit-learn.org/stable/modules/clustering.html"
  },
  {
    "objectID": "posts/sklearn-unsupervised-learning/index.html#credit",
    "href": "posts/sklearn-unsupervised-learning/index.html#credit",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "Credit",
    "text": "Credit\n\nDo you seriously want to learn Programming and Data Analysis with Python?\nIf you’re serious about learning Programming, Data Analysis with Python and getting prepared for Data Science roles, I highly encourage you to enroll in my Programming for Data Science Course, which I’ve taught to hundreds of students. Don’t waste your time following disconnected, outdated tutorials\nMy Complete Programming for Data Science Course has everything you need in one place.\nThe course offers:\n\nDuration: Usually 3-4 months\nSessions: Four times a week (one on one)\nLocation: Online or/and at UMF House, Sir Apollo Kagwa Road\n\nWhat you’l learn:\n\nFundamentals of programming\nData manipulation and analysis\nVisualization techniques\nIntroduction to machine learning\nDatabase Management with SQL (optional)\nWeb Development with Django (optional)\n\nBest\nJuma Shafara\nData Scientist, Instructor\njumashafara0@gmail.com / dataideaorg@gmail.com\n+256701520768 / +256771754118"
  },
  {
    "objectID": "posts/programmer-puns/index.html",
    "href": "posts/programmer-puns/index.html",
    "title": "Puns Only Programmers Will Get",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nWhat are the best puns? Definitely the ones only a specific group of people will understand and enjoy. Especially in the era we’re living in, we need to feel like a family and share “inside” jokes with one another. So, let’s take a look at some of the best programming puns we came upon during the pre-summer times.\n\nWhy do programmers like dark mode?\n\nBecause light attracts bugs.\n\nI used to work as a programmer for autocorrect…\n\nThen they fried me for no raisin.\n\nWhat do NASA programmers do on the weekends?\n\nThey hit the space bar.\n\nWhy did the programmer quit his job?\n\nBecause he didn’t get arrays.\n\nWhat was the SNES programmers’ favorite drink?\n\nSprite\n\nWhat do programmers do when they’re hungry?\n\nThey grab a byte.\n\nWhy couldn’t the programmer dance to the song?\n\nBecause he didn’t get the… algo-rhythm…\n\nWhat you call it when computer programmers make fun of each other?\n\nCyber boolean…\n\nI am now a successful programmer…\n\nBut back in the days I was a noobgrammer.\n\nWhy do programmers always mix up Halloween and Christmas?\n\nBecause Oct 31 equals Dec 25.\n\nWhy did the programmer get a huge telephone bill?\n\nBecause his program was CALLING a lot of subroutines.\n\nWhat do Spanish programmers code in?\n\nSí ++\n\nWhich way did the programmer go?\n\nHe went data way!\n\nWhy was the programmer always running into walls?\n\nHe couldn’t see sharp.\n\nHow programmers curse?\n\nOh shift!\n\nWhy do programmers make good politicians?\n\nTheir goto is to switch statements.\n\nHow did the programmer lose weight?\n\nHe switched to a byte-sized diet.\n\nI almost bought a huge library out of old computer programming books…\n\nBut the ascii price was way too high.\n\nWhat’s a Jedi’s favorite programming language?\n\nJabbaScript…\n\n\n\nWant to learn programming and become an expert?\nIf you’re serious about learning Programming and getting a job as a Developer, I highly encourage you to enroll in my programming courses for Python, JavaScript, Data Science and Web Development.\nDon’t waste your time following disconnected, outdated tutorials. I can teach you all that you need to kickstart your career.\nContact me at +256771754118 or jumashafara@proton.me\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/budget-ml-deploy-options/index.html",
    "href": "posts/budget-ml-deploy-options/index.html",
    "title": "Budget-Friendly Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models doesn’t have to break the bank. If you’re working within a budget, there are several cost-effective options available that still provide robust capabilities. In this blog, we’ll explore some of the best budget-friendly deployment strategies for machine learning models.\n\n\nServerless computing is a great way to minimize costs because you only pay for what you use, without worrying about managing infrastructure. Here are some economical serverless options:\nAWS Lambda:\n\nCost: Pay-per-request pricing.\nPros: Automatically scales with demand, integrates well with other AWS services.\nCons: Limited to short-duration tasks and smaller memory limits.\n\nGoogle Cloud Functions:\n\nCost: Pay-per-use based on the number of invocations, compute time, and memory allocated.\nPros: Excellent for event-driven architectures, integrates smoothly with other Google Cloud services.\nCons: Similar constraints on execution time and memory as AWS Lambda.\n\nAzure Functions:\n\nCost: Pay-per-execution pricing model.\nPros: Scalable and integrates with a wide range of Azure services.\nCons: Limited by the maximum execution duration and resource allocation.\n\n\n\n\nContainers can be a cost-effective way to deploy models by ensuring efficient use of resources and consistent performance across different environments.\nDocker:\n\nCost: Free to use; additional costs come from the underlying infrastructure you deploy it on.\nPros: Lightweight, portable, and scalable. Works well with various cloud and on-premises environments.\nCons: Requires some setup and management expertise.\n\nKubernetes (K8s):\n\nCost: Free, but incurs costs related to the infrastructure on which it runs.\nPros: Manages containerized applications at scale, great for complex deployments.\nCons: More complex setup and management compared to Docker alone.\n\n\n\n\nPaaS solutions can reduce operational overhead and are generally cost-effective for small to medium-scale deployments.\nGoogle App Engine:\n\nCost: Offers a free tier and then pay-as-you-go pricing.\nPros: Fully managed, scalable service for web applications and APIs.\nCons: Less control over the underlying infrastructure.\n\nHeroku:\n\nCost: Free tier available, with affordable pricing plans for higher usage.\nPros: Easy to use, supports multiple programming languages, and integrates with various databases.\nCons: Might become expensive as you scale up.\n\n\n\n\nIf you need more control over your deployment environment but still want to keep costs low, consider budget-friendly cloud services:\nDigitalOcean:\n\nCost: Very competitive pricing with flexible plans starting as low as $5 per month.\nPros: Simple to set up, provides good performance for the price, supports Docker and Kubernetes.\nCons: Fewer advanced features compared to larger cloud providers.\n\nLinode:\n\nCost: Affordable plans starting at $5 per month.\nPros: Good performance, simple pricing structure, supports various deployment configurations.\nCons: Smaller ecosystem and fewer services compared to larger cloud providers.\n\nVultr:\n\nCost: Plans also starting at $5 per month.\nPros: Wide range of instances, including high-frequency compute instances.\nCons: Similar to DigitalOcean and Linode in terms of limitations.\n\n\n\n\nFor specific use cases where you need low latency and reduced data transfer costs, edge deployment can be both effective and economical:\nRaspberry Pi:\n\nCost: Low-cost hardware starting around $35.\nPros: Great for small-scale deployments, supports various machine learning frameworks.\nCons: Limited computational power, suitable for less intensive tasks.\n\nNVIDIA Jetson Nano:\n\nCost: Around $99.\nPros: Affordable, powerful enough for real-time AI tasks on edge devices.\nCons: Higher cost than Raspberry Pi but offers better performance."
  },
  {
    "objectID": "posts/budget-ml-deploy-options/index.html#budget-friendly-options-for-deploying-machine-learning-models",
    "href": "posts/budget-ml-deploy-options/index.html#budget-friendly-options-for-deploying-machine-learning-models",
    "title": "Budget-Friendly Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models doesn’t have to break the bank. If you’re working within a budget, there are several cost-effective options available that still provide robust capabilities. In this blog, we’ll explore some of the best budget-friendly deployment strategies for machine learning models.\n\n\nServerless computing is a great way to minimize costs because you only pay for what you use, without worrying about managing infrastructure. Here are some economical serverless options:\nAWS Lambda:\n\nCost: Pay-per-request pricing.\nPros: Automatically scales with demand, integrates well with other AWS services.\nCons: Limited to short-duration tasks and smaller memory limits.\n\nGoogle Cloud Functions:\n\nCost: Pay-per-use based on the number of invocations, compute time, and memory allocated.\nPros: Excellent for event-driven architectures, integrates smoothly with other Google Cloud services.\nCons: Similar constraints on execution time and memory as AWS Lambda.\n\nAzure Functions:\n\nCost: Pay-per-execution pricing model.\nPros: Scalable and integrates with a wide range of Azure services.\nCons: Limited by the maximum execution duration and resource allocation.\n\n\n\n\nContainers can be a cost-effective way to deploy models by ensuring efficient use of resources and consistent performance across different environments.\nDocker:\n\nCost: Free to use; additional costs come from the underlying infrastructure you deploy it on.\nPros: Lightweight, portable, and scalable. Works well with various cloud and on-premises environments.\nCons: Requires some setup and management expertise.\n\nKubernetes (K8s):\n\nCost: Free, but incurs costs related to the infrastructure on which it runs.\nPros: Manages containerized applications at scale, great for complex deployments.\nCons: More complex setup and management compared to Docker alone.\n\n\n\n\nPaaS solutions can reduce operational overhead and are generally cost-effective for small to medium-scale deployments.\nGoogle App Engine:\n\nCost: Offers a free tier and then pay-as-you-go pricing.\nPros: Fully managed, scalable service for web applications and APIs.\nCons: Less control over the underlying infrastructure.\n\nHeroku:\n\nCost: Free tier available, with affordable pricing plans for higher usage.\nPros: Easy to use, supports multiple programming languages, and integrates with various databases.\nCons: Might become expensive as you scale up.\n\n\n\n\nIf you need more control over your deployment environment but still want to keep costs low, consider budget-friendly cloud services:\nDigitalOcean:\n\nCost: Very competitive pricing with flexible plans starting as low as $5 per month.\nPros: Simple to set up, provides good performance for the price, supports Docker and Kubernetes.\nCons: Fewer advanced features compared to larger cloud providers.\n\nLinode:\n\nCost: Affordable plans starting at $5 per month.\nPros: Good performance, simple pricing structure, supports various deployment configurations.\nCons: Smaller ecosystem and fewer services compared to larger cloud providers.\n\nVultr:\n\nCost: Plans also starting at $5 per month.\nPros: Wide range of instances, including high-frequency compute instances.\nCons: Similar to DigitalOcean and Linode in terms of limitations.\n\n\n\n\nFor specific use cases where you need low latency and reduced data transfer costs, edge deployment can be both effective and economical:\nRaspberry Pi:\n\nCost: Low-cost hardware starting around $35.\nPros: Great for small-scale deployments, supports various machine learning frameworks.\nCons: Limited computational power, suitable for less intensive tasks.\n\nNVIDIA Jetson Nano:\n\nCost: Around $99.\nPros: Affordable, powerful enough for real-time AI tasks on edge devices.\nCons: Higher cost than Raspberry Pi but offers better performance."
  },
  {
    "objectID": "posts/budget-ml-deploy-options/index.html#key-tips-for-budget-friendly-deployment",
    "href": "posts/budget-ml-deploy-options/index.html#key-tips-for-budget-friendly-deployment",
    "title": "Budget-Friendly Options for Deploying Machine Learning Models",
    "section": "Key Tips for Budget-Friendly Deployment",
    "text": "Key Tips for Budget-Friendly Deployment\n\nOptimize Your Model: Smaller and optimized models can reduce computational requirements and, consequently, deployment costs.\nLeverage Free Tiers: Many cloud providers offer free tiers or credits for new users; take advantage of these offers.\nAuto-scaling: Use auto-scaling features to ensure you’re only paying for the resources you need at any given time.\nMonitor and Optimize: Regularly monitor resource usage and optimize configurations to avoid unnecessary costs.\nUse Spot Instances: For non-critical workloads, consider using spot instances, which are significantly cheaper than regular instances.\n\nBy carefully selecting your deployment strategy and optimizing your resources, you can deploy machine learning models effectively without exceeding your budget.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/data-imputation/index.html",
    "href": "posts/data-imputation/index.html",
    "title": "Handling Missing Data",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/data-imputation/index.html#handling-missing-data",
    "href": "posts/data-imputation/index.html#handling-missing-data",
    "title": "Handling Missing Data",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/data-imputation/index.html#introduction",
    "href": "posts/data-imputation/index.html#introduction",
    "title": "Handling Missing Data",
    "section": "Introduction:",
    "text": "Introduction:\nMissing data is a common hurdle in data analysis, impacting the reliability of insights drawn from datasets. Python offers a range of solutions to address this issue, some of which we discussed in the earlier weeks. In this notebook, we look into the top three missing data imputation methods in Python—SimpleImputer, KNNImputer, and IterativeImputer from scikit-learn—providing insights into their functionalities and practical considerations. We’ll explore these essential techniques, using the weather dataset.\n# install the libraries for this demonstration\n! pip install dataidea==0.2.5\nfrom dataidea.packages import *\nfrom dataidea.datasets import loadDataset\nfrom dataidea.packages import * imports for us np, pd, plt, etc. loadDataset allows us to load datasets inbuilt in the dataidea library\nweather = loadDataset('weather')\nweather\n\n\n\n\n\n\n\n\nday\n\n\ntemperature\n\n\nwindspead\n\n\nevent\n\n\n\n\n\n\n0\n\n\n01/01/2017\n\n\n32.0\n\n\n6.0\n\n\nRain\n\n\n\n\n1\n\n\n04/01/2017\n\n\nNaN\n\n\n9.0\n\n\nSunny\n\n\n\n\n2\n\n\n05/01/2017\n\n\n28.0\n\n\nNaN\n\n\nSnow\n\n\n\n\n3\n\n\n06/01/2017\n\n\nNaN\n\n\n7.0\n\n\nNaN\n\n\n\n\n4\n\n\n07/01/2017\n\n\n32.0\n\n\nNaN\n\n\nRain\n\n\n\n\n5\n\n\n08/01/2017\n\n\nNaN\n\n\nNaN\n\n\nSunny\n\n\n\n\n6\n\n\n09/01/2017\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n7\n\n\n10/01/2017\n\n\n34.0\n\n\n8.0\n\n\nCloudy\n\n\n\n\n8\n\n\n11/01/2017\n\n\n40.0\n\n\n12.0\n\n\nSunny\n\n\n\n\n\nweather.isna().sum()\nday            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64\nLet’s demonstrate how to use the top three missing data imputation methods—SimpleImputer, KNNImputer, and IterativeImputer—using the simple weather dataset.\n# select age from the data\ntemp_wind = weather[['temperature', 'windspead']].copy()\ntemp_wind_imputed = temp_wind.copy()"
  },
  {
    "objectID": "posts/data-imputation/index.html#simpleimputer-from-scikit-learn",
    "href": "posts/data-imputation/index.html#simpleimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "SimpleImputer from scikit-learn:",
    "text": "SimpleImputer from scikit-learn:\n\nUsage: SimpleImputer is a straightforward method for imputing missing values by replacing them with a constant, mean, median, or most frequent value along each column.\nPros:\n\nEasy to use and understand.\nCan handle both numerical and categorical data.\nOffers flexibility with different imputation strategies.\n\nCons:\n\nIt doesn’t consider relationships between features.\nMay not be the best choice for datasets with complex patterns of missingness.\n\nExample:\n\nfrom sklearn.impute import SimpleImputer\n\nsimple_imputer = SimpleImputer(strategy='mean')\ntemp_wind_simple_imputed = simple_imputer.fit_transform(temp_wind)\n\ntemp_wind_simple_imputed_df = pd.DataFrame(temp_wind_simple_imputed, columns=temp_wind.columns)\nLet’s have a look at the outcome\ntemp_wind_simple_imputed_df\n\n\n\n\n\n\n\n\ntemperature\n\n\nwindspead\n\n\n\n\n\n\n0\n\n\n32.0\n\n\n6.0\n\n\n\n\n1\n\n\n33.2\n\n\n9.0\n\n\n\n\n2\n\n\n28.0\n\n\n8.4\n\n\n\n\n3\n\n\n33.2\n\n\n7.0\n\n\n\n\n4\n\n\n32.0\n\n\n8.4\n\n\n\n\n5\n\n\n33.2\n\n\n8.4\n\n\n\n\n6\n\n\n33.2\n\n\n8.4\n\n\n\n\n7\n\n\n34.0\n\n\n8.0\n\n\n\n\n8\n\n\n40.0\n\n\n12.0"
  },
  {
    "objectID": "posts/data-imputation/index.html#knnimputer-from-scikit-learn",
    "href": "posts/data-imputation/index.html#knnimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "KNNImputer from scikit-learn:",
    "text": "KNNImputer from scikit-learn:\n\nUsage: KNNImputer imputes missing values using k-nearest neighbors, replacing them with the mean value of the nearest neighbors.\nPros:\n\nConsiders relationships between features, making it suitable for datasets with complex patterns of missingness.\nCan handle both numerical and categorical data.\n\nCons:\n\nComputationally expensive for large datasets.\nRequires careful selection of the number of neighbors (k).\n\nExample:\n\nfrom sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\ntemp_wind_knn_imputed = knn_imputer.fit_transform(temp_wind)\n\ntemp_wind_knn_imputed_df = pd.DataFrame(temp_wind_knn_imputed, columns=temp_wind.columns)\nIf we take a look at the outcome\ntemp_wind_knn_imputed_df\n\n\n\n\n\n\n\n\ntemperature\n\n\nwindspead\n\n\n\n\n\n\n0\n\n\n32.0\n\n\n6.0\n\n\n\n\n1\n\n\n33.0\n\n\n9.0\n\n\n\n\n2\n\n\n28.0\n\n\n7.0\n\n\n\n\n3\n\n\n33.0\n\n\n7.0\n\n\n\n\n4\n\n\n32.0\n\n\n7.0\n\n\n\n\n5\n\n\n33.2\n\n\n8.4\n\n\n\n\n6\n\n\n33.2\n\n\n8.4\n\n\n\n\n7\n\n\n34.0\n\n\n8.0\n\n\n\n\n8\n\n\n40.0\n\n\n12.0"
  },
  {
    "objectID": "posts/data-imputation/index.html#iterativeimputer-from-scikit-learn",
    "href": "posts/data-imputation/index.html#iterativeimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "IterativeImputer from scikit-learn:",
    "text": "IterativeImputer from scikit-learn:\n\nUsage: IterativeImputer models each feature with missing values as a function of other features and uses that estimate for imputation. It iteratively estimates the missing values.\nPros:\n\nTakes into account relationships between features, making it suitable for datasets with complex missing patterns.\nMore robust than SimpleImputer for handling missing data.\n\nCons:\n\nCan be computationally intensive and slower than SimpleImputer.\nRequires careful tuning of model parameters.\n\nExample:\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\niterative_imputer = IterativeImputer()\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\nLet’s take a look at the outcome\ntemp_wind_iterative_imputed_df\n\n\n\n\n\n\n\n\ntemperature\n\n\nwindspead\n\n\n\n\n\n\n0\n\n\n32.000000\n\n\n6.000000\n\n\n\n\n1\n\n\n35.773287\n\n\n9.000000\n\n\n\n\n2\n\n\n28.000000\n\n\n3.321648\n\n\n\n\n3\n\n\n33.042537\n\n\n7.000000\n\n\n\n\n4\n\n\n32.000000\n\n\n6.238915\n\n\n\n\n5\n\n\n33.545118\n\n\n7.365795\n\n\n\n\n6\n\n\n33.545118\n\n\n7.365795\n\n\n\n\n7\n\n\n34.000000\n\n\n8.000000\n\n\n\n\n8\n\n\n40.000000\n\n\n12.000000"
  },
  {
    "objectID": "posts/data-imputation/index.html#datawig",
    "href": "posts/data-imputation/index.html#datawig",
    "title": "Handling Missing Data",
    "section": "Datawig:",
    "text": "Datawig:\nDatawig is a library specifically designed for imputing missing values in tabular data using deep learning models.\n# import datawig\n\n# # Impute missing values\n# df_imputed = datawig.SimpleImputer.complete(weather)\nThese top imputation methods offer different trade-offs in terms of computational complexity, handling of missing data patterns, and ease of use. The choice between them depends on the specific characteristics of the dataset and the requirements of the analysis."
  },
  {
    "objectID": "posts/data-imputation/index.html#homework",
    "href": "posts/data-imputation/index.html#homework",
    "title": "Handling Missing Data",
    "section": "Homework",
    "text": "Homework\n\nTry out these techniques for categorical data"
  },
  {
    "objectID": "posts/data-imputation/index.html#credit",
    "href": "posts/data-imputation/index.html#credit",
    "title": "Handling Missing Data",
    "section": "Credit",
    "text": "Credit\n\nDo you seriously want to learn Programming and Data Analysis with Python?\nIf you’re serious about learning Programming, Data Analysis with Python and getting prepared for Data Science roles, I highly encourage you to enroll in my Programming for Data Science Course, which I’ve taught to hundreds of students. Don’t waste your time following disconnected, outdated tutorials\nMy Complete Programming for Data Science Course has everything you need in one place.\nThe course offers:\n\nDuration: Usually 3-4 months\nSessions: Four times a week (one on one)\nLocation: Online or/and at UMF House, Sir Apollo Kagwa Road\n\nWhat you’l learn:\n\nFundamentals of programming\nData manipulation and analysis\nVisualization techniques\nIntroduction to machine learning\nDatabase Management with SQL (optional)\nWeb Development with Django (optional)\n\nBest\nJuma Shafara\nData Scientist, Instructor\njumashafara0@gmail.com / dataideaorg@gmail.com\n+256701520768 / +256771754118\n\n\n\n\nYou may also like:\n\n\n\nHandling Missing Data in Pandas, When to Use bfill and ffill Methods"
  },
  {
    "objectID": "posts/top-5-operating-systems/index.html",
    "href": "posts/top-5-operating-systems/index.html",
    "title": "To 5 Operating Systems",
    "section": "",
    "text": "Thumbnail\nOperating systems are the backbone of modern computing, providing the fundamental software foundation for computers, smartphones, servers, and more. Here’s a comprehensive overview of five prominent operating systems that have made significant impacts in various domains."
  },
  {
    "objectID": "posts/top-5-operating-systems/index.html#windows",
    "href": "posts/top-5-operating-systems/index.html#windows",
    "title": "To 5 Operating Systems",
    "section": "Windows",
    "text": "Windows\n\nOverview\nDeveloped by Microsoft, Windows is one of the most widely used operating systems across PCs, servers, and embedded systems. Known for its user-friendly interface and extensive software compatibility, Windows offers a variety of versions tailored for different devices and use cases.\n\n\nFeatures\n\nGUI: Graphical User Interface for intuitive navigation.\nCompatibility: Broad support for software and hardware.\nSecurity: Regular updates and security features."
  },
  {
    "objectID": "posts/top-5-operating-systems/index.html#macos",
    "href": "posts/top-5-operating-systems/index.html#macos",
    "title": "To 5 Operating Systems",
    "section": "macOS",
    "text": "macOS\n\nOverview\nExclusive to Apple’s hardware, macOS offers a sleek and intuitive user experience. It is known for its stability, design aesthetics, and seamless integration with other Apple devices and services.\n\n\nFeatures\n\nIntegration: Seamless integration with other Apple devices.\nPerformance: Optimized for Mac hardware.\nSecurity: Built-in encryption and privacy features."
  },
  {
    "objectID": "posts/top-5-operating-systems/index.html#linux",
    "href": "posts/top-5-operating-systems/index.html#linux",
    "title": "To 5 Operating Systems",
    "section": "Linux",
    "text": "Linux\n\nOverview\nLinux is an open-source operating system available in various distributions (distros), catering to diverse user needs. Its flexibility, stability, and robustness have made it a favorite among developers, server administrators, and tech enthusiasts.\n\n\nFeatures\n\nOpen Source: Community-driven development and customization.\nVariety: Diverse distributions catering to different needs.\nStability: Known for reliability and performance."
  },
  {
    "objectID": "posts/top-5-operating-systems/index.html#android",
    "href": "posts/top-5-operating-systems/index.html#android",
    "title": "To 5 Operating Systems",
    "section": "Android",
    "text": "Android\n\nOverview\nDeveloped by Google, Android is the leading operating system for mobile devices globally. Its open-source nature, customizable interface, and extensive app ecosystem have made it a dominant force in the mobile market.\n\n\nFeatures\n\nCustomization: Ability to personalize interface and features.\nApp Ecosystem: Extensive library of applications.\nIntegration: Seamless integration with Google services."
  },
  {
    "objectID": "posts/top-5-operating-systems/index.html#iosipados",
    "href": "posts/top-5-operating-systems/index.html#iosipados",
    "title": "To 5 Operating Systems",
    "section": "iOS/iPadOS",
    "text": "iOS/iPadOS\n\nOverview\nExclusive to Apple’s mobile devices, iOS and iPadOS are renowned for their smooth performance, security features, and seamless integration within the Apple ecosystem.\n\n\nFeatures\n\nPerformance: Smooth and efficient operation on Apple devices.\nSecurity: Robust security measures and privacy controls.\nEcosystem Integration: Seamless integration with other Apple devices.\n\nIn conclusion, these operating systems showcase diversity in terms of architecture, platform support, and features, catering to the varied needs of users across different devices and industries.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/time-series-intro/index.html",
    "href": "posts/time-series-intro/index.html",
    "title": "What is Time Series",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nAny data recorded with some fixed interval of time is called as time series data. This fixed interval can be hourly, daily, monthly or yearly. e.g. hourly temperature reading, daily changing fuel prices, monthly electricity bill, annul company profit report etc. In time series data, time will always be independent variable and there can be one or many dependent variable.\nSales forecasting time series with shampoo sales for every month will look like this,\n\n\n\nShampoo_Sales\n\n\nIn above example since there is only one variable dependent on time so its called as univariate time series. If there are multiple dependent variables, then its called as multivariate time series.\nObjective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals.\n[ad]"
  },
  {
    "objectID": "posts/time-series-intro/index.html#what-is-time-series",
    "href": "posts/time-series-intro/index.html#what-is-time-series",
    "title": "What is Time Series",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nAny data recorded with some fixed interval of time is called as time series data. This fixed interval can be hourly, daily, monthly or yearly. e.g. hourly temperature reading, daily changing fuel prices, monthly electricity bill, annul company profit report etc. In time series data, time will always be independent variable and there can be one or many dependent variable.\nSales forecasting time series with shampoo sales for every month will look like this,\n\n\n\nShampoo_Sales\n\n\nIn above example since there is only one variable dependent on time so its called as univariate time series. If there are multiple dependent variables, then its called as multivariate time series.\nObjective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals.\n[ad]"
  },
  {
    "objectID": "posts/time-series-intro/index.html#time-series-characteristics",
    "href": "posts/time-series-intro/index.html#time-series-characteristics",
    "title": "What is Time Series",
    "section": "Time Series Characteristics",
    "text": "Time Series Characteristics\nMean, standard deviation and seasonality defines different characteristics of the time series.\n\n\n\nTime_Series_Characteristics\n\n\nImportant characteristics of the time series are as below\n\nTrend\nTrend represent the change in dependent variables with respect to time from start to end. In case of increasing trend dependent variable will increase with time and vice versa. It’s not necessary to have definite trend in time series, we can have a single time series with increasing and decreasing trend. In short trend represent the varying mean of time series data.\n\n\n\nTrend\n\n\n\n\nSeasonality\nIf observations repeats after fixed time interval then they are referred as seasonal observations. These seasonal changes in data can occur because of natural events or man-made events. For example every year warm cloths sales increases just before winter season. So seasonality represent the data variations at fixed intervals.\n\n\n\nSeasonality\n\n\n\n\nIrregularities\nThis is also called as noise. Strange dips and jump in the data are called as irregularities. These fluctuations are caused by uncontrollable events like earthquakes, wars, flood, pandemic etc. For example because of COVID-19 pandemic there is huge demand for hand sanitizers and masks.\n\n\n\nIrregularities\n\n\n\n\nCyclicity\nCyclicity occurs when observations in the series repeats in random pattern. Note that if there is any fixed pattern then it becomes seasonality, in case of cyclicity observations may repeat after a week, months or may be after a year. These kinds of patterns are much harder to predict.\n\n\n\nCyclicity\n\n\nTime series data which has above characteristics is called as ‘Non-Stationary Data’. For any analysis on time series data we must convert it to ‘Stationary Data’\nThe general guideline is to estimate the trend and seasonality in the time series, and then make the time series stationary for data modeling. In data modeling step statistical techniques are used for time series analysis and forecasting. Once we have the predictions, in the final step forecasted values converted into the original scale by applying trend and seasonality constraints back.\n[ad]\n\n\n\n\n\nPart 2 of this Time Series Analysis series will introduce you to doing time series analysis. The link to part 2 is here"
  },
  {
    "objectID": "posts/creating-venvs/index.html",
    "href": "posts/creating-venvs/index.html",
    "title": "Python Virtual Environments",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nIn the vast ecosystem of Python programming, managing dependencies can sometimes be a daunting task. As projects grow in complexity, so does the need for a clean and isolated environment where dependencies can be installed without affecting other projects or the system-wide Python installation. This is where Python virtual environments come into play. In this guide, we’ll walk through the process of creating a virtual environment using venv and installing a package, like DataIdea, within it.\n\nWhat is a Python Virtual Environment?\nA virtual environment is a self-contained directory tree that contains a Python installation for a particular version of Python, plus a number of additional packages. It allows you to work on a Python project in isolation, ensuring that your project’s dependencies are maintained separately from other projects or the system Python.\n\n\nStep 1: Setting Up a Virtual Environment\nPython 3 comes with a built-in module called venv, which is used to create virtual environments. To create a virtual environment, open your terminal or command prompt and navigate to the directory where you want to create the environment. Then, run the following command:\n\nOn macOS and Linux:\n\npython3 -m venv myenv\n\nOn Windows:\n\npython -m venv myenv\nReplace myenv with the name you want to give to your virtual environment. This command will create a directory named myenv (or whatever name you provided) containing a Python interpreter and other necessary files.\n\n\n\n\n\n\n\nStep 2: Activating the Virtual Environment\nOnce the virtual environment is created, you need to activate it. Activation sets up the environment variables and modifies your shell prompt to indicate that you are now working within the virtual environment. Activate the virtual environment by running the appropriate command for your operating system:\n\nOn macOS and Linux:\n\nsource myenv/bin/activate\n\nOn Windows:\n\nmyenv\\Scripts\\activate\nYou’ll notice that your command prompt changes to show the name of the activated virtual environment.\n\n\n\n\n\n\n\nStep 3: Installing Packages\nWith the virtual environment activated, you can now install packages without affecting the global Python installation. Let’s install dataidea, as an example:\npip install dataidea\nyou can replace dataidea with another name of the package you want to install.\n\n\nStep 4: Using dataidea in Your Project\nOnce the package is installed, you can start using it in your Python project. Simply import it in your Python scripts as you would with any other package:\nimport dataidea\n\n\n\n\n\n\n\nStep 5: Deactivating the Virtual Environment\nWhen you’re done working on your project and want to leave the virtual environment, you can deactivate it by simply typing:\ndeactivate\n\n\nConclusion\nPython virtual environments are indispensable tools for managing dependencies and keeping project environments clean and isolated. With the venv module, creating and managing virtual environments has become easier than ever. By following the steps outlined in this guide, you can create a virtual environment, install packages like DataIdea, and develop Python projects with confidence. Happy coding!\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/ml-model-deployment/index.html",
    "href": "posts/ml-model-deployment/index.html",
    "title": "Exploring Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models is a crucial step in turning your data science projects into practical, real-world applications. With a variety of deployment options available, it’s essential to choose the one that best fits your needs. In this blog, we’ll explore the different deployment options and help you determine the best approach for your machine learning models.\n\n\nCloud platforms provide scalable and flexible environments for deploying machine learning models, making them a popular choice. Here’s a look at some of the leading cloud services:\nAmazon Web Services (AWS):\n\nSageMaker: A comprehensive service that offers tools to build, train, and deploy models at scale.\nLambda: Ideal for deploying lightweight models that need to respond to API requests.\nEC2 Instances: Customizable virtual servers suitable for deploying larger, more complex models.\n\nGoogle Cloud Platform (GCP):\n\nAI Platform: An integrated toolset designed for deploying and managing models efficiently.\nCloud Functions: A serverless environment perfect for lightweight model deployments.\nCompute Engine: Scalable virtual machines for more extensive and customized deployments.\n\nMicrosoft Azure:\n\nAzure Machine Learning: An end-to-end platform for building, training, and deploying models.\nAzure Functions: Serverless compute service ideal for deploying lightweight models.\nVirtual Machines: Provide a custom deployment environment tailored to your needs.\n\n\n\n\nFor organizations that require control over their infrastructure due to security, compliance, or performance needs, on-premises deployment is an excellent choice. Here are some options:\n\nDocker Containers: Ensures consistency across different environments by containerizing models.\nKubernetes: Manages and orchestrates containerized applications, offering scaling and management capabilities.\nDedicated Hardware: Utilizes specific hardware like GPUs or TPUs for high-performance requirements.\n\n\n\n\nEdge deployment is perfect for real-time processing and reduced latency by deploying models on edge devices like IoT devices and mobile phones. Some popular solutions include:\n\nTensorFlow Lite: Deploys TensorFlow models on mobile and embedded devices.\nONNX Runtime: Runs models trained in various frameworks on edge devices.\nNVIDIA Jetson: A platform for deploying AI models on edge devices with GPU acceleration.\n\n\n\n\nHybrid deployment combines multiple environments to leverage the advantages of each. For example, you might use the cloud for training and initial deployment but edge devices for inference to minimize latency and bandwidth usage.\n\n\n\nServerless computing allows you to deploy models without managing the underlying infrastructure. This option is ideal for applications with variable traffic and includes services like:\n\nAWS Lambda\nGoogle Cloud Functions\nAzure Functions\n\nThese services automatically scale with the load, simplifying the deployment process.\n\n\n\nExposing your model as a web service via RESTful or GraphQL APIs is a flexible and widely used approach. This allows various applications and services to access your model via HTTP requests. Some popular frameworks include:\n\nFlask/Django: Web application frameworks for Python.\nFastAPI: A modern, high-performance web framework for building APIs with Python 3.7+.\nTensorFlow Serving: Serves TensorFlow models as APIs, streamlining deployment.\n\n\n\n\nWhen real-time inference isn’t necessary, batch processing is a great option for handling large volumes of data at scheduled intervals. Key tools include:\n\nApache Spark: A unified analytics engine for large-scale data processing.\nHadoop: A framework for distributed storage and processing of big data."
  },
  {
    "objectID": "posts/ml-model-deployment/index.html#exploring-options-for-deploying-machine-learning-models",
    "href": "posts/ml-model-deployment/index.html#exploring-options-for-deploying-machine-learning-models",
    "title": "Exploring Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models is a crucial step in turning your data science projects into practical, real-world applications. With a variety of deployment options available, it’s essential to choose the one that best fits your needs. In this blog, we’ll explore the different deployment options and help you determine the best approach for your machine learning models.\n\n\nCloud platforms provide scalable and flexible environments for deploying machine learning models, making them a popular choice. Here’s a look at some of the leading cloud services:\nAmazon Web Services (AWS):\n\nSageMaker: A comprehensive service that offers tools to build, train, and deploy models at scale.\nLambda: Ideal for deploying lightweight models that need to respond to API requests.\nEC2 Instances: Customizable virtual servers suitable for deploying larger, more complex models.\n\nGoogle Cloud Platform (GCP):\n\nAI Platform: An integrated toolset designed for deploying and managing models efficiently.\nCloud Functions: A serverless environment perfect for lightweight model deployments.\nCompute Engine: Scalable virtual machines for more extensive and customized deployments.\n\nMicrosoft Azure:\n\nAzure Machine Learning: An end-to-end platform for building, training, and deploying models.\nAzure Functions: Serverless compute service ideal for deploying lightweight models.\nVirtual Machines: Provide a custom deployment environment tailored to your needs.\n\n\n\n\nFor organizations that require control over their infrastructure due to security, compliance, or performance needs, on-premises deployment is an excellent choice. Here are some options:\n\nDocker Containers: Ensures consistency across different environments by containerizing models.\nKubernetes: Manages and orchestrates containerized applications, offering scaling and management capabilities.\nDedicated Hardware: Utilizes specific hardware like GPUs or TPUs for high-performance requirements.\n\n\n\n\nEdge deployment is perfect for real-time processing and reduced latency by deploying models on edge devices like IoT devices and mobile phones. Some popular solutions include:\n\nTensorFlow Lite: Deploys TensorFlow models on mobile and embedded devices.\nONNX Runtime: Runs models trained in various frameworks on edge devices.\nNVIDIA Jetson: A platform for deploying AI models on edge devices with GPU acceleration.\n\n\n\n\nHybrid deployment combines multiple environments to leverage the advantages of each. For example, you might use the cloud for training and initial deployment but edge devices for inference to minimize latency and bandwidth usage.\n\n\n\nServerless computing allows you to deploy models without managing the underlying infrastructure. This option is ideal for applications with variable traffic and includes services like:\n\nAWS Lambda\nGoogle Cloud Functions\nAzure Functions\n\nThese services automatically scale with the load, simplifying the deployment process.\n\n\n\nExposing your model as a web service via RESTful or GraphQL APIs is a flexible and widely used approach. This allows various applications and services to access your model via HTTP requests. Some popular frameworks include:\n\nFlask/Django: Web application frameworks for Python.\nFastAPI: A modern, high-performance web framework for building APIs with Python 3.7+.\nTensorFlow Serving: Serves TensorFlow models as APIs, streamlining deployment.\n\n\n\n\nWhen real-time inference isn’t necessary, batch processing is a great option for handling large volumes of data at scheduled intervals. Key tools include:\n\nApache Spark: A unified analytics engine for large-scale data processing.\nHadoop: A framework for distributed storage and processing of big data."
  },
  {
    "objectID": "posts/ml-model-deployment/index.html#key-considerations-for-model-deployment",
    "href": "posts/ml-model-deployment/index.html#key-considerations-for-model-deployment",
    "title": "Exploring Options for Deploying Machine Learning Models",
    "section": "Key Considerations for Model Deployment",
    "text": "Key Considerations for Model Deployment\nWhen deciding on a deployment option, keep these factors in mind:\n\nScalability: Can the solution handle the expected load?\nLatency: Is real-time inference required?\nCost: What are the costs associated with running the model in production?\nSecurity: Does the deployment meet your security and compliance requirements?\nEase of Use: How straightforward is it to deploy, manage, and update the model?\nIntegration: How well does the deployment option integrate with your existing systems and workflows?\n\nSelecting the right deployment option is essential for balancing performance, cost, and operational complexity. By carefully considering your specific requirements and constraints, you can ensure a successful deployment of your machine learning models.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/time-series-forecasting/index.html",
    "href": "posts/time-series-forecasting/index.html",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 3 of our Time Series Analysis Series, Part 2 introduces you to doing time series analysis. The link to part 2 is here\nForecasting refers to the future predictions based on the time series data analysis. Below are the steps performed during time series forecasting\n\nStep 1: Understand the time series characteristics like trend, seasonality etc\nStep 2: Do the analysis and identify the best method to make the time series stationary\nStep 3: Note down the transformation steps performed to make the time series stationary and make sure that the reverse transformation of data is possible to get the original scale back\nStep 4: Based on data analysis choose the appropriate model for time series forecasting\nStep 5: We can assess the performance of a model by applying simple metrics such as residual sum of squares(RSS). Make sure to use whole data for prediction.\nStep 6: Now we will have an array of predictions which are in transformed scale. We just need to apply the reverse transformation to get the prediction values in original scale.\nStep 7: At the end we can do the future forecasting and get the future forecasted values in original scale."
  },
  {
    "objectID": "posts/time-series-forecasting/index.html#time-series-forecasting",
    "href": "posts/time-series-forecasting/index.html#time-series-forecasting",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 3 of our Time Series Analysis Series, Part 2 introduces you to doing time series analysis. The link to part 2 is here\nForecasting refers to the future predictions based on the time series data analysis. Below are the steps performed during time series forecasting\n\nStep 1: Understand the time series characteristics like trend, seasonality etc\nStep 2: Do the analysis and identify the best method to make the time series stationary\nStep 3: Note down the transformation steps performed to make the time series stationary and make sure that the reverse transformation of data is possible to get the original scale back\nStep 4: Based on data analysis choose the appropriate model for time series forecasting\nStep 5: We can assess the performance of a model by applying simple metrics such as residual sum of squares(RSS). Make sure to use whole data for prediction.\nStep 6: Now we will have an array of predictions which are in transformed scale. We just need to apply the reverse transformation to get the prediction values in original scale.\nStep 7: At the end we can do the future forecasting and get the future forecasted values in original scale."
  },
  {
    "objectID": "posts/time-series-forecasting/index.html#models-used-for-time-series-forecasting",
    "href": "posts/time-series-forecasting/index.html#models-used-for-time-series-forecasting",
    "title": "Time Series Forecasting",
    "section": "Models Used For Time Series Forecasting",
    "text": "Models Used For Time Series Forecasting\n\nAutoregression (AR)\nMoving Average (MA)\nAutoregressive Moving Average (ARMA)\nAutoregressive Integrated Moving Average (ARIMA)\nSeasonal Autoregressive Integrated Moving-Average (SARIMA)\nSeasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)\nVector Autoregression (VAR)\nVector Autoregression Moving-Average (VARMA)\nVector Autoregression Moving-Average with Exogenous Regressors (VARMAX)\nSimple Exponential Smoothing (SES)\nHolt Winter’s Exponential Smoothing (HWES)\n\nNext part of this article we are going to analyze and forecast air passengers time series data using ARIMA model. Brief introduction of ARIMA model is as below\n[ad]"
  },
  {
    "objectID": "posts/time-series-forecasting/index.html#arima",
    "href": "posts/time-series-forecasting/index.html#arima",
    "title": "Time Series Forecasting",
    "section": "ARIMA",
    "text": "ARIMA\n\nARIMA stands for Auto-Regressive Integrated Moving Averages. It is actually a combination of AR and MA model.\nARIMA has three parameters ‘p’ for the order of Auto-Regressive (AR) part, ‘q’ for the order of Moving Average (MA) part and ‘d’ for the order of integrated part.\n\n\nAuto-Regressive (AR) Model:\n\nAs the name indicates, its the regression of the variables against itself. In this model linear combination of the past values are used to forecast the future values.\nTo figure out the order of AR model we will use PACF function\n\n\n\nIntegration(I):\n\nUses differencing of observations (subtracting an observation from observation at the previous time step) in order to make the time series stationary. Differencing involves the subtraction of the current values of a series with its previous values \\(d\\) number of times.\nMost of the time value of \\(d = 1\\), means first order of difference.\n\n\n\nMoving Average (MA) Model:\n\nRather than using past values of the forecast variable in a regression, a moving average model uses linear combination of past forecast errors\nTo figure out the order of MA model we will use ACF function\n\n[ad]"
  },
  {
    "objectID": "posts/rethinking-chatgpt/index.html#introduction",
    "href": "posts/rethinking-chatgpt/index.html#introduction",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "1 Introduction",
    "text": "1 Introduction\nArtificial Intelligence (AI) has revolutionized the way we interact with technology, making many tasks more efficient and accessible. Among the AI applications, chatbots and virtual assistants have gained significant popularity, with ChatGPT being one of the leading models developed by OpenAI. While ChatGPT and similar AI tools offer numerous benefits, there are also several reasons to be cautious about their use. This blog aims to explore in detail why you might reconsider relying on ChatGPT, touching on aspects such as privacy concerns, ethical considerations, potential biases, reliability, and the broader societal implications of AI."
  },
  {
    "objectID": "posts/rethinking-chatgpt/index.html#privacy-concerns",
    "href": "posts/rethinking-chatgpt/index.html#privacy-concerns",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "2 Privacy Concerns",
    "text": "2 Privacy Concerns\n\n2.1 Data Collection and Storage\nOne of the primary concerns with using ChatGPT is related to data privacy. When you interact with ChatGPT, your conversations are often recorded and stored. Although companies like OpenAI implement stringent data security measures, there is always a risk of data breaches. The data collected from users can potentially be accessed by unauthorized individuals, leading to privacy violations.\n\n\n2.2 Misuse of Personal Information\nThe personal information you share with ChatGPT could be misused. While AI developers claim that user data is anonymized and used solely to improve AI models, there is always a risk that this data could be sold to third parties or used for targeted advertising. This not only undermines your privacy but also raises ethical questions about the exploitation of user data.\n\n\n2.3 Lack of Transparency\nMany users are unaware of how their data is being used when they interact with AI models like ChatGPT. The lack of transparency in data handling practices can lead to mistrust. Users should be informed about what data is being collected, how it is stored, and who has access to it. Without this transparency, it is difficult to trust that your data is being handled responsibly."
  },
  {
    "objectID": "posts/rethinking-chatgpt/index.html#ethical-considerations",
    "href": "posts/rethinking-chatgpt/index.html#ethical-considerations",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "3 Ethical Considerations",
    "text": "3 Ethical Considerations\n\n3.1 Manipulation and Misinformation\nAI models like ChatGPT can be used to manipulate opinions and spread misinformation. The ability of AI to generate human-like text makes it a powerful tool for creating fake news, misleading articles, and deceptive social media posts. This can have serious consequences, including influencing elections, spreading conspiracy theories, and undermining public trust in information sources.\n\n\n3.2 Dehumanization of Interaction\nRelying on AI for communication can lead to the dehumanization of interactions. While ChatGPT can simulate human conversation, it lacks the emotional intelligence and empathy that come from genuine human interaction. This can result in a diminished quality of communication, where the nuances of human emotions and connections are lost.\n\n\n3.3 Ethical Use in Sensitive Areas\nThe use of AI in sensitive areas such as mental health support, legal advice, and medical consultations raises ethical concerns. AI models may provide inaccurate or inappropriate advice, potentially causing harm to users. The reliance on AI in these critical areas should be approached with caution, ensuring that human oversight is always present to verify and validate the information provided by AI."
  },
  {
    "objectID": "posts/rethinking-chatgpt/index.html#potential-biases",
    "href": "posts/rethinking-chatgpt/index.html#potential-biases",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "4 Potential Biases",
    "text": "4 Potential Biases\n\n4.1 Inherent Biases in Training Data\nAI models like ChatGPT are trained on vast amounts of data from the internet, which inevitably includes biased and prejudiced content. These biases can be reflected in the responses generated by the AI, perpetuating stereotypes and discriminatory attitudes. For instance, gender, racial, and cultural biases can be inadvertently reinforced through AI-generated text.\n\n\n4.2 Impact on Marginalized Communities\nThe biases present in AI models can disproportionately impact marginalized communities. AI-generated content that reflects societal biases can contribute to the marginalization and discrimination of these groups. It is essential to recognize and address these biases to ensure that AI technologies do not perpetuate inequality and injustice.\n\n\n4.3 Difficulty in Mitigating Biases\nWhile efforts are being made to reduce biases in AI models, it is a challenging task. Biases are deeply ingrained in the data used to train these models, and completely eliminating them is nearly impossible. Continuous monitoring and updating of AI models are required to mitigate these biases, but the effectiveness of these measures is still a subject of ongoing research and debate."
  },
  {
    "objectID": "posts/rethinking-chatgpt/index.html#reliability-and-accuracy",
    "href": "posts/rethinking-chatgpt/index.html#reliability-and-accuracy",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "5 Reliability and Accuracy",
    "text": "5 Reliability and Accuracy\n\n5.1 Hallucinations and Errors\nAI models like ChatGPT are prone to generating incorrect or nonsensical information, often referred to as “hallucinations.” These errors can be misleading and potentially harmful, especially when users rely on the AI for accurate information. The inability to distinguish between correct and incorrect responses can undermine the reliability of AI-generated content.\n\n\n5.2 Lack of Accountability\nWhen using AI models, it can be challenging to determine who is accountable for the information provided. Unlike human professionals who can be held responsible for their advice and actions, AI lacks accountability. This lack of accountability can lead to a scenario where misinformation is spread without any consequences for those responsible.\n\n\n5.3 Dependence on AI\nOver-reliance on AI for information and decision-making can reduce critical thinking skills and the ability to independently verify information. Users may become dependent on AI-generated responses, leading to a decline in their analytical abilities and judgment. It is crucial to maintain a balance between using AI as a tool and exercising independent thought."
  },
  {
    "objectID": "posts/rethinking-chatgpt/index.html#broader-societal-implications",
    "href": "posts/rethinking-chatgpt/index.html#broader-societal-implications",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "6 Broader Societal Implications",
    "text": "6 Broader Societal Implications\n\n6.1 Job Displacement\nThe increasing use of AI technologies, including ChatGPT, has the potential to displace jobs. As AI becomes more capable of performing tasks traditionally done by humans, there is a risk of job loss in various sectors. This can lead to economic instability and exacerbate issues related to unemployment and income inequality.\n\n\n6.2 Ethical Dilemmas in AI Development\nThe development and deployment of AI technologies pose significant ethical dilemmas. Decisions about what data to use, how to handle biases, and how to ensure the responsible use of AI are complex and multifaceted. The ethical challenges associated with AI development require careful consideration and the involvement of diverse stakeholders to navigate effectively.\n\n\n6.3 Impact on Human Creativity and Innovation\nThe reliance on AI for creative tasks, such as writing, music composition, and art, raises questions about the impact on human creativity and innovation. While AI can assist in these areas, there is a concern that it may stifle human creativity by providing easy solutions and reducing the incentive to develop original ideas. It is important to find a balance where AI complements human creativity rather than replacing it.\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe."
  },
  {
    "objectID": "posts/rethinking-chatgpt/index.html#conclusion",
    "href": "posts/rethinking-chatgpt/index.html#conclusion",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nWhile ChatGPT and similar AI models offer numerous benefits, it is essential to be aware of the potential drawbacks and risks associated with their use. Privacy concerns, ethical considerations, potential biases, reliability issues, and broader societal implications are all important factors to consider. As we continue to integrate AI into our lives, it is crucial to approach its use with caution, ensuring that we prioritize ethical practices, transparency, and human oversight to mitigate the risks and maximize the benefits."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html",
    "href": "posts/mr-robot-softwares/index.html",
    "title": "All Software used in Mr. Robot",
    "section": "",
    "text": "Thumbnail\nHere’s a list of the most popular softwares used by the hackers in the Emmy and Golden Globe award winning drama/thriller series Mr. Robot."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#kali-linux",
    "href": "posts/mr-robot-softwares/index.html#kali-linux",
    "title": "All Software used in Mr. Robot",
    "section": "Kali Linux",
    "text": "Kali Linux\nKali Linux is a Linux distro made for security researchers for penetration testing, but is also used by hackers since it is jam packed with hacking tools. It is regularly featured in Mr. Robot since it is the hackers’ operating system of choice."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#tor-browser",
    "href": "posts/mr-robot-softwares/index.html#tor-browser",
    "title": "All Software used in Mr. Robot",
    "section": "Tor Browser",
    "text": "Tor Browser\nTor Browser is widely considered to be the best anonymizing tool out there. It will make your Internet activity very hard to trace, which fsociety takes advantage of when Trenton in season 2, episode 8 uploads a leaked FBI conference call about illegal mass surveillance to Vimeo using Tor Browser."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#raspberry-pi",
    "href": "posts/mr-robot-softwares/index.html#raspberry-pi",
    "title": "All Software used in Mr. Robot",
    "section": "Raspberry Pi",
    "text": "Raspberry Pi\nRaspberry Pi is a small, programmable computer board designed to teach children about computer science. It is also favourite among hobbyists and programmers due to its low-cost, versatility and simplicity. In season 1, episode 5 Elliot installs a Raspberry Pi into Steel Mountain’s climate control system so that fsociety at a later point in time can remotely raise the temperature in the storage room where Evil Corp’s tape backups are stored, resulting in the backups of the records of a significant portion of the US’ consumer debt being destroyed."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#proton-mail",
    "href": "posts/mr-robot-softwares/index.html#proton-mail",
    "title": "All Software used in Mr. Robot",
    "section": "Proton Mail",
    "text": "Proton Mail\nProtonMail is a secure, end-to-end encrypted e-mail service based in Switzerland that is used by Elliot in season 1, episode 8. The team behind Mr. Robot researched secure e-mail services to the extent that they actually contacted the ProtonMail developers and asked if it was possible for users to monitor their own e-mail activity in ProtonMail. The ProtonMail developers liked the idea of account access logs so much that they ended up implementing it in their v2.0 release of ProtonMail."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#pycharm",
    "href": "posts/mr-robot-softwares/index.html#pycharm",
    "title": "All Software used in Mr. Robot",
    "section": "PyCharm",
    "text": "PyCharm\nPyCharm is a Python and Django IDE (Integrated Developer Environment), which is a type of code editor software. It is used by Trenton in season 1, episode 4."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#bluesniff",
    "href": "posts/mr-robot-softwares/index.html#bluesniff",
    "title": "All Software used in Mr. Robot",
    "section": "Bluesniff",
    "text": "Bluesniff\nBluesniff is a Bluetooth device discovery tool. In season 1, episode 6 Elliot uses Bluesniff in combination with btscanner and Metasploit when he connects to the computer in a nearby police car via a MultiBlue Bluetooth USB Dongle to compromise a prison’s network in order to break a drug dealer called Vera out of prison."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#btscanner",
    "href": "posts/mr-robot-softwares/index.html#btscanner",
    "title": "All Software used in Mr. Robot",
    "section": "btscanner",
    "text": "btscanner\nbtscanner is a tool that is included in Kali Linux that extracts as much information as possible about Bluetooth devices without having to pair. In season 1, episode 6 Elliot uses btscanner in combination with Bluesniff and Metasploit when he connects to the computer in a nearby police car using a MultiBlue Bluetooth USB Dongle to compromise a prison’s network in order to break a drug dealer called Vera out of prison."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#mozilla-firefox",
    "href": "posts/mr-robot-softwares/index.html#mozilla-firefox",
    "title": "All Software used in Mr. Robot",
    "section": "Mozilla Firefox",
    "text": "Mozilla Firefox\nElliot uses Firefox as his default web browser. Trenton uses Firefox in season 2, episode 8."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#vlc-media-player",
    "href": "posts/mr-robot-softwares/index.html#vlc-media-player",
    "title": "All Software used in Mr. Robot",
    "section": "VLC Media Player",
    "text": "VLC Media Player\nVLC Media Player was used in season 2, episode 4 when Elliot and Darlene watched a VHS rip of the fake horror movie Careful Massacre of the Bourgeoisie together. VLC is also used in season 2, episode 8 when fsociety preview the video they are about to upload a leaked FBI conference call about illegal mass surveillance."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#µtorrent",
    "href": "posts/mr-robot-softwares/index.html#µtorrent",
    "title": "All Software used in Mr. Robot",
    "section": "µTorrent",
    "text": "µTorrent\nIn season 2, episode 4 Darlene was downloading a VHS rip of the fake horror movie Careful Massacre of the Bourgeoisie using µTorrent."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#flexispy",
    "href": "posts/mr-robot-softwares/index.html#flexispy",
    "title": "All Software used in Mr. Robot",
    "section": "FlexiSPY",
    "text": "FlexiSPY\nFlexiSPY is a spyware software for Android, iOS and BlackBerry that lets the user monitor all activities on the victims phone. In season 1, episode 3 Tyrell Wellick covertly installs it on a co-worker’s Android phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#kingoroot",
    "href": "posts/mr-robot-softwares/index.html#kingoroot",
    "title": "All Software used in Mr. Robot",
    "section": "KingoRoot",
    "text": "KingoRoot\nKingo Root is used by Tyrell Wellick in season 1, episode 3 to root a co-worker’s Android phone so that he can covertly install the FlexiSPY spyware on the phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#kvm-kernel-based-virtual-machine",
    "href": "posts/mr-robot-softwares/index.html#kvm-kernel-based-virtual-machine",
    "title": "All Software used in Mr. Robot",
    "section": "KVM (Kernel-based Virtual Machine)",
    "text": "KVM (Kernel-based Virtual Machine)\nKVM is a hypervisor, which is a software that can run other operating systems via virtual machines. Elliot uses KVM to virtualize Windows 7 inside of Kali Linux. In season 1, episode 6 Elliot uses KVM to run Metasploit and Metapreter and in season 1, episode 8 he uses KVM to run DeepSound."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#metasploit",
    "href": "posts/mr-robot-softwares/index.html#metasploit",
    "title": "All Software used in Mr. Robot",
    "section": "Metasploit",
    "text": "Metasploit\nMetasploit Framework is a software included in Kali Linux that makes it easier to discover vulnerabilities in networks for penetration testers. Meterpreter is one of several hundreds of payloads that can be run in the Metasploit Framework and it is used in season 1, episode 6. In season 1, episode 6 Elliot uses Metasploit Framwork and Metapreter in combination with btscanner and Bluesniff when he connects to the computer in a nearby police car via a MultiBlue Bluetooth USB Dongle to compromise a prison’s network in order to break a drug dealer called Vera out of prison."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#framaroot",
    "href": "posts/mr-robot-softwares/index.html#framaroot",
    "title": "All Software used in Mr. Robot",
    "section": "Framaroot",
    "text": "Framaroot\nFramaroot - called RooterFrame in the show - is used by Tyrell Wellick in season 1, episode 3 to root a co-worker’s Android phone so that he can covertly install the FlexiSPY spyware on the phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/mr-robot-softwares/index.html#supersu",
    "href": "posts/mr-robot-softwares/index.html#supersu",
    "title": "All Software used in Mr. Robot",
    "section": "SuperSU",
    "text": "SuperSU\nSuperSU is an app that managed superuser privileges on rooted Android phones. In season 1, episode 3 Tyrell Wellick covertly installs FlexiSPY - which uses SuperSU to give itself superuser access - on a co-worker’s Android phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/handling-imbalanced-datasets/handling_imbalanced_data.html",
    "href": "posts/handling-imbalanced-datasets/handling_imbalanced_data.html",
    "title": "Handling Imbalanced Datasets",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nHandling imbalanced datasets is a common challenge in machine learning, especially in classification tasks where one class significantly outnumbers the other(s). Let’s go through a simple example using the popular Iris dataset, which we’ll artificially imbalance for demonstration purposes.\nThe Iris dataset consists of 150 samples, each belonging to one of three classes: Iris Setosa, Iris Versicolour, and Iris Virginica. We’ll create an imbalanced version of this dataset where one class is underrepresented.\nFirst, let’s load the dataset and create the imbalance:\n# !pip install imbalanced-learn\n# !pip install --upgrade dataidea\nfrom dataidea.packages import pd, plt\nfrom sklearn.datasets import load_iris\n# Load Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Convert to DataFrame for manipulation\ndf = pd.DataFrame(data=np.c_[X, y],\n                  columns=iris.feature_names + ['target'])\n\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\ntarget\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n0.0\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n0.0\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n\n\n\n# Introduce imbalance by removing samples from one class\nclass_to_remove = 2  # Iris Virginica\nimbalance_ratio = 0.5  # Ratio of samples to be removed\nindices_to_remove = np.random.choice(df[df['target'] == class_to_remove].index,\n                                     size=int(imbalance_ratio * len(df[df['target'] == class_to_remove])),\n                                     replace=False)\ndf_imbalanced = df.drop(indices_to_remove)\n\n# Check the class distribution\nvalue_counts = df_imbalanced['target'].value_counts()\nprint(value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    25\nName: count, dtype: int64\nplt.bar(value_counts.index,\n        value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\nNow, df_imbalanced contains the imbalanced dataset. Next, we’ll demonstrate a few techniques to handle this imbalance:\n\nResampling Methods:\n\nOversampling: Randomly duplicate samples from the minority class.\nUndersampling: Randomly remove samples from the majority class.\n\nSynthetic Sampling Methods:\n\nSMOTE (Synthetic Minority Over-sampling Technique): Generates synthetic samples for the minority class.\n\nAlgorithmic Techniques:\n\nAlgorithm Tuning: Adjusting class weights in the algorithm.\nEnsemble Methods: Using ensemble techniques like bagging or boosting."
  },
  {
    "objectID": "posts/handling-imbalanced-datasets/handling_imbalanced_data.html#handling-imbalanced-dataset",
    "href": "posts/handling-imbalanced-datasets/handling_imbalanced_data.html#handling-imbalanced-dataset",
    "title": "Handling Imbalanced Datasets",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nHandling imbalanced datasets is a common challenge in machine learning, especially in classification tasks where one class significantly outnumbers the other(s). Let’s go through a simple example using the popular Iris dataset, which we’ll artificially imbalance for demonstration purposes.\nThe Iris dataset consists of 150 samples, each belonging to one of three classes: Iris Setosa, Iris Versicolour, and Iris Virginica. We’ll create an imbalanced version of this dataset where one class is underrepresented.\nFirst, let’s load the dataset and create the imbalance:\n# !pip install imbalanced-learn\n# !pip install --upgrade dataidea\nfrom dataidea.packages import pd, plt\nfrom sklearn.datasets import load_iris\n# Load Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Convert to DataFrame for manipulation\ndf = pd.DataFrame(data=np.c_[X, y],\n                  columns=iris.feature_names + ['target'])\n\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\ntarget\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n0.0\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n0.0\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n\n\n\n# Introduce imbalance by removing samples from one class\nclass_to_remove = 2  # Iris Virginica\nimbalance_ratio = 0.5  # Ratio of samples to be removed\nindices_to_remove = np.random.choice(df[df['target'] == class_to_remove].index,\n                                     size=int(imbalance_ratio * len(df[df['target'] == class_to_remove])),\n                                     replace=False)\ndf_imbalanced = df.drop(indices_to_remove)\n\n# Check the class distribution\nvalue_counts = df_imbalanced['target'].value_counts()\nprint(value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    25\nName: count, dtype: int64\nplt.bar(value_counts.index,\n        value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\nNow, df_imbalanced contains the imbalanced dataset. Next, we’ll demonstrate a few techniques to handle this imbalance:\n\nResampling Methods:\n\nOversampling: Randomly duplicate samples from the minority class.\nUndersampling: Randomly remove samples from the majority class.\n\nSynthetic Sampling Methods:\n\nSMOTE (Synthetic Minority Over-sampling Technique): Generates synthetic samples for the minority class.\n\nAlgorithmic Techniques:\n\nAlgorithm Tuning: Adjusting class weights in the algorithm.\nEnsemble Methods: Using ensemble techniques like bagging or boosting."
  },
  {
    "objectID": "posts/handling-imbalanced-datasets/handling_imbalanced_data.html#resampling",
    "href": "posts/handling-imbalanced-datasets/handling_imbalanced_data.html#resampling",
    "title": "Handling Imbalanced Datasets",
    "section": "Resampling",
    "text": "Resampling\n\nOversampling\nLet’s implement oversampling using the imbalanced-learn library:\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Separate features and target\nX_imbalanced = df_imbalanced.drop('target', axis=1)\ny_imbalanced = df_imbalanced['target']\n\n# Apply Random Over-Sampling\noversample = RandomOverSampler(sampling_strategy='auto',\n                               random_state=42)\nX_resampled, y_resampled = oversample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after oversampling\noversampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(oversampled_data_value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\nplt.bar(oversampled_data_value_counts.index,\n        oversampled_data_value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\n\n\nUndersampling:\nUndersampling involves reducing the number of samples in the majority class to balance the dataset. Here’s how you can apply random undersampling using the imbalanced-learn library:\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Apply Random Under-Sampling\nundersample = RandomUnderSampler(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = undersample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after undersampling\nundersampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(undersampled_data_value_counts)\ntarget\n0.0    25\n1.0    25\n2.0    25\nName: count, dtype: int64\nplt.bar(undersampled_data_value_counts.index,\n        undersampled_data_value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "posts/handling-imbalanced-datasets/handling_imbalanced_data.html#smote-synthetic-minority-over-sampling-technique",
    "href": "posts/handling-imbalanced-datasets/handling_imbalanced_data.html#smote-synthetic-minority-over-sampling-technique",
    "title": "Handling Imbalanced Datasets",
    "section": "SMOTE (Synthetic Minority Over-sampling Technique)",
    "text": "SMOTE (Synthetic Minority Over-sampling Technique)\nSMOTE generates synthetic samples for the minority class by interpolating between existing minority class samples.\nHere’s how you can apply SMOTE using the imbalanced-learn library:\nfrom imblearn.over_sampling import SMOTE\n\n# Apply SMOTE\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after SMOTE\nsmoted_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(smoted_data_value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\nplt.bar(smoted_data_value_counts.index,\n        smoted_data_value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\nBy using SMOTE, you can generate synthetic samples for the minority class, effectively increasing its representation in the dataset. This can help to mitigate the class imbalance issue and improve the performance of your machine learning model."
  },
  {
    "objectID": "posts/handling-imbalanced-datasets/handling_imbalanced_data.html#algorithmic-techniques",
    "href": "posts/handling-imbalanced-datasets/handling_imbalanced_data.html#algorithmic-techniques",
    "title": "Handling Imbalanced Datasets",
    "section": "Algorithmic Techniques",
    "text": "Algorithmic Techniques\n\nAlgorithm Tuning:\nMany algorithms allow you to specify class weights to penalize misclassifications of the minority class more heavily. Here’s an example using the class_weight parameter in a logistic regression classifier:\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_imbalanced, y_imbalanced,\n                                                    test_size=0.2, random_state=42)\n\n# Define the logistic regression classifier with class weights\nclass_weights = {0: 1, 1: 1, 2: 20}  # Penalize the minority class more heavily\nlog_reg = LogisticRegression(class_weight=class_weights)\n\n# Train the model\nlog_reg.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = log_reg.predict(X_test)\n\n# display classification report\npd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n\n\n\n\n\n\n\n\nprecision\n\n\nrecall\n\n\nf1-score\n\n\nsupport\n\n\n\n\n\n\n0.0\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n13.00\n\n\n\n\n1.0\n\n\n1.000000\n\n\n0.750000\n\n\n0.857143\n\n\n8.00\n\n\n\n\n2.0\n\n\n0.666667\n\n\n1.000000\n\n\n0.800000\n\n\n4.00\n\n\n\n\naccuracy\n\n\n0.920000\n\n\n0.920000\n\n\n0.920000\n\n\n0.92\n\n\n\n\nmacro avg\n\n\n0.888889\n\n\n0.916667\n\n\n0.885714\n\n\n25.00\n\n\n\n\nweighted avg\n\n\n0.946667\n\n\n0.920000\n\n\n0.922286\n\n\n25.00\n\n\n\n\n\nIn this example, the class weight for the minority class is increased to penalize misclassifications more heavily.\n\n\nEnsemble Methods\nEnsemble methods can also be effective for handling imbalanced datasets. Techniques such as bagging and boosting can improve the performance of classifiers, especially when dealing with imbalanced classes.\nHere’s an example of using ensemble methods like Random Forest, a popular bagging algorithm, with an imbalanced dataset:\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Define and train Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_rf = rf_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"Random Forest Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_rf, output_dict=True)).transpose()\nRandom Forest Classifier:\n\n\n\n\n\n\n\n\nprecision\n\n\nrecall\n\n\nf1-score\n\n\nsupport\n\n\n\n\n\n\n0.0\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n13.00\n\n\n\n\n1.0\n\n\n1.000000\n\n\n0.875000\n\n\n0.933333\n\n\n8.00\n\n\n\n\n2.0\n\n\n0.800000\n\n\n1.000000\n\n\n0.888889\n\n\n4.00\n\n\n\n\naccuracy\n\n\n0.960000\n\n\n0.960000\n\n\n0.960000\n\n\n0.96\n\n\n\n\nmacro avg\n\n\n0.933333\n\n\n0.958333\n\n\n0.940741\n\n\n25.00\n\n\n\n\nweighted avg\n\n\n0.968000\n\n\n0.960000\n\n\n0.960889\n\n\n25.00\n\n\n\n\n\nEnsemble methods like Random Forest build multiple decision trees and combine their predictions to make a final prediction. This can often lead to better generalization and performance, even in the presence of imbalanced classes.\n\n\nAdaBoost Classifier\nAnother ensemble method that specifically addresses class imbalance is AdaBoost (Adaptive Boosting). AdaBoost focuses more on those training instances that were previously misclassified, thus giving higher weight to the minority class instances.\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Define and train AdaBoost classifier\nada_classifier = AdaBoostClassifier(n_estimators=100, random_state=42)\nada_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_ada = ada_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"AdaBoost Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_ada, output_dict=True)).transpose()\nAdaBoost Classifier:\n\n\n/home/jumashafara/venvs/dataidea/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n  warnings.warn(\n\n\n\n\n\n\n\n\nprecision\n\n\nrecall\n\n\nf1-score\n\n\nsupport\n\n\n\n\n\n\n0.0\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n13.00\n\n\n\n\n1.0\n\n\n1.000000\n\n\n0.875000\n\n\n0.933333\n\n\n8.00\n\n\n\n\n2.0\n\n\n0.800000\n\n\n1.000000\n\n\n0.888889\n\n\n4.00\n\n\n\n\naccuracy\n\n\n0.960000\n\n\n0.960000\n\n\n0.960000\n\n\n0.96\n\n\n\n\nmacro avg\n\n\n0.933333\n\n\n0.958333\n\n\n0.940741\n\n\n25.00\n\n\n\n\nweighted avg\n\n\n0.968000\n\n\n0.960000\n\n\n0.960889\n\n\n25.00\n\n\n\n\n\nBy utilizing ensemble methods like Random Forest and AdaBoost, you can often achieve better performance on imbalanced datasets compared to individual classifiers, as these methods inherently mitigate the effects of class imbalance through their construction.\nThese are just a few techniques for handling imbalanced datasets. It’s crucial to experiment with different methods and evaluate their performance using appropriate evaluation metrics to find the best approach for your specific problem.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/anova-for-feature-selection/index.html#anova-for-feature-selection",
    "href": "posts/anova-for-feature-selection/index.html#anova-for-feature-selection",
    "title": "ANOVA for Feature Selection",
    "section": "ANOVA for Feature Selection",
    "text": "ANOVA for Feature Selection\nIn this notebook, we demonstrate how ANOVA (Analysis of Variance) can be used to identify better features for machine learning models. We’ll use the Fantasy Premier League (FPL) dataset to show how ANOVA helps in selecting features that best differentiate categories.\n# Uncomment the line below if you need to install the dataidea package\n# !pip install -U dataidea\nFirst, we’ll import the necessary packages: scipy for performing ANOVA, dataidea for loading the FPL dataset, and SelectKBest from scikit-learn for univariate feature selection based on statistical tests.\nimport scipy as sp\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport dataidea as di\nLet’s load the FPL dataset and preview the top 5 rows.\n# Load FPL dataset\nfpl = di.loadDataset('fpl')\n\n# Preview the top 5 rows\nfpl.head(n=5)\n\n\n\n\n\n\n\n\n\nFirst_Name\n\n\nSecond_Name\n\n\nClub\n\n\nGoals_Scored\n\n\nAssists\n\n\nTotal_Points\n\n\nMinutes\n\n\nSaves\n\n\nGoals_Conceded\n\n\nCreativity\n\n\nInfluence\n\n\nThreat\n\n\nBonus\n\n\nBPS\n\n\nICT_Index\n\n\nClean_Sheets\n\n\nRed_Cards\n\n\nYellow_Cards\n\n\nPosition\n\n\n\n\n\n\n0\n\n\nBruno\n\n\nFernandes\n\n\nMUN\n\n\n18\n\n\n14\n\n\n244\n\n\n3101\n\n\n0\n\n\n36\n\n\n1414.9\n\n\n1292.6\n\n\n1253\n\n\n36\n\n\n870\n\n\n396.2\n\n\n13\n\n\n0\n\n\n6\n\n\nMID\n\n\n\n\n1\n\n\nHarry\n\n\nKane\n\n\nTOT\n\n\n23\n\n\n14\n\n\n242\n\n\n3083\n\n\n0\n\n\n39\n\n\n659.1\n\n\n1318.2\n\n\n1585\n\n\n40\n\n\n880\n\n\n355.9\n\n\n12\n\n\n0\n\n\n1\n\n\nFWD\n\n\n\n\n2\n\n\nMohamed\n\n\nSalah\n\n\nLIV\n\n\n22\n\n\n6\n\n\n231\n\n\n3077\n\n\n0\n\n\n41\n\n\n825.7\n\n\n1056.0\n\n\n1980\n\n\n21\n\n\n657\n\n\n385.8\n\n\n11\n\n\n0\n\n\n0\n\n\nMID\n\n\n\n\n3\n\n\nHeung-Min\n\n\nSon\n\n\nTOT\n\n\n17\n\n\n11\n\n\n228\n\n\n3119\n\n\n0\n\n\n36\n\n\n1049.9\n\n\n1052.2\n\n\n1046\n\n\n26\n\n\n777\n\n\n315.2\n\n\n13\n\n\n0\n\n\n0\n\n\nMID\n\n\n\n\n4\n\n\nPatrick\n\n\nBamford\n\n\nLEE\n\n\n17\n\n\n11\n\n\n194\n\n\n3052\n\n\n0\n\n\n50\n\n\n371.0\n\n\n867.2\n\n\n1512\n\n\n26\n\n\n631\n\n\n274.6\n\n\n10\n\n\n0\n\n\n3\n\n\nFWD\n\n\n\n\n\nANOVA helps us determine if there’s a significant difference between the means of different groups. We use it to select features that best show the difference between categories. Features with higher F-statistics are preferred.\n\nANOVA for Goals Scored\nWe will create groups of goals scored by each player position (forwards, midfielders, defenders, and goalkeepers) and run an ANOVA test.\n# Create groups of goals scored for each player position\nforwards_goals = fpl[fpl.Position == 'FWD']['Goals_Scored']\nmidfielders_goals = fpl[fpl.Position == 'MID']['Goals_Scored']\ndefenders_goals = fpl[fpl.Position == 'DEF']['Goals_Scored']\ngoalkeepers_goals = fpl[fpl.Position == 'GK']['Goals_Scored']\n\n# Perform the ANOVA test for the groups\nf_statistic, p_value = sp.stats.f_oneway(forwards_goals, midfielders_goals, defenders_goals, goalkeepers_goals)\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\nF-statistic: 33.281034594400445\np-value: 3.9257634156019246e-20\nWe observe an F-statistic of 33.281 and a p-value of 3.926e-20, indicating a significant difference at multiple confidence levels.\n\n\nANOVA for Assists\nNext, we’ll create groups for assists and run an ANOVA test.\n# Create groups of assists for each player position\nforwards_assists = fpl[fpl.Position == 'FWD']['Assists']\nmidfielders_assists = fpl[fpl.Position == 'MID']['Assists']\ndefenders_assists = fpl[fpl.Position == 'DEF']['Assists']\ngoalkeepers_assists = fpl[fpl.Position == 'GK']['Assists']\n\n# Perform the ANOVA test for the groups\nf_statistic, p_value = sp.stats.f_oneway(forwards_assists, midfielders_assists, defenders_assists, goalkeepers_assists)\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\nF-statistic: 19.263717036430815\np-value: 5.124889288362087e-12\nWe observe an F-statistic of 19.264 and a p-value of 5.125e-12, again indicating significance.\n\n\nComparing Results\nBoth features show significant F-statistics, but goals scored has a higher value, indicating it is a better feature for differentiating player positions.\n\n\nUsing SelectKBest for Feature Selection\nWe can also use SelectKBest from scikit-learn to automate this process.\n# Use scikit-learn's SelectKBest (with f_classif)\ntest = SelectKBest(score_func=f_classif, k=1)\n\n# Fit the model to the data\nfit = test.fit(fpl[['Goals_Scored', 'Assists']], fpl.Position)\n\n# Get the F-statistics\nscores = fit.scores_\n\n# Select the best feature\nfeatures = fit.transform(fpl[['Goals_Scored', 'Assists']])\n\n# Get the indices of the selected features (optional)\nselected_indices = test.get_support(indices=True)\n\n# Print indices and scores\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\nFeature Scores:  [33.28103459 19.26371704]\nSelected Features Indices:  [0]\nThe 0th feature (Goals Scored) is selected as the best feature based on the F-statistics.\n\n\n\n\n\n\n\nDon’t Miss Any Updates!\n\n\n\nTo be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\n\nSummary\nIn this notebook, we demonstrated how to use ANOVA for feature selection in the Fantasy Premier League dataset. By comparing the F-statistics of different features, we identified that ‘Goals Scored’ is a more significant feature than ‘Assists’ for differentiating player positions. Using SelectKBest from scikit-learn, we confirmed that ‘Goals Scored’ is the best feature among the two. This method can be applied to other datasets and features to enhance the performance of machine learning models.\n\n\nYou may also like:\n\n\n\nOverview of Machine Learning"
  },
  {
    "objectID": "posts/classification-metrics/index.html",
    "href": "posts/classification-metrics/index.html",
    "title": "Classification Metrics Practice",
    "section": "",
    "text": "Photo by DATAIDEA\nIn this notebook, we’ll walk through the process of building and evaluating a decision tree classifier using Scikit-Learn. We’ll use the Iris dataset for demonstration and then provide an exercise to apply the same steps to the Wine dataset."
  },
  {
    "objectID": "posts/classification-metrics/index.html#importing-necessary-libraries",
    "href": "posts/classification-metrics/index.html#importing-necessary-libraries",
    "title": "Classification Metrics Practice",
    "section": "Importing Necessary Libraries",
    "text": "Importing Necessary Libraries\nFirst, we import the necessary libraries for data manipulation and loading the dataset.\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\nnumpy and pandas are imported for data manipulation.\nload_iris from sklearn.datasets is imported to load the Iris dataset."
  },
  {
    "objectID": "posts/classification-metrics/index.html#loading-the-iris-dataset",
    "href": "posts/classification-metrics/index.html#loading-the-iris-dataset",
    "title": "Classification Metrics Practice",
    "section": "Loading the Iris Dataset",
    "text": "Loading the Iris Dataset\niris = load_iris()\nThe Iris dataset is loaded and stored in the variable iris."
  },
  {
    "objectID": "posts/classification-metrics/index.html#displaying-dataset-description",
    "href": "posts/classification-metrics/index.html#displaying-dataset-description",
    "title": "Classification Metrics Practice",
    "section": "Displaying Dataset Description",
    "text": "Displaying Dataset Description\nFor a better understanding of the dataset, we can uncomment the following line to print the description of the Iris dataset.\n## uncomment and run to read the data description\n# print(iris['DESCR'])"
  },
  {
    "objectID": "posts/classification-metrics/index.html#extracting-features-and-target-variables",
    "href": "posts/classification-metrics/index.html#extracting-features-and-target-variables",
    "title": "Classification Metrics Practice",
    "section": "Extracting Features and Target Variables",
    "text": "Extracting Features and Target Variables\nX = iris['data']\ny = iris['target']\n\nX contains the feature data (sepal length, sepal width, petal length, petal width).\ny contains the target data (class labels: 0, 1, 2)."
  },
  {
    "objectID": "posts/classification-metrics/index.html#importing-train-test-split-function",
    "href": "posts/classification-metrics/index.html#importing-train-test-split-function",
    "title": "Classification Metrics Practice",
    "section": "Importing Train-Test Split Function",
    "text": "Importing Train-Test Split Function\nfrom sklearn.model_selection import train_test_split\ntrain_test_split is imported to split the data into training and testing sets."
  },
  {
    "objectID": "posts/classification-metrics/index.html#splitting-the-data",
    "href": "posts/classification-metrics/index.html#splitting-the-data",
    "title": "Classification Metrics Practice",
    "section": "Splitting the Data",
    "text": "Splitting the Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nThe dataset is split into training (70%) and testing (30%) sets."
  },
  {
    "objectID": "posts/classification-metrics/index.html#importing-decision-tree-classifier",
    "href": "posts/classification-metrics/index.html#importing-decision-tree-classifier",
    "title": "Classification Metrics Practice",
    "section": "Importing Decision Tree Classifier",
    "text": "Importing Decision Tree Classifier\nNext, we import the Decision Tree classifier from Scikit-Learn.\nfrom sklearn.tree import DecisionTreeClassifier"
  },
  {
    "objectID": "posts/classification-metrics/index.html#initializing-the-classifier",
    "href": "posts/classification-metrics/index.html#initializing-the-classifier",
    "title": "Classification Metrics Practice",
    "section": "Initializing the Classifier",
    "text": "Initializing the Classifier\nWe create an instance of the Decision Tree classifier\nclassifier = DecisionTreeClassifier()"
  },
  {
    "objectID": "posts/classification-metrics/index.html#training-the-classifier",
    "href": "posts/classification-metrics/index.html#training-the-classifier",
    "title": "Classification Metrics Practice",
    "section": "Training the Classifier",
    "text": "Training the Classifier\nWe train the classifier using the training data.\nclassifier.fit(X_train, y_train)\n\n\n\nDecisionTreeClassifier()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted\n\nDecisionTreeClassifier()"
  },
  {
    "objectID": "posts/classification-metrics/index.html#making-predictions",
    "href": "posts/classification-metrics/index.html#making-predictions",
    "title": "Classification Metrics Practice",
    "section": "Making Predictions",
    "text": "Making Predictions\nWe then make predictions on the test data using the the predict() method on the model\npreds = classifier.predict(X_test)"
  },
  {
    "objectID": "posts/classification-metrics/index.html#importing-metrics-for-evaluation",
    "href": "posts/classification-metrics/index.html#importing-metrics-for-evaluation",
    "title": "Classification Metrics Practice",
    "section": "Importing Metrics for Evaluation",
    "text": "Importing Metrics for Evaluation\nTo evaluate our model, we import various metrics from Scikit-Learn.\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
  },
  {
    "objectID": "posts/classification-metrics/index.html#calculating-accuracy",
    "href": "posts/classification-metrics/index.html#calculating-accuracy",
    "title": "Classification Metrics Practice",
    "section": "Calculating Accuracy",
    "text": "Calculating Accuracy\nAccuracy refers to the proportion of correctly predicted instances out of the total instances.\naccuracy_score(y_test, preds)\n0.9777777777777777"
  },
  {
    "objectID": "posts/classification-metrics/index.html#calculating-precision",
    "href": "posts/classification-metrics/index.html#calculating-precision",
    "title": "Classification Metrics Practice",
    "section": "Calculating Precision",
    "text": "Calculating Precision\nPrecision is the ratio of correctly predicted positive observations to the total predicted positives.\nprecision_score(y_test, preds, average='weighted')\n0.9791666666666666"
  },
  {
    "objectID": "posts/classification-metrics/index.html#calculating-recall",
    "href": "posts/classification-metrics/index.html#calculating-recall",
    "title": "Classification Metrics Practice",
    "section": "Calculating Recall",
    "text": "Calculating Recall\nRecall is the ratio of correctly predicted positive observations to all the actual positives.\nrecall_score(y_test, preds, average='weighted')\n0.9777777777777777"
  },
  {
    "objectID": "posts/classification-metrics/index.html#calculating-f1-score",
    "href": "posts/classification-metrics/index.html#calculating-f1-score",
    "title": "Classification Metrics Practice",
    "section": "Calculating F1 Score",
    "text": "Calculating F1 Score\nThe f1 score refers to the Harmonic mean of Precision and Recall.\nf1_score(y_test, preds, average='weighted')\n0.9777530589543938"
  },
  {
    "objectID": "posts/classification-metrics/index.html#displaying-the-classification-report",
    "href": "posts/classification-metrics/index.html#displaying-the-classification-report",
    "title": "Classification Metrics Practice",
    "section": "Displaying the Classification Report",
    "text": "Displaying the Classification Report\nWe can print the classification report, which provides precision, recall, F1-score, and support for each class.\nfrom sklearn.metrics import classification_report\nclassification_report = classification_report(y_test, preds)\nprint(classification_report)\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        15\n           1       1.00      0.93      0.97        15\n           2       0.94      1.00      0.97        15\n\n    accuracy                           0.98        45\n   macro avg       0.98      0.98      0.98        45\nweighted avg       0.98      0.98      0.98        45\nThe results show how well the model performs in classifying the iris species, with metrics providing insights into different aspects of the model’s performance."
  },
  {
    "objectID": "posts/classification-metrics/index.html#exercise",
    "href": "posts/classification-metrics/index.html#exercise",
    "title": "Classification Metrics Practice",
    "section": "Exercise:",
    "text": "Exercise:\nPerform the steps above using the wine dataset from sklearn\n\n\nDon’t Miss Any Updates!\n\n\n\nTo be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\n\nYou may also like:\n\n\n\nHow the Decision Tree Classifier Works\n\n\n\n\nHow the Decision Tree Classifier Works"
  },
  {
    "objectID": "posts/2023/ml-model-deployment/index.html",
    "href": "posts/2023/ml-model-deployment/index.html",
    "title": "Exploring Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models is a crucial step in turning your data science projects into practical, real-world applications. With a variety of deployment options available, it’s essential to choose the one that best fits your needs. In this blog, we’ll explore the different deployment options and help you determine the best approach for your machine learning models.\n\n\nCloud platforms provide scalable and flexible environments for deploying machine learning models, making them a popular choice. Here’s a look at some of the leading cloud services:\nAmazon Web Services (AWS):\n\nSageMaker: A comprehensive service that offers tools to build, train, and deploy models at scale.\nLambda: Ideal for deploying lightweight models that need to respond to API requests.\nEC2 Instances: Customizable virtual servers suitable for deploying larger, more complex models.\n\nGoogle Cloud Platform (GCP):\n\nAI Platform: An integrated toolset designed for deploying and managing models efficiently.\nCloud Functions: A serverless environment perfect for lightweight model deployments.\nCompute Engine: Scalable virtual machines for more extensive and customized deployments.\n\nMicrosoft Azure:\n\nAzure Machine Learning: An end-to-end platform for building, training, and deploying models.\nAzure Functions: Serverless compute service ideal for deploying lightweight models.\nVirtual Machines: Provide a custom deployment environment tailored to your needs.\n\n\n\n\nFor organizations that require control over their infrastructure due to security, compliance, or performance needs, on-premises deployment is an excellent choice. Here are some options:\n\nDocker Containers: Ensures consistency across different environments by containerizing models.\nKubernetes: Manages and orchestrates containerized applications, offering scaling and management capabilities.\nDedicated Hardware: Utilizes specific hardware like GPUs or TPUs for high-performance requirements.\n\n\n\n\nEdge deployment is perfect for real-time processing and reduced latency by deploying models on edge devices like IoT devices and mobile phones. Some popular solutions include:\n\nTensorFlow Lite: Deploys TensorFlow models on mobile and embedded devices.\nONNX Runtime: Runs models trained in various frameworks on edge devices.\nNVIDIA Jetson: A platform for deploying AI models on edge devices with GPU acceleration.\n\n\n\n\nHybrid deployment combines multiple environments to leverage the advantages of each. For example, you might use the cloud for training and initial deployment but edge devices for inference to minimize latency and bandwidth usage.\n\n\n\nServerless computing allows you to deploy models without managing the underlying infrastructure. This option is ideal for applications with variable traffic and includes services like:\n\nAWS Lambda\nGoogle Cloud Functions\nAzure Functions\n\nThese services automatically scale with the load, simplifying the deployment process.\n\n\n\nExposing your model as a web service via RESTful or GraphQL APIs is a flexible and widely used approach. This allows various applications and services to access your model via HTTP requests. Some popular frameworks include:\n\nFlask/Django: Web application frameworks for Python.\nFastAPI: A modern, high-performance web framework for building APIs with Python 3.7+.\nTensorFlow Serving: Serves TensorFlow models as APIs, streamlining deployment.\n\n\n\n\nWhen real-time inference isn’t necessary, batch processing is a great option for handling large volumes of data at scheduled intervals. Key tools include:\n\nApache Spark: A unified analytics engine for large-scale data processing.\nHadoop: A framework for distributed storage and processing of big data."
  },
  {
    "objectID": "posts/2023/ml-model-deployment/index.html#exploring-options-for-deploying-machine-learning-models",
    "href": "posts/2023/ml-model-deployment/index.html#exploring-options-for-deploying-machine-learning-models",
    "title": "Exploring Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models is a crucial step in turning your data science projects into practical, real-world applications. With a variety of deployment options available, it’s essential to choose the one that best fits your needs. In this blog, we’ll explore the different deployment options and help you determine the best approach for your machine learning models.\n\n\nCloud platforms provide scalable and flexible environments for deploying machine learning models, making them a popular choice. Here’s a look at some of the leading cloud services:\nAmazon Web Services (AWS):\n\nSageMaker: A comprehensive service that offers tools to build, train, and deploy models at scale.\nLambda: Ideal for deploying lightweight models that need to respond to API requests.\nEC2 Instances: Customizable virtual servers suitable for deploying larger, more complex models.\n\nGoogle Cloud Platform (GCP):\n\nAI Platform: An integrated toolset designed for deploying and managing models efficiently.\nCloud Functions: A serverless environment perfect for lightweight model deployments.\nCompute Engine: Scalable virtual machines for more extensive and customized deployments.\n\nMicrosoft Azure:\n\nAzure Machine Learning: An end-to-end platform for building, training, and deploying models.\nAzure Functions: Serverless compute service ideal for deploying lightweight models.\nVirtual Machines: Provide a custom deployment environment tailored to your needs.\n\n\n\n\nFor organizations that require control over their infrastructure due to security, compliance, or performance needs, on-premises deployment is an excellent choice. Here are some options:\n\nDocker Containers: Ensures consistency across different environments by containerizing models.\nKubernetes: Manages and orchestrates containerized applications, offering scaling and management capabilities.\nDedicated Hardware: Utilizes specific hardware like GPUs or TPUs for high-performance requirements.\n\n\n\n\nEdge deployment is perfect for real-time processing and reduced latency by deploying models on edge devices like IoT devices and mobile phones. Some popular solutions include:\n\nTensorFlow Lite: Deploys TensorFlow models on mobile and embedded devices.\nONNX Runtime: Runs models trained in various frameworks on edge devices.\nNVIDIA Jetson: A platform for deploying AI models on edge devices with GPU acceleration.\n\n\n\n\nHybrid deployment combines multiple environments to leverage the advantages of each. For example, you might use the cloud for training and initial deployment but edge devices for inference to minimize latency and bandwidth usage.\n\n\n\nServerless computing allows you to deploy models without managing the underlying infrastructure. This option is ideal for applications with variable traffic and includes services like:\n\nAWS Lambda\nGoogle Cloud Functions\nAzure Functions\n\nThese services automatically scale with the load, simplifying the deployment process.\n\n\n\nExposing your model as a web service via RESTful or GraphQL APIs is a flexible and widely used approach. This allows various applications and services to access your model via HTTP requests. Some popular frameworks include:\n\nFlask/Django: Web application frameworks for Python.\nFastAPI: A modern, high-performance web framework for building APIs with Python 3.7+.\nTensorFlow Serving: Serves TensorFlow models as APIs, streamlining deployment.\n\n\n\n\nWhen real-time inference isn’t necessary, batch processing is a great option for handling large volumes of data at scheduled intervals. Key tools include:\n\nApache Spark: A unified analytics engine for large-scale data processing.\nHadoop: A framework for distributed storage and processing of big data."
  },
  {
    "objectID": "posts/2023/ml-model-deployment/index.html#key-considerations-for-model-deployment",
    "href": "posts/2023/ml-model-deployment/index.html#key-considerations-for-model-deployment",
    "title": "Exploring Options for Deploying Machine Learning Models",
    "section": "Key Considerations for Model Deployment",
    "text": "Key Considerations for Model Deployment\nWhen deciding on a deployment option, keep these factors in mind:\n\nScalability: Can the solution handle the expected load?\nLatency: Is real-time inference required?\nCost: What are the costs associated with running the model in production?\nSecurity: Does the deployment meet your security and compliance requirements?\nEase of Use: How straightforward is it to deploy, manage, and update the model?\nIntegration: How well does the deployment option integrate with your existing systems and workflows?\n\nSelecting the right deployment option is essential for balancing performance, cost, and operational complexity. By carefully considering your specific requirements and constraints, you can ensure a successful deployment of your machine learning models.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/2023/creating-venvs/index.html",
    "href": "posts/2023/creating-venvs/index.html",
    "title": "Python Virtual Environments",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nIn the vast ecosystem of Python programming, managing dependencies can sometimes be a daunting task. As projects grow in complexity, so does the need for a clean and isolated environment where dependencies can be installed without affecting other projects or the system-wide Python installation. This is where Python virtual environments come into play. In this guide, we’ll walk through the process of creating a virtual environment using venv and installing a package, like DataIdea, within it.\n\nWhat is a Python Virtual Environment?\nA virtual environment is a self-contained directory tree that contains a Python installation for a particular version of Python, plus a number of additional packages. It allows you to work on a Python project in isolation, ensuring that your project’s dependencies are maintained separately from other projects or the system Python.\n\n\nStep 1: Setting Up a Virtual Environment\nPython 3 comes with a built-in module called venv, which is used to create virtual environments. To create a virtual environment, open your terminal or command prompt and navigate to the directory where you want to create the environment. Then, run the following command:\n\nOn macOS and Linux:\n\npython3 -m venv myenv\n\nOn Windows:\n\npython -m venv myenv\nReplace myenv with the name you want to give to your virtual environment. This command will create a directory named myenv (or whatever name you provided) containing a Python interpreter and other necessary files.\n\n\n\n\n\n\n\nStep 2: Activating the Virtual Environment\nOnce the virtual environment is created, you need to activate it. Activation sets up the environment variables and modifies your shell prompt to indicate that you are now working within the virtual environment. Activate the virtual environment by running the appropriate command for your operating system:\n\nOn macOS and Linux:\n\nsource myenv/bin/activate\n\nOn Windows:\n\nmyenv\\Scripts\\activate\nYou’ll notice that your command prompt changes to show the name of the activated virtual environment.\n\n\n\n\n\n\n\nStep 3: Installing Packages\nWith the virtual environment activated, you can now install packages without affecting the global Python installation. Let’s install dataidea, as an example:\npip install dataidea\nyou can replace dataidea with another name of the package you want to install.\n\n\nStep 4: Using dataidea in Your Project\nOnce the package is installed, you can start using it in your Python project. Simply import it in your Python scripts as you would with any other package:\nimport dataidea\n\n\n\n\n\n\n\nStep 5: Deactivating the Virtual Environment\nWhen you’re done working on your project and want to leave the virtual environment, you can deactivate it by simply typing:\ndeactivate\n\n\nConclusion\nPython virtual environments are indispensable tools for managing dependencies and keeping project environments clean and isolated. With the venv module, creating and managing virtual environments has become easier than ever. By following the steps outlined in this guide, you can create a virtual environment, install packages like DataIdea, and develop Python projects with confidence. Happy coding!\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023/top-5-operating-systems/index.html",
    "href": "posts/2023/top-5-operating-systems/index.html",
    "title": "To 5 Operating Systems",
    "section": "",
    "text": "Thumbnail\nOperating systems are the backbone of modern computing, providing the fundamental software foundation for computers, smartphones, servers, and more. Here’s a comprehensive overview of five prominent operating systems that have made significant impacts in various domains."
  },
  {
    "objectID": "posts/2023/top-5-operating-systems/index.html#windows",
    "href": "posts/2023/top-5-operating-systems/index.html#windows",
    "title": "To 5 Operating Systems",
    "section": "Windows",
    "text": "Windows\n\nOverview\nDeveloped by Microsoft, Windows is one of the most widely used operating systems across PCs, servers, and embedded systems. Known for its user-friendly interface and extensive software compatibility, Windows offers a variety of versions tailored for different devices and use cases.\n\n\nFeatures\n\nGUI: Graphical User Interface for intuitive navigation.\nCompatibility: Broad support for software and hardware.\nSecurity: Regular updates and security features."
  },
  {
    "objectID": "posts/2023/top-5-operating-systems/index.html#macos",
    "href": "posts/2023/top-5-operating-systems/index.html#macos",
    "title": "To 5 Operating Systems",
    "section": "macOS",
    "text": "macOS\n\nOverview\nExclusive to Apple’s hardware, macOS offers a sleek and intuitive user experience. It is known for its stability, design aesthetics, and seamless integration with other Apple devices and services.\n\n\nFeatures\n\nIntegration: Seamless integration with other Apple devices.\nPerformance: Optimized for Mac hardware.\nSecurity: Built-in encryption and privacy features."
  },
  {
    "objectID": "posts/2023/top-5-operating-systems/index.html#linux",
    "href": "posts/2023/top-5-operating-systems/index.html#linux",
    "title": "To 5 Operating Systems",
    "section": "Linux",
    "text": "Linux\n\nOverview\nLinux is an open-source operating system available in various distributions (distros), catering to diverse user needs. Its flexibility, stability, and robustness have made it a favorite among developers, server administrators, and tech enthusiasts.\n\n\nFeatures\n\nOpen Source: Community-driven development and customization.\nVariety: Diverse distributions catering to different needs.\nStability: Known for reliability and performance."
  },
  {
    "objectID": "posts/2023/top-5-operating-systems/index.html#android",
    "href": "posts/2023/top-5-operating-systems/index.html#android",
    "title": "To 5 Operating Systems",
    "section": "Android",
    "text": "Android\n\nOverview\nDeveloped by Google, Android is the leading operating system for mobile devices globally. Its open-source nature, customizable interface, and extensive app ecosystem have made it a dominant force in the mobile market.\n\n\nFeatures\n\nCustomization: Ability to personalize interface and features.\nApp Ecosystem: Extensive library of applications.\nIntegration: Seamless integration with Google services."
  },
  {
    "objectID": "posts/2023/top-5-operating-systems/index.html#iosipados",
    "href": "posts/2023/top-5-operating-systems/index.html#iosipados",
    "title": "To 5 Operating Systems",
    "section": "iOS/iPadOS",
    "text": "iOS/iPadOS\n\nOverview\nExclusive to Apple’s mobile devices, iOS and iPadOS are renowned for their smooth performance, security features, and seamless integration within the Apple ecosystem.\n\n\nFeatures\n\nPerformance: Smooth and efficient operation on Apple devices.\nSecurity: Robust security measures and privacy controls.\nEcosystem Integration: Seamless integration with other Apple devices.\n\nIn conclusion, these operating systems showcase diversity in terms of architecture, platform support, and features, catering to the varied needs of users across different devices and industries.\n\n\nDon’t Miss Any Updates!\n\n\n\nTo be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel."
  },
  {
    "objectID": "posts/2022/handling-imbalanced-datasets/handling_imbalanced_data.html",
    "href": "posts/2022/handling-imbalanced-datasets/handling_imbalanced_data.html",
    "title": "Handling Imbalanced Datasets",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nHandling imbalanced datasets is a common challenge in machine learning, especially in classification tasks where one class significantly outnumbers the other(s). Let’s go through a simple example using the popular Iris dataset, which we’ll artificially imbalance for demonstration purposes.\nThe Iris dataset consists of 150 samples, each belonging to one of three classes: Iris Setosa, Iris Versicolour, and Iris Virginica. We’ll create an imbalanced version of this dataset where one class is underrepresented.\nFirst, let’s load the dataset and create the imbalance:\n# !pip install imbalanced-learn\n# !pip install --upgrade dataidea\nfrom dataidea.packages import pd, plt\nfrom sklearn.datasets import load_iris\n# Load Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Convert to DataFrame for manipulation\ndf = pd.DataFrame(data=np.c_[X, y],\n                  columns=iris.feature_names + ['target'])\n\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\ntarget\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n0.0\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n0.0\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n\n\n\n# Introduce imbalance by removing samples from one class\nclass_to_remove = 2  # Iris Virginica\nimbalance_ratio = 0.5  # Ratio of samples to be removed\nindices_to_remove = np.random.choice(df[df['target'] == class_to_remove].index,\n                                     size=int(imbalance_ratio * len(df[df['target'] == class_to_remove])),\n                                     replace=False)\ndf_imbalanced = df.drop(indices_to_remove)\n\n# Check the class distribution\nvalue_counts = df_imbalanced['target'].value_counts()\nprint(value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    25\nName: count, dtype: int64\nplt.bar(value_counts.index,\n        value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\nNow, df_imbalanced contains the imbalanced dataset. Next, we’ll demonstrate a few techniques to handle this imbalance:\n\nResampling Methods:\n\nOversampling: Randomly duplicate samples from the minority class.\nUndersampling: Randomly remove samples from the majority class.\n\nSynthetic Sampling Methods:\n\nSMOTE (Synthetic Minority Over-sampling Technique): Generates synthetic samples for the minority class.\n\nAlgorithmic Techniques:\n\nAlgorithm Tuning: Adjusting class weights in the algorithm.\nEnsemble Methods: Using ensemble techniques like bagging or boosting."
  },
  {
    "objectID": "posts/2022/handling-imbalanced-datasets/handling_imbalanced_data.html#handling-imbalanced-dataset",
    "href": "posts/2022/handling-imbalanced-datasets/handling_imbalanced_data.html#handling-imbalanced-dataset",
    "title": "Handling Imbalanced Datasets",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nHandling imbalanced datasets is a common challenge in machine learning, especially in classification tasks where one class significantly outnumbers the other(s). Let’s go through a simple example using the popular Iris dataset, which we’ll artificially imbalance for demonstration purposes.\nThe Iris dataset consists of 150 samples, each belonging to one of three classes: Iris Setosa, Iris Versicolour, and Iris Virginica. We’ll create an imbalanced version of this dataset where one class is underrepresented.\nFirst, let’s load the dataset and create the imbalance:\n# !pip install imbalanced-learn\n# !pip install --upgrade dataidea\nfrom dataidea.packages import pd, plt\nfrom sklearn.datasets import load_iris\n# Load Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Convert to DataFrame for manipulation\ndf = pd.DataFrame(data=np.c_[X, y],\n                  columns=iris.feature_names + ['target'])\n\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\ntarget\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n0.0\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n0.0\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n0.0\n\n\n\n\n\n\n\n# Introduce imbalance by removing samples from one class\nclass_to_remove = 2  # Iris Virginica\nimbalance_ratio = 0.5  # Ratio of samples to be removed\nindices_to_remove = np.random.choice(df[df['target'] == class_to_remove].index,\n                                     size=int(imbalance_ratio * len(df[df['target'] == class_to_remove])),\n                                     replace=False)\ndf_imbalanced = df.drop(indices_to_remove)\n\n# Check the class distribution\nvalue_counts = df_imbalanced['target'].value_counts()\nprint(value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    25\nName: count, dtype: int64\nplt.bar(value_counts.index,\n        value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\nNow, df_imbalanced contains the imbalanced dataset. Next, we’ll demonstrate a few techniques to handle this imbalance:\n\nResampling Methods:\n\nOversampling: Randomly duplicate samples from the minority class.\nUndersampling: Randomly remove samples from the majority class.\n\nSynthetic Sampling Methods:\n\nSMOTE (Synthetic Minority Over-sampling Technique): Generates synthetic samples for the minority class.\n\nAlgorithmic Techniques:\n\nAlgorithm Tuning: Adjusting class weights in the algorithm.\nEnsemble Methods: Using ensemble techniques like bagging or boosting."
  },
  {
    "objectID": "posts/2022/handling-imbalanced-datasets/handling_imbalanced_data.html#resampling",
    "href": "posts/2022/handling-imbalanced-datasets/handling_imbalanced_data.html#resampling",
    "title": "Handling Imbalanced Datasets",
    "section": "Resampling",
    "text": "Resampling\n\nOversampling\nLet’s implement oversampling using the imbalanced-learn library:\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Separate features and target\nX_imbalanced = df_imbalanced.drop('target', axis=1)\ny_imbalanced = df_imbalanced['target']\n\n# Apply Random Over-Sampling\noversample = RandomOverSampler(sampling_strategy='auto',\n                               random_state=42)\nX_resampled, y_resampled = oversample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after oversampling\noversampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(oversampled_data_value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\nplt.bar(oversampled_data_value_counts.index,\n        oversampled_data_value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\n\n\nUndersampling:\nUndersampling involves reducing the number of samples in the majority class to balance the dataset. Here’s how you can apply random undersampling using the imbalanced-learn library:\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Apply Random Under-Sampling\nundersample = RandomUnderSampler(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = undersample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after undersampling\nundersampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(undersampled_data_value_counts)\ntarget\n0.0    25\n1.0    25\n2.0    25\nName: count, dtype: int64\nplt.bar(undersampled_data_value_counts.index,\n        undersampled_data_value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "posts/2022/handling-imbalanced-datasets/handling_imbalanced_data.html#smote-synthetic-minority-over-sampling-technique",
    "href": "posts/2022/handling-imbalanced-datasets/handling_imbalanced_data.html#smote-synthetic-minority-over-sampling-technique",
    "title": "Handling Imbalanced Datasets",
    "section": "SMOTE (Synthetic Minority Over-sampling Technique)",
    "text": "SMOTE (Synthetic Minority Over-sampling Technique)\nSMOTE generates synthetic samples for the minority class by interpolating between existing minority class samples.\nHere’s how you can apply SMOTE using the imbalanced-learn library:\nfrom imblearn.over_sampling import SMOTE\n\n# Apply SMOTE\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after SMOTE\nsmoted_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(smoted_data_value_counts)\ntarget\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\nplt.bar(smoted_data_value_counts.index,\n        smoted_data_value_counts.values,\n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\npng\n\n\nBy using SMOTE, you can generate synthetic samples for the minority class, effectively increasing its representation in the dataset. This can help to mitigate the class imbalance issue and improve the performance of your machine learning model."
  },
  {
    "objectID": "posts/2022/handling-imbalanced-datasets/handling_imbalanced_data.html#algorithmic-techniques",
    "href": "posts/2022/handling-imbalanced-datasets/handling_imbalanced_data.html#algorithmic-techniques",
    "title": "Handling Imbalanced Datasets",
    "section": "Algorithmic Techniques",
    "text": "Algorithmic Techniques\n\nAlgorithm Tuning:\nMany algorithms allow you to specify class weights to penalize misclassifications of the minority class more heavily. Here’s an example using the class_weight parameter in a logistic regression classifier:\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_imbalanced, y_imbalanced,\n                                                    test_size=0.2, random_state=42)\n\n# Define the logistic regression classifier with class weights\nclass_weights = {0: 1, 1: 1, 2: 20}  # Penalize the minority class more heavily\nlog_reg = LogisticRegression(class_weight=class_weights)\n\n# Train the model\nlog_reg.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = log_reg.predict(X_test)\n\n# display classification report\npd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n\n\n\n\n\n\n\n\nprecision\n\n\nrecall\n\n\nf1-score\n\n\nsupport\n\n\n\n\n\n\n0.0\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n13.00\n\n\n\n\n1.0\n\n\n1.000000\n\n\n0.750000\n\n\n0.857143\n\n\n8.00\n\n\n\n\n2.0\n\n\n0.666667\n\n\n1.000000\n\n\n0.800000\n\n\n4.00\n\n\n\n\naccuracy\n\n\n0.920000\n\n\n0.920000\n\n\n0.920000\n\n\n0.92\n\n\n\n\nmacro avg\n\n\n0.888889\n\n\n0.916667\n\n\n0.885714\n\n\n25.00\n\n\n\n\nweighted avg\n\n\n0.946667\n\n\n0.920000\n\n\n0.922286\n\n\n25.00\n\n\n\n\n\nIn this example, the class weight for the minority class is increased to penalize misclassifications more heavily.\n\n\nEnsemble Methods\nEnsemble methods can also be effective for handling imbalanced datasets. Techniques such as bagging and boosting can improve the performance of classifiers, especially when dealing with imbalanced classes.\nHere’s an example of using ensemble methods like Random Forest, a popular bagging algorithm, with an imbalanced dataset:\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Define and train Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_rf = rf_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"Random Forest Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_rf, output_dict=True)).transpose()\nRandom Forest Classifier:\n\n\n\n\n\n\n\n\nprecision\n\n\nrecall\n\n\nf1-score\n\n\nsupport\n\n\n\n\n\n\n0.0\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n13.00\n\n\n\n\n1.0\n\n\n1.000000\n\n\n0.875000\n\n\n0.933333\n\n\n8.00\n\n\n\n\n2.0\n\n\n0.800000\n\n\n1.000000\n\n\n0.888889\n\n\n4.00\n\n\n\n\naccuracy\n\n\n0.960000\n\n\n0.960000\n\n\n0.960000\n\n\n0.96\n\n\n\n\nmacro avg\n\n\n0.933333\n\n\n0.958333\n\n\n0.940741\n\n\n25.00\n\n\n\n\nweighted avg\n\n\n0.968000\n\n\n0.960000\n\n\n0.960889\n\n\n25.00\n\n\n\n\n\nEnsemble methods like Random Forest build multiple decision trees and combine their predictions to make a final prediction. This can often lead to better generalization and performance, even in the presence of imbalanced classes.\n\n\nAdaBoost Classifier\nAnother ensemble method that specifically addresses class imbalance is AdaBoost (Adaptive Boosting). AdaBoost focuses more on those training instances that were previously misclassified, thus giving higher weight to the minority class instances.\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Define and train AdaBoost classifier\nada_classifier = AdaBoostClassifier(n_estimators=100, random_state=42)\nada_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_ada = ada_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"AdaBoost Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_ada, output_dict=True)).transpose()\nAdaBoost Classifier:\n\n\n/home/jumashafara/venvs/dataidea/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n  warnings.warn(\n\n\n\n\n\n\n\n\nprecision\n\n\nrecall\n\n\nf1-score\n\n\nsupport\n\n\n\n\n\n\n0.0\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n13.00\n\n\n\n\n1.0\n\n\n1.000000\n\n\n0.875000\n\n\n0.933333\n\n\n8.00\n\n\n\n\n2.0\n\n\n0.800000\n\n\n1.000000\n\n\n0.888889\n\n\n4.00\n\n\n\n\naccuracy\n\n\n0.960000\n\n\n0.960000\n\n\n0.960000\n\n\n0.96\n\n\n\n\nmacro avg\n\n\n0.933333\n\n\n0.958333\n\n\n0.940741\n\n\n25.00\n\n\n\n\nweighted avg\n\n\n0.968000\n\n\n0.960000\n\n\n0.960889\n\n\n25.00\n\n\n\n\n\nBy utilizing ensemble methods like Random Forest and AdaBoost, you can often achieve better performance on imbalanced datasets compared to individual classifiers, as these methods inherently mitigate the effects of class imbalance through their construction.\nThese are just a few techniques for handling imbalanced datasets. It’s crucial to experiment with different methods and evaluate their performance using appropriate evaluation metrics to find the best approach for your specific problem.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/2024/time-series-analysis/index.html",
    "href": "posts/2024/time-series-analysis/index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 2 of our Time Series Analysis Series, Part 1 introduces you to what time series is. The link to part 1 is here\nAs name suggest its analysis of the time series data to identify the patterns in it. I will briefly explain the different techniques and test for time series data analysis.\n\n\nWe have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.\n[ad]"
  },
  {
    "objectID": "posts/2024/time-series-analysis/index.html#time-series-analysis",
    "href": "posts/2024/time-series-analysis/index.html#time-series-analysis",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 2 of our Time Series Analysis Series, Part 1 introduces you to what time series is. The link to part 1 is here\nAs name suggest its analysis of the time series data to identify the patterns in it. I will briefly explain the different techniques and test for time series data analysis.\n\n\nWe have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.\n[ad]"
  },
  {
    "objectID": "posts/2024/time-series-analysis/index.html#decomposition-of-time-series",
    "href": "posts/2024/time-series-analysis/index.html#decomposition-of-time-series",
    "title": "Time Series Analysis",
    "section": "Decomposition of Time Series",
    "text": "Decomposition of Time Series\nTime series decomposition helps to deconstruct the time series into several component like trend and seasonality for better visualization of its characteristics. Using time-series decomposition makes it easier to quickly identify a changing mean or variation in the data\n\n\n\nDecomposition_of_Time_Series"
  },
  {
    "objectID": "posts/2024/time-series-analysis/index.html#stationary-data",
    "href": "posts/2024/time-series-analysis/index.html#stationary-data",
    "title": "Time Series Analysis",
    "section": "Stationary Data",
    "text": "Stationary Data\nFor accurate analysis and forecasting trend and seasonality is removed from the time series and converted it into stationary series. Time series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time.\n\n\n\nStationarity"
  },
  {
    "objectID": "posts/2024/time-series-analysis/index.html#test-for-stationarity",
    "href": "posts/2024/time-series-analysis/index.html#test-for-stationarity",
    "title": "Time Series Analysis",
    "section": "Test for Stationarity",
    "text": "Test for Stationarity\nEasy way is to look at the plot and look for any obvious trend or seasonality. While working on real world data we can also use more sophisticated methods like rolling statistic and Augmented Dickey Fuller test to check stationarity of the data.\n\nRolling Statistics\nIn rolling statistics technique we define a size of window to calculate the mean and standard deviation throughout the series. For stationary series mean and standard deviation shouldn’t change with time.\n\n\nAugmented Dickey Fuller (ADF) Test\nI won’t go into the details of how this test works. I will concentrate more on how to interpret the result of this test to determine the stationarity of the series. ADF test will return ‘p-value’ and ‘Test Statistics’ output values.\n\np-value &gt; 0.05: non-stationary.\np-value &lt;= 0.05: stationary.\nTest statistics: More negative this value more likely we have stationary series. Also, this value should be smaller than critical values(1%, 5%, 10%). For e.g. If test statistic is smaller than the 5% critical values, then we can say with 95% confidence that this is a stationary series\n\n[ad]"
  },
  {
    "objectID": "posts/2024/time-series-analysis/index.html#convert-non-stationary-data-to-stationary-data",
    "href": "posts/2024/time-series-analysis/index.html#convert-non-stationary-data-to-stationary-data",
    "title": "Time Series Analysis",
    "section": "Convert Non-Stationary Data to Stationary Data",
    "text": "Convert Non-Stationary Data to Stationary Data\nAccounting for the time series data characteristics like trend and seasonality is called as making data stationary. So by making the mean and variance of the time series constant, we will get the stationary data. Below are the few technique used for the same…\n\nDifferencing\nDifferencing technique helps to remove the trend and seasonality from time series data. Differencing is performed by subtracting the previous observation from the current observation. The differenced data will contain one less data point than original data. So differencing actually reduces the number of observations and stabilize the mean of a time series.\n\\[d = t - t0\\]\nAfter performing the differencing it’s recommended to plot the data and visualize the change. In case there is not sufficient improvement you can perform second order or even third order differencing.\n\n\nTransformation\nA simple but often effective way to stabilize the variance across time is to apply a power transformation to the time series. Log, square root, cube root are most commonly used transformation techniques. Most of the time you can pick the type of growth of the time series and accordingly choose the transformation method. For. e.g. A time series that has a quadratic growth trend can be made linear by taking the square root. In case differencing don’t work, you may first want to use one of above transformation technique to remove the variation from the series.\n\n\n\nLog_Transformation\n\n\n\n\nMoving Average\nIn moving averages technique, a new series is created by taking the averages of data points from original series. In this technique we can use two or more raw data points to calculate the average. This is also called as ‘window width (w)’. Once window width is decided, averages are calculated from start to the end for each set of w consecutive values, hence the name moving averages. It can also be used for time series forecasting.\n\n\n\nMoving_Average\n\n\n\nWeighted Moving Averages(WMA)\nWMA is a technical indicator that assigns a greater weighting to the most recent data points, and less weighting to data points in the distant past. The WMA is obtained by multiplying each number in the data set by a predetermined weight and summing up the resulting values. There can be many techniques for assigning weights. A popular one is exponentially weighted moving average where weights are assigned to all the previous values with a decay factor.\n\n\nCentered Moving Averages(CMS)\nIn a centered moving average, the value of the moving average at time t is computed by centering the window around time t and averaging across the w values within the window. For example, a center moving average with a window of 3 would be calculated as\n\\[CMA(t) = mean(t-1, t, t+1)\\]\nCMA is very useful for visualizing the time series data\n\n\nTrailing Moving Averages(TMA)\nIn trailing moving average, instead of averaging over a window that is centered around a time period of interest, it simply takes the average of the last w values. For example, a trailing moving average with a window of 3 would be calculated as:\n\\[TMA(t) = mean(t-2, t-1, t)\\]\nTMA are useful for forecasting.\n[ad]"
  },
  {
    "objectID": "posts/2024/time-series-analysis/index.html#correlation",
    "href": "posts/2024/time-series-analysis/index.html#correlation",
    "title": "Time Series Analysis",
    "section": "Correlation",
    "text": "Correlation\n\nMost important point about values in time series is its dependence on the previous values.\nWe can calculate the correlation for time series observations with previous time steps, called as lags.\nBecause the correlation of the time series observations is calculated with values of the same series at previous times, this is called an autocorrelation or serial correlation.\nTo understand it better lets consider the example of fish prices. We will use below notation to represent the fish prices.\n\n\\(P(t)\\)= Fish price of today\n\\(P(t-1)\\) = Fish price of last month\n\\(P(t-2)\\) =Fish price of last to last month\n\nTime series of fish prices can be represented as \\(P(t-n),..... P(t-3), P(t-2),P(t-1), P(t)\\)\nSo if we have fish prices for last few months then it will be easy for us to predict the fish price for today (Here we are ignoring all other external factors that may affect the fish prices)\n\nAll the past and future data points are related in time series and ACF and PACF functions help us to determine correlation in it.\n\nAuto Correlation Function (ACF)\n\nACF tells you how correlated points are with each other, based on how many time steps they are separated by.\nNow to understand it better lets consider above example of fish prices. Let’s try to find the correlation between fish price for current month P(t) and two months ago P(t-2). Important thing to note that, fish price of two months ago can directly affect the today’s fish price or it can indirectly affect the fish price through last months price P(t-1)\nSo ACF consider the direct as well indirect effect between the points while determining the correlation\n\n\n\nPartial Auto Correlation Function (PACF)\n\nUnlike ACF, PACF only consider the direct effect between the points while determining the correlation\nIn case of above fish price example PACF will determine the correlation between fish price for current month P(t) and two months ago P(t-2) by considering only P(t) and P(t-2) and ignoring P(t-1)\n\n[ad]"
  },
  {
    "objectID": "posts/2024/bffill-and-ffill-for-handling-missing-data/index.html",
    "href": "posts/2024/bffill-and-ffill-for-handling-missing-data/index.html",
    "title": "Handling Missing Data in Pandas, When to Use bfill and ffill Methods",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThe bfill (backward fill) and ffill (forward fill) methods are used in data analysis and manipulation, particularly for handling missing data in pandas DataFrames or Series. Here’s a detailed explanation of when and how to use each method:\n\nffill (Forward Fill)\nDescription: ffill propagates the last valid observation forward to the next valid one.\nUse Cases:\n\nTime Series Data: When dealing with time series data, if a certain value is missing, it might be logical to assume that the last observed value remains in effect until a new value is observed. For example, if you have daily stock prices and some days are missing, you might want to fill those missing days with the last known stock price.\nData Smoothing: In some cases, for smoothing purposes, forward filling can help maintain a continuous data series without introducing significant bias.\nSurvey Data: In survey data, if a respondent skips a question but answers the following ones, you might assume their previous answer holds until they provide a new one.\n\nExample:\nimport pandas as pd\n\n# Sample data with missing values\ndata = pd.Series([1, 2, None, 4, None, None, 7])\nfilled_data = data.ffill()\nprint(filled_data)\nOutput:\n0    1.0\n1    2.0\n2    2.0\n3    4.0\n4    4.0\n5    4.0\n6    7.0\ndtype: float64\n\n\n\n\n\n\nbfill (Backward Fill)\nDescription: bfill propagates the next valid observation backward to fill the missing values.\nUse Cases:\n\nPredictive Models: In certain predictive models, you might want to assume the next known value affects the previous unknown period. This can be useful when the data naturally reflects future events impacting current states.\nData Preparation: In data preparation, backward filling can sometimes be used to prepare data for algorithms that require no missing values, ensuring that any gap is filled with the next available data point.\nInterim Reporting: When generating interim reports, you might use backward fill to assume that future values (like projected sales or stock levels) should be filled backward to reflect estimates.\n\nExample:\nimport pandas as pd\n\n# Sample data with missing values\ndata = pd.Series([1, 2, None, 4, None, None, 7])\nfilled_data = data.bfill()\nprint(filled_data)\nOutput:\n0    1.0\n1    2.0\n2    4.0\n3    4.0\n4    7.0\n5    7.0\n6    7.0\ndtype: float64\n\n\n\n\n\n\nChoosing Between ffill and bfill\n\nContext: Choose based on the context and the logical assumption that fits the nature of your data. If the past influences the present, use ffill. If the future influences the present, use bfill.\nData Patterns: Consider the patterns in your data and what makes sense for your specific analysis or model. Ensure that the method you choose maintains the integrity and meaning of your data.\n\nIn summary, use ffill to carry forward the last known value to fill missing data points and bfill to use the next known value to fill previous missing data points. Both methods are useful for maintaining data continuity and preparing datasets for analysis.\n\n\n\n\n\n\nYou may also like:\n\n\n\nHandling Missing Data\n\n \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html",
    "href": "posts/2024/classification-metrics/index.html",
    "title": "Classification Metrics Practice",
    "section": "",
    "text": "Photo by DATAIDEA\nIn this notebook, we’ll walk through the process of building and evaluating a decision tree classifier using Scikit-Learn. We’ll use the Iris dataset for demonstration and then provide an exercise to apply the same steps to the Wine dataset."
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#importing-necessary-libraries",
    "href": "posts/2024/classification-metrics/index.html#importing-necessary-libraries",
    "title": "Classification Metrics Practice",
    "section": "Importing Necessary Libraries",
    "text": "Importing Necessary Libraries\nFirst, we import the necessary libraries for data manipulation and loading the dataset.\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\nnumpy and pandas are imported for data manipulation.\nload_iris from sklearn.datasets is imported to load the Iris dataset."
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#loading-the-iris-dataset",
    "href": "posts/2024/classification-metrics/index.html#loading-the-iris-dataset",
    "title": "Classification Metrics Practice",
    "section": "Loading the Iris Dataset",
    "text": "Loading the Iris Dataset\niris = load_iris()\nThe Iris dataset is loaded and stored in the variable iris."
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#displaying-dataset-description",
    "href": "posts/2024/classification-metrics/index.html#displaying-dataset-description",
    "title": "Classification Metrics Practice",
    "section": "Displaying Dataset Description",
    "text": "Displaying Dataset Description\nFor a better understanding of the dataset, we can uncomment the following line to print the description of the Iris dataset.\n## uncomment and run to read the data description\n# print(iris['DESCR'])"
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#extracting-features-and-target-variables",
    "href": "posts/2024/classification-metrics/index.html#extracting-features-and-target-variables",
    "title": "Classification Metrics Practice",
    "section": "Extracting Features and Target Variables",
    "text": "Extracting Features and Target Variables\nX = iris['data']\ny = iris['target']\n\nX contains the feature data (sepal length, sepal width, petal length, petal width).\ny contains the target data (class labels: 0, 1, 2)."
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#importing-train-test-split-function",
    "href": "posts/2024/classification-metrics/index.html#importing-train-test-split-function",
    "title": "Classification Metrics Practice",
    "section": "Importing Train-Test Split Function",
    "text": "Importing Train-Test Split Function\nfrom sklearn.model_selection import train_test_split\ntrain_test_split is imported to split the data into training and testing sets."
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#splitting-the-data",
    "href": "posts/2024/classification-metrics/index.html#splitting-the-data",
    "title": "Classification Metrics Practice",
    "section": "Splitting the Data",
    "text": "Splitting the Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nThe dataset is split into training (70%) and testing (30%) sets."
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#importing-decision-tree-classifier",
    "href": "posts/2024/classification-metrics/index.html#importing-decision-tree-classifier",
    "title": "Classification Metrics Practice",
    "section": "Importing Decision Tree Classifier",
    "text": "Importing Decision Tree Classifier\nNext, we import the Decision Tree classifier from Scikit-Learn.\nfrom sklearn.tree import DecisionTreeClassifier"
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#initializing-the-classifier",
    "href": "posts/2024/classification-metrics/index.html#initializing-the-classifier",
    "title": "Classification Metrics Practice",
    "section": "Initializing the Classifier",
    "text": "Initializing the Classifier\nWe create an instance of the Decision Tree classifier\nclassifier = DecisionTreeClassifier()"
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#training-the-classifier",
    "href": "posts/2024/classification-metrics/index.html#training-the-classifier",
    "title": "Classification Metrics Practice",
    "section": "Training the Classifier",
    "text": "Training the Classifier\nWe train the classifier using the training data.\nclassifier.fit(X_train, y_train)\n\n\n\nDecisionTreeClassifier()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted\n\nDecisionTreeClassifier()"
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#making-predictions",
    "href": "posts/2024/classification-metrics/index.html#making-predictions",
    "title": "Classification Metrics Practice",
    "section": "Making Predictions",
    "text": "Making Predictions\nWe then make predictions on the test data using the the predict() method on the model\npreds = classifier.predict(X_test)"
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#importing-metrics-for-evaluation",
    "href": "posts/2024/classification-metrics/index.html#importing-metrics-for-evaluation",
    "title": "Classification Metrics Practice",
    "section": "Importing Metrics for Evaluation",
    "text": "Importing Metrics for Evaluation\nTo evaluate our model, we import various metrics from Scikit-Learn.\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#calculating-accuracy",
    "href": "posts/2024/classification-metrics/index.html#calculating-accuracy",
    "title": "Classification Metrics Practice",
    "section": "Calculating Accuracy",
    "text": "Calculating Accuracy\nAccuracy refers to the proportion of correctly predicted instances out of the total instances.\naccuracy_score(y_test, preds)\n0.9777777777777777"
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#calculating-precision",
    "href": "posts/2024/classification-metrics/index.html#calculating-precision",
    "title": "Classification Metrics Practice",
    "section": "Calculating Precision",
    "text": "Calculating Precision\nPrecision is the ratio of correctly predicted positive observations to the total predicted positives.\nprecision_score(y_test, preds, average='weighted')\n0.9791666666666666"
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#calculating-recall",
    "href": "posts/2024/classification-metrics/index.html#calculating-recall",
    "title": "Classification Metrics Practice",
    "section": "Calculating Recall",
    "text": "Calculating Recall\nRecall is the ratio of correctly predicted positive observations to all the actual positives.\nrecall_score(y_test, preds, average='weighted')\n0.9777777777777777"
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#calculating-f1-score",
    "href": "posts/2024/classification-metrics/index.html#calculating-f1-score",
    "title": "Classification Metrics Practice",
    "section": "Calculating F1 Score",
    "text": "Calculating F1 Score\nThe f1 score refers to the Harmonic mean of Precision and Recall.\nf1_score(y_test, preds, average='weighted')\n0.9777530589543938"
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#displaying-the-classification-report",
    "href": "posts/2024/classification-metrics/index.html#displaying-the-classification-report",
    "title": "Classification Metrics Practice",
    "section": "Displaying the Classification Report",
    "text": "Displaying the Classification Report\nWe can print the classification report, which provides precision, recall, F1-score, and support for each class.\nfrom sklearn.metrics import classification_report\nclassification_report = classification_report(y_test, preds)\nprint(classification_report)\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        15\n           1       1.00      0.93      0.97        15\n           2       0.94      1.00      0.97        15\n\n    accuracy                           0.98        45\n   macro avg       0.98      0.98      0.98        45\nweighted avg       0.98      0.98      0.98        45\nThe results show how well the model performs in classifying the iris species, with metrics providing insights into different aspects of the model’s performance."
  },
  {
    "objectID": "posts/2024/classification-metrics/index.html#exercise",
    "href": "posts/2024/classification-metrics/index.html#exercise",
    "title": "Classification Metrics Practice",
    "section": "Exercise:",
    "text": "Exercise:\nPerform the steps above using the wine dataset from sklearn\n\n\nDon’t Miss Any Updates!\n\n\n\nTo be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\n\nYou may also like:\n\n\n\nHow the Decision Tree Classifier Works\n\n\n\n\nHow the Decision Tree Classifier Works"
  },
  {
    "objectID": "posts/2024/time-series-forecasting/index.html",
    "href": "posts/2024/time-series-forecasting/index.html",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 3 of our Time Series Analysis Series, Part 2 introduces you to doing time series analysis. The link to part 2 is here\nForecasting refers to the future predictions based on the time series data analysis. Below are the steps performed during time series forecasting\n\nStep 1: Understand the time series characteristics like trend, seasonality etc\nStep 2: Do the analysis and identify the best method to make the time series stationary\nStep 3: Note down the transformation steps performed to make the time series stationary and make sure that the reverse transformation of data is possible to get the original scale back\nStep 4: Based on data analysis choose the appropriate model for time series forecasting\nStep 5: We can assess the performance of a model by applying simple metrics such as residual sum of squares(RSS). Make sure to use whole data for prediction.\nStep 6: Now we will have an array of predictions which are in transformed scale. We just need to apply the reverse transformation to get the prediction values in original scale.\nStep 7: At the end we can do the future forecasting and get the future forecasted values in original scale."
  },
  {
    "objectID": "posts/2024/time-series-forecasting/index.html#time-series-forecasting",
    "href": "posts/2024/time-series-forecasting/index.html#time-series-forecasting",
    "title": "Time Series Forecasting",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nThis is part 3 of our Time Series Analysis Series, Part 2 introduces you to doing time series analysis. The link to part 2 is here\nForecasting refers to the future predictions based on the time series data analysis. Below are the steps performed during time series forecasting\n\nStep 1: Understand the time series characteristics like trend, seasonality etc\nStep 2: Do the analysis and identify the best method to make the time series stationary\nStep 3: Note down the transformation steps performed to make the time series stationary and make sure that the reverse transformation of data is possible to get the original scale back\nStep 4: Based on data analysis choose the appropriate model for time series forecasting\nStep 5: We can assess the performance of a model by applying simple metrics such as residual sum of squares(RSS). Make sure to use whole data for prediction.\nStep 6: Now we will have an array of predictions which are in transformed scale. We just need to apply the reverse transformation to get the prediction values in original scale.\nStep 7: At the end we can do the future forecasting and get the future forecasted values in original scale."
  },
  {
    "objectID": "posts/2024/time-series-forecasting/index.html#models-used-for-time-series-forecasting",
    "href": "posts/2024/time-series-forecasting/index.html#models-used-for-time-series-forecasting",
    "title": "Time Series Forecasting",
    "section": "Models Used For Time Series Forecasting",
    "text": "Models Used For Time Series Forecasting\n\nAutoregression (AR)\nMoving Average (MA)\nAutoregressive Moving Average (ARMA)\nAutoregressive Integrated Moving Average (ARIMA)\nSeasonal Autoregressive Integrated Moving-Average (SARIMA)\nSeasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)\nVector Autoregression (VAR)\nVector Autoregression Moving-Average (VARMA)\nVector Autoregression Moving-Average with Exogenous Regressors (VARMAX)\nSimple Exponential Smoothing (SES)\nHolt Winter’s Exponential Smoothing (HWES)\n\nNext part of this article we are going to analyze and forecast air passengers time series data using ARIMA model. Brief introduction of ARIMA model is as below\n[ad]"
  },
  {
    "objectID": "posts/2024/time-series-forecasting/index.html#arima",
    "href": "posts/2024/time-series-forecasting/index.html#arima",
    "title": "Time Series Forecasting",
    "section": "ARIMA",
    "text": "ARIMA\n\nARIMA stands for Auto-Regressive Integrated Moving Averages. It is actually a combination of AR and MA model.\nARIMA has three parameters ‘p’ for the order of Auto-Regressive (AR) part, ‘q’ for the order of Moving Average (MA) part and ‘d’ for the order of integrated part.\n\n\nAuto-Regressive (AR) Model:\n\nAs the name indicates, its the regression of the variables against itself. In this model linear combination of the past values are used to forecast the future values.\nTo figure out the order of AR model we will use PACF function\n\n\n\nIntegration(I):\n\nUses differencing of observations (subtracting an observation from observation at the previous time step) in order to make the time series stationary. Differencing involves the subtraction of the current values of a series with its previous values \\(d\\) number of times.\nMost of the time value of \\(d = 1\\), means first order of difference.\n\n\n\nMoving Average (MA) Model:\n\nRather than using past values of the forecast variable in a regression, a moving average model uses linear combination of past forecast errors\nTo figure out the order of MA model we will use ACF function\n\n[ad]"
  },
  {
    "objectID": "posts/2024/overview-of-machine-learning/index.html",
    "href": "posts/2024/overview-of-machine-learning/index.html",
    "title": "Overview of Machine Learning",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/2024/overview-of-machine-learning/index.html#references",
    "href": "posts/2024/overview-of-machine-learning/index.html#references",
    "title": "Overview of Machine Learning",
    "section": "References",
    "text": "References\n\nDATAIDEA - What is Demographic Data\nIBM - What is Machine Learning\nData Camp - What is Machine Learning\n\n\n\nYou may also like:\n\n\n\nWhat is Demographic Data"
  },
  {
    "objectID": "posts/2024/time-series-intro/index.html",
    "href": "posts/2024/time-series-intro/index.html",
    "title": "What is Time Series",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nAny data recorded with some fixed interval of time is called as time series data. This fixed interval can be hourly, daily, monthly or yearly. e.g. hourly temperature reading, daily changing fuel prices, monthly electricity bill, annul company profit report etc. In time series data, time will always be independent variable and there can be one or many dependent variable.\nSales forecasting time series with shampoo sales for every month will look like this,\n\n\n\nShampoo_Sales\n\n\nIn above example since there is only one variable dependent on time so its called as univariate time series. If there are multiple dependent variables, then its called as multivariate time series.\nObjective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals.\n[ad]"
  },
  {
    "objectID": "posts/2024/time-series-intro/index.html#what-is-time-series",
    "href": "posts/2024/time-series-intro/index.html#what-is-time-series",
    "title": "What is Time Series",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nAny data recorded with some fixed interval of time is called as time series data. This fixed interval can be hourly, daily, monthly or yearly. e.g. hourly temperature reading, daily changing fuel prices, monthly electricity bill, annul company profit report etc. In time series data, time will always be independent variable and there can be one or many dependent variable.\nSales forecasting time series with shampoo sales for every month will look like this,\n\n\n\nShampoo_Sales\n\n\nIn above example since there is only one variable dependent on time so its called as univariate time series. If there are multiple dependent variables, then its called as multivariate time series.\nObjective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals.\n[ad]"
  },
  {
    "objectID": "posts/2024/time-series-intro/index.html#time-series-characteristics",
    "href": "posts/2024/time-series-intro/index.html#time-series-characteristics",
    "title": "What is Time Series",
    "section": "Time Series Characteristics",
    "text": "Time Series Characteristics\nMean, standard deviation and seasonality defines different characteristics of the time series.\n\n\n\nTime_Series_Characteristics\n\n\nImportant characteristics of the time series are as below\n\nTrend\nTrend represent the change in dependent variables with respect to time from start to end. In case of increasing trend dependent variable will increase with time and vice versa. It’s not necessary to have definite trend in time series, we can have a single time series with increasing and decreasing trend. In short trend represent the varying mean of time series data.\n\n\n\nTrend\n\n\n\n\nSeasonality\nIf observations repeats after fixed time interval then they are referred as seasonal observations. These seasonal changes in data can occur because of natural events or man-made events. For example every year warm cloths sales increases just before winter season. So seasonality represent the data variations at fixed intervals.\n\n\n\nSeasonality\n\n\n\n\nIrregularities\nThis is also called as noise. Strange dips and jump in the data are called as irregularities. These fluctuations are caused by uncontrollable events like earthquakes, wars, flood, pandemic etc. For example because of COVID-19 pandemic there is huge demand for hand sanitizers and masks.\n\n\n\nIrregularities\n\n\n\n\nCyclicity\nCyclicity occurs when observations in the series repeats in random pattern. Note that if there is any fixed pattern then it becomes seasonality, in case of cyclicity observations may repeat after a week, months or may be after a year. These kinds of patterns are much harder to predict.\n\n\n\nCyclicity\n\n\nTime series data which has above characteristics is called as ‘Non-Stationary Data’. For any analysis on time series data we must convert it to ‘Stationary Data’\nThe general guideline is to estimate the trend and seasonality in the time series, and then make the time series stationary for data modeling. In data modeling step statistical techniques are used for time series analysis and forecasting. Once we have the predictions, in the final step forecasted values converted into the original scale by applying trend and seasonality constraints back.\n[ad]\n\n\n\n\n\nPart 2 of this Time Series Analysis series will introduce you to doing time series analysis. The link to part 2 is here"
  },
  {
    "objectID": "posts/2024/fpl_standings/index.html",
    "href": "posts/2024/fpl_standings/index.html",
    "title": "Update on DATAIDEA Fantasy Football League Standings",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nHey Fantasy Football aficionados,\nIt’s time for another exciting update on the DATAIDEA Fantasy Football League! With each matchweek passing by, the competition has been heating up, and the leaderboard has seen its fair share of twists and turns. Let’s dive straight into the latest standings and highlights from the league.\n\nCurrent Standings:\nHere’s how the teams stack up on the leaderboard:\n\n\n\nRank\nTeam & Manager\nGW\nTOT\n\n\n\n\n1\nBARÇA ya KAHUNGYE\n47\n2089\n\n\n2\nAŁbramo FC\n61\n2053\n\n\n3\nLethal Weapon\n53\n2009\n\n\n4\nBlyckfc\n30\n2002\n\n\n5\nBrine FC\n17\n1959\n\n\n6\nJ T\n78\n1958\n\n\n7\nJumaShafara@DATAIDEA\n43\n1951\n\n\n8\nRwenzori\n59\n1949\n\n\n9\nGem FC\n59\n1911\n\n\n10\nGULU UNITED F.C\n38\n1908\n\n\n11\nAJ12\n45\n1887\n\n\n12\nLegends FC\n44\n1823\n\n\n13\nM142 HIMARS\n56\n1804\n\n\n14\nColaz guluz\n42\n1781\n\n\n15\nPaperChaser of Uganda\n34\n1732\n\n\n16\nAFC Adventurer\n39\n1669\n\n\n\n\n\nHighlights:\n\nBARCA ya KAHUNGYE continues to dominate the league, maintaining their top position with an impressive total score of 2042 points.\nAlBramo FC is trailing closely behind in second place, displaying consistent performance throughout the season.\nJumaShafara@DATAIDEA has climbed up the ranks to secure the fifth position, showcasing the competitive spirit of DATAIDEA’s own fantasy football enthusiasts.\n\n\n\nLooking Ahead:\nAs the league progresses, the competition is only expected to intensify further. With managers strategizing and making crucial transfers, every matchweek brings forth new challenges and opportunities for teams to climb the ranks.\nStay tuned for more updates as the DATAIDEA Fantasy Football League unfolds its thrilling saga of goals, assists, and tactical maneuvers.\nUntil next time, may your fantasy team flourish on the virtual pitch!\nBest regards,\nJuma Shafara\nInstructor, DATAIDEA\n+256771754118 / jumashafara0@gmail.com\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024/who-will-win-the-euros-2024/index.html",
    "href": "posts/2024/who-will-win-the-euros-2024/index.html",
    "title": "Who Will Win Euro 2024? The Opta Predictions",
    "section": "",
    "text": "Image by Opta\n\n\nEuro 2024 is set to be a thrilling tournament, and the Opta Supercomputer’s pre-tournament predictions give us a fascinating insight into the likely outcomes. Here’s a summary of the key predictions and contenders:\n\nFavorites to Win\n\nEngland (19.9%)\n\nKey Players: Harry Kane, Jude Bellingham, Phil Foden\nGroup: Denmark, Serbia, Slovenia\nChances: England are favorites to win, with strong attacking talent. They have a high probability (95.4%) of reaching the last 16, and a significant chance of progressing to the quarter-finals (70.0%) and semi-finals (48.2%). They have a 19.9% chance of winning the tournament.\n\nFrance (19.1%)\n\nKey Players: Kylian Mbappé, Antoine Griezmann\nGroup: Netherlands, other teams\nChances: France are close behind England as favorites. They have a 69.2% chance of reaching the quarter-finals and a 48.1% chance of making it to the semi-finals. Their probability of winning the tournament stands at 19.1%.\n\nGermany (12.4%)\n\nKey Players: Manuel Neuer, Toni Kroos, Thomas Müller, Kai Havertz\nGroup: Other teams\nChances: Despite recent struggles, Germany are strong contenders with a home advantage. They have a 36.5% chance of reaching the semi-finals and a 12.4% chance of winning the tournament.\n\nSpain (9.6%)\n\nKey Players: Álvaro Morata, other key players\nGroup: Italy, Croatia, Albania\nChances: Spain are seen as strong contenders with a 59.1% chance of reaching the quarter-finals. They have a 9.6% chance of winning the tournament.\n\nPortugal (9.2%)\n\nKey Players: Cristiano Ronaldo, Bruno Fernandes\nGroup: Czech Republic, Turkey, Georgia\nChances: Portugal have a high likelihood (93.6%) of progressing from the group stage and a 33.6% chance of reaching the semi-finals. Their probability of winning is 9.2%.\n\n\n\n\nOther Contenders\n\nNetherlands (5.1%)\nItaly (5.0%)\nBelgium (4.7%)\n\n\n\nSummary\nThe Opta Supercomputer simulations highlight England and France as the top favorites to win Euro 2024, with Germany, Spain, and Portugal also having strong chances. The tournament promises intense competition with key matchups and potential surprises. Fans can look forward to an exciting summer of football as these top nations vie for European glory.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024/cost-function-in-machine-learning/index.html",
    "href": "posts/2024/cost-function-in-machine-learning/index.html",
    "title": "Understanding the Cost Function in Linear Regression",
    "section": "",
    "text": "Image by DATAIDEA\n\n\nLinear regression is a fundamental algorithm in machine learning and statistics, used to model the relationship between a dependent variable and one or more independent variables. At the heart of linear regression lies the concept of the cost function, a crucial element that helps the model learn and make accurate predictions. In this article, I’ll get into what a cost function is, why it’s important, and how it works in the context of linear regression.\n\nWhat is a Cost Function?\nA cost function, also known as a loss function or error function, quantifies the error between predicted values and actual values. It is a mathematical function that the model aims to minimize during the training process. By minimizing the cost function, the model adjusts its parameters (weights and biases) to produce the most accurate predictions possible.\n\n\n\n\n\n\nLinear Regression Recap\nBefore I go deep into the cost function, let me briefly revisit the basics of linear regression. The goal of linear regression is to find the best-fitting straight line through the data points, which can be represented by the equation:\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon\\]\nWhere:\n\n\\(y\\) is the dependent variable (the outcome we’re trying to predict).\n\\(x\\) is the independent variable (the feature or input).\n\\(\\beta_0\\) is the y-intercept (the value of \\(y\\) when \\(x\\) is zero).\n\\(\\beta_1\\) is the slope of the line (the change in \\(y\\) for a unit change in \\(x\\)).\n\\(\\epsilon\\) is the error term (the difference between the observed and predicted values).\n\n\n\nThe Role of the Cost Function\nThe cost function in linear regression measures how well the model’s predictions match the actual data. One of the most commonly used cost functions is the Mean Squared Error (MSE), which is defined as:\n\\[\\text{MSE}  = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2\\]\nWhere:\n\n\\(\\text{MSE}\\) is the cost function.\n\\(n\\) is the number of training examples.\n\\(\\hat{y}^{(i)}\\) is the predicted value for the \\(i\\)-th training example, given by the hypothesis function \\(\\hat{y}\\).\n\\(y^{(i)}\\) is the actual value for the \\(i\\)-th training example.\n\\(\\theta\\) represents the parameters of the hypothesis.\n\nThe MSE calculates the average of the squares of the errors (the differences between actual and predicted values). Squaring the errors ensures that both positive and negative errors are treated equally and magnifies larger errors, making them more impactful on the cost function.\n\n\nWhy Minimize the Cost Function?\nMinimizing the cost function is essential because it directly translates to improving the model’s accuracy. When the cost function is minimized, the predicted values are as close as possible to the actual values, indicating a well-fitting model. This process involves finding the optimal values for the model parameters (\\(\\beta_0\\) and \\(\\beta_1\\)).\n\n\n\n\n\n\nGradient Descent: An Optimization Technique\nTo minimize the cost function, linear regression often employs an optimization technique called gradient descent. Gradient descent iteratively adjusts the model parameters in the direction that reduces the cost function. The update rules for the parameters are:\n\\[\\beta_0 = \\beta_0 - \\alpha \\frac{\\partial}{\\partial \\beta_0} \\text{MSE}\\] \\[\\beta_1 = \\beta_1 - \\alpha \\frac{\\partial}{\\partial \\beta_1} \\text{MSE}\\]\nHere:\n\n\\(\\alpha\\) is the learning rate, a hyperparameter that controls the step size of each update.\n\\(\\frac{\\partial}{\\partial \\beta_0} \\text{MSE}\\) and \\(\\frac{\\partial}{\\partial \\beta_1} \\text{MSE}\\) are the partial derivatives of the MSE with respect to \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\n\nThese partial derivatives (also called gradients) indicate the direction and magnitude of the steepest increase in the cost function. By moving in the opposite direction of the gradients, gradient descent reduces the cost function, gradually leading to the optimal parameter values.\n\n\nConclusion\nThe cost function is a fundamental concept in linear regression, serving as the guiding metric for model optimization. By quantifying the difference between predicted and actual values, the cost function enables the model to learn and improve its predictions through techniques like gradient descent. Understanding and minimizing the cost function is crucial for building accurate and reliable linear regression models, making it a cornerstone of predictive analytics and machine learning.\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024/data-imputation/index.html",
    "href": "posts/2024/data-imputation/index.html",
    "title": "Handling Missing Data",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/2024/data-imputation/index.html#handling-missing-data",
    "href": "posts/2024/data-imputation/index.html#handling-missing-data",
    "title": "Handling Missing Data",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/2024/data-imputation/index.html#introduction",
    "href": "posts/2024/data-imputation/index.html#introduction",
    "title": "Handling Missing Data",
    "section": "Introduction:",
    "text": "Introduction:\nMissing data is a common hurdle in data analysis, impacting the reliability of insights drawn from datasets. Python offers a range of solutions to address this issue, some of which we discussed in the earlier weeks. In this notebook, we look into the top three missing data imputation methods in Python—SimpleImputer, KNNImputer, and IterativeImputer from scikit-learn—providing insights into their functionalities and practical considerations. We’ll explore these essential techniques, using the weather dataset.\n# install the libraries for this demonstration\n! pip install dataidea==0.2.5\nfrom dataidea.packages import *\nfrom dataidea.datasets import loadDataset\nfrom dataidea.packages import * imports for us np, pd, plt, etc. loadDataset allows us to load datasets inbuilt in the dataidea library\nweather = loadDataset('weather')\nweather\n\n\n\n\n\n\n\n\nday\n\n\ntemperature\n\n\nwindspead\n\n\nevent\n\n\n\n\n\n\n0\n\n\n01/01/2017\n\n\n32.0\n\n\n6.0\n\n\nRain\n\n\n\n\n1\n\n\n04/01/2017\n\n\nNaN\n\n\n9.0\n\n\nSunny\n\n\n\n\n2\n\n\n05/01/2017\n\n\n28.0\n\n\nNaN\n\n\nSnow\n\n\n\n\n3\n\n\n06/01/2017\n\n\nNaN\n\n\n7.0\n\n\nNaN\n\n\n\n\n4\n\n\n07/01/2017\n\n\n32.0\n\n\nNaN\n\n\nRain\n\n\n\n\n5\n\n\n08/01/2017\n\n\nNaN\n\n\nNaN\n\n\nSunny\n\n\n\n\n6\n\n\n09/01/2017\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n7\n\n\n10/01/2017\n\n\n34.0\n\n\n8.0\n\n\nCloudy\n\n\n\n\n8\n\n\n11/01/2017\n\n\n40.0\n\n\n12.0\n\n\nSunny\n\n\n\n\n\nweather.isna().sum()\nday            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64\nLet’s demonstrate how to use the top three missing data imputation methods—SimpleImputer, KNNImputer, and IterativeImputer—using the simple weather dataset.\n# select age from the data\ntemp_wind = weather[['temperature', 'windspead']].copy()\ntemp_wind_imputed = temp_wind.copy()"
  },
  {
    "objectID": "posts/2024/data-imputation/index.html#simpleimputer-from-scikit-learn",
    "href": "posts/2024/data-imputation/index.html#simpleimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "SimpleImputer from scikit-learn:",
    "text": "SimpleImputer from scikit-learn:\n\nUsage: SimpleImputer is a straightforward method for imputing missing values by replacing them with a constant, mean, median, or most frequent value along each column.\nPros:\n\nEasy to use and understand.\nCan handle both numerical and categorical data.\nOffers flexibility with different imputation strategies.\n\nCons:\n\nIt doesn’t consider relationships between features.\nMay not be the best choice for datasets with complex patterns of missingness.\n\nExample:\n\nfrom sklearn.impute import SimpleImputer\n\nsimple_imputer = SimpleImputer(strategy='mean')\ntemp_wind_simple_imputed = simple_imputer.fit_transform(temp_wind)\n\ntemp_wind_simple_imputed_df = pd.DataFrame(temp_wind_simple_imputed, columns=temp_wind.columns)\nLet’s have a look at the outcome\ntemp_wind_simple_imputed_df\n\n\n\n\n\n\n\n\ntemperature\n\n\nwindspead\n\n\n\n\n\n\n0\n\n\n32.0\n\n\n6.0\n\n\n\n\n1\n\n\n33.2\n\n\n9.0\n\n\n\n\n2\n\n\n28.0\n\n\n8.4\n\n\n\n\n3\n\n\n33.2\n\n\n7.0\n\n\n\n\n4\n\n\n32.0\n\n\n8.4\n\n\n\n\n5\n\n\n33.2\n\n\n8.4\n\n\n\n\n6\n\n\n33.2\n\n\n8.4\n\n\n\n\n7\n\n\n34.0\n\n\n8.0\n\n\n\n\n8\n\n\n40.0\n\n\n12.0"
  },
  {
    "objectID": "posts/2024/data-imputation/index.html#knnimputer-from-scikit-learn",
    "href": "posts/2024/data-imputation/index.html#knnimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "KNNImputer from scikit-learn:",
    "text": "KNNImputer from scikit-learn:\n\nUsage: KNNImputer imputes missing values using k-nearest neighbors, replacing them with the mean value of the nearest neighbors.\nPros:\n\nConsiders relationships between features, making it suitable for datasets with complex patterns of missingness.\nCan handle both numerical and categorical data.\n\nCons:\n\nComputationally expensive for large datasets.\nRequires careful selection of the number of neighbors (k).\n\nExample:\n\nfrom sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\ntemp_wind_knn_imputed = knn_imputer.fit_transform(temp_wind)\n\ntemp_wind_knn_imputed_df = pd.DataFrame(temp_wind_knn_imputed, columns=temp_wind.columns)\nIf we take a look at the outcome\ntemp_wind_knn_imputed_df\n\n\n\n\n\n\n\n\ntemperature\n\n\nwindspead\n\n\n\n\n\n\n0\n\n\n32.0\n\n\n6.0\n\n\n\n\n1\n\n\n33.0\n\n\n9.0\n\n\n\n\n2\n\n\n28.0\n\n\n7.0\n\n\n\n\n3\n\n\n33.0\n\n\n7.0\n\n\n\n\n4\n\n\n32.0\n\n\n7.0\n\n\n\n\n5\n\n\n33.2\n\n\n8.4\n\n\n\n\n6\n\n\n33.2\n\n\n8.4\n\n\n\n\n7\n\n\n34.0\n\n\n8.0\n\n\n\n\n8\n\n\n40.0\n\n\n12.0"
  },
  {
    "objectID": "posts/2024/data-imputation/index.html#iterativeimputer-from-scikit-learn",
    "href": "posts/2024/data-imputation/index.html#iterativeimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "IterativeImputer from scikit-learn:",
    "text": "IterativeImputer from scikit-learn:\n\nUsage: IterativeImputer models each feature with missing values as a function of other features and uses that estimate for imputation. It iteratively estimates the missing values.\nPros:\n\nTakes into account relationships between features, making it suitable for datasets with complex missing patterns.\nMore robust than SimpleImputer for handling missing data.\n\nCons:\n\nCan be computationally intensive and slower than SimpleImputer.\nRequires careful tuning of model parameters.\n\nExample:\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\niterative_imputer = IterativeImputer()\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\nLet’s take a look at the outcome\ntemp_wind_iterative_imputed_df\n\n\n\n\n\n\n\n\ntemperature\n\n\nwindspead\n\n\n\n\n\n\n0\n\n\n32.000000\n\n\n6.000000\n\n\n\n\n1\n\n\n35.773287\n\n\n9.000000\n\n\n\n\n2\n\n\n28.000000\n\n\n3.321648\n\n\n\n\n3\n\n\n33.042537\n\n\n7.000000\n\n\n\n\n4\n\n\n32.000000\n\n\n6.238915\n\n\n\n\n5\n\n\n33.545118\n\n\n7.365795\n\n\n\n\n6\n\n\n33.545118\n\n\n7.365795\n\n\n\n\n7\n\n\n34.000000\n\n\n8.000000\n\n\n\n\n8\n\n\n40.000000\n\n\n12.000000"
  },
  {
    "objectID": "posts/2024/data-imputation/index.html#datawig",
    "href": "posts/2024/data-imputation/index.html#datawig",
    "title": "Handling Missing Data",
    "section": "Datawig:",
    "text": "Datawig:\nDatawig is a library specifically designed for imputing missing values in tabular data using deep learning models.\n# import datawig\n\n# # Impute missing values\n# df_imputed = datawig.SimpleImputer.complete(weather)\nThese top imputation methods offer different trade-offs in terms of computational complexity, handling of missing data patterns, and ease of use. The choice between them depends on the specific characteristics of the dataset and the requirements of the analysis."
  },
  {
    "objectID": "posts/2024/data-imputation/index.html#homework",
    "href": "posts/2024/data-imputation/index.html#homework",
    "title": "Handling Missing Data",
    "section": "Homework",
    "text": "Homework\n\nTry out these techniques for categorical data"
  },
  {
    "objectID": "posts/2024/data-imputation/index.html#credit",
    "href": "posts/2024/data-imputation/index.html#credit",
    "title": "Handling Missing Data",
    "section": "Credit",
    "text": "Credit\n\nDo you seriously want to learn Programming and Data Analysis with Python?\nIf you’re serious about learning Programming, Data Analysis with Python and getting prepared for Data Science roles, I highly encourage you to enroll in my Programming for Data Science Course, which I’ve taught to hundreds of students. Don’t waste your time following disconnected, outdated tutorials\nMy Complete Programming for Data Science Course has everything you need in one place.\nThe course offers:\n\nDuration: Usually 3-4 months\nSessions: Four times a week (one on one)\nLocation: Online or/and at UMF House, Sir Apollo Kagwa Road\n\nWhat you’l learn:\n\nFundamentals of programming\nData manipulation and analysis\nVisualization techniques\nIntroduction to machine learning\nDatabase Management with SQL (optional)\nWeb Development with Django (optional)\n\nBest\nJuma Shafara\nData Scientist, Instructor\njumashafara0@gmail.com / dataideaorg@gmail.com\n+256701520768 / +256771754118\n\n\n\n\nYou may also like:\n\n\n\nHandling Missing Data in Pandas, When to Use bfill and ffill Methods"
  },
  {
    "objectID": "posts/2024/new-programming-for-data-science-app/index.html",
    "href": "posts/2024/new-programming-for-data-science-app/index.html",
    "title": "Introducing Our New Programming for Data Science App!",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nWe are thrilled to announce the launch of our brand-new app designed specifically for learning “Programming for Data Science.” Whether you’re diving into Python, exploring data analysis techniques or machine learning concepts, our app is tailored to support your learning journey every step of the way.\n\nHow to Access the App:\nAs a valued subscriber of our newsletter, you have exclusive early access to our app. Simply click here (download link) to download it now, install and start exploring the exciting world of data science programming!\n\n\nWe Want Your Feedback:\nYour feedback is invaluable to us as we continue to improve and expand our app. Please feel free to share your thoughts, suggestions, or any issues you encounter. Your input will help us tailor the app to meet your learning needs better.\nThank you for being a part of our community and for your continued support. We’re excited to embark on this learning journey with you through our new app!\nWarm regards,\nJuma Shafara\nData Scientist, Instructor, CoFounder\nDATAIDEA\nhttps://www.dataidea.org\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024/what-is-supervised-machine-learning/index.html",
    "href": "posts/2024/what-is-supervised-machine-learning/index.html",
    "title": "What is Supervised Machine Learning",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/2024/what-is-supervised-machine-learning/index.html#understanding-supervised-machine-learning",
    "href": "posts/2024/what-is-supervised-machine-learning/index.html#understanding-supervised-machine-learning",
    "title": "What is Supervised Machine Learning",
    "section": "Understanding Supervised Machine Learning",
    "text": "Understanding Supervised Machine Learning\nSupervised machine learning is a subfield of machine learning where an algorithm learns from labeled training data to make predictions or decisions without being explicitly programmed to perform the task. The process involves two primary phases: training and testing. Here, we’ll delve into the details of how supervised learning works, using formulas and tables to illustrate key concepts.\n\nKey Concepts in Supervised Learning\n\nLabeled Data: The foundational element in supervised learning is labeled data, which consists of input-output pairs. Each input (feature vector \\(\\mathbf{x}\\)) has a corresponding output (label \\(y\\)). For instance, in a housing price prediction model, the features could include the size of the house, number of bedrooms, etc., and the label would be the price of the house.\nTraining Phase: During the training phase, the algorithm learns a mapping from inputs to outputs using the labeled dataset. This involves optimizing a model to minimize the difference between the predicted outputs and the actual outputs.\nTesting Phase: After training, the model’s performance is evaluated on a separate testing set, which was not seen during training. This helps assess the model’s generalization ability to new, unseen data.\n\n\n\nSupervised Learning Algorithms\nThere are two main types of supervised learning tasks: regression and classification.\n\nRegression: Used when the output variable is continuous. Example: Predicting house prices.\nClassification: Used when the output variable is categorical. Example: Spam email detection.\n\n\n\nMathematical Formulation\n\nRegression\nFor regression tasks, a common algorithm is Linear Regression. The goal is to find the best-fitting line through the data points.\nThe model is defined as: \\[y = \\mathbf{w}^T \\mathbf{x} + b\\]\nwhere: - \\(y\\) is the predicted output. - \\(\\mathbf{x}\\) is the feature vector. - \\(\\mathbf{w}\\) is the weight vector. - \\(b\\) is the bias term.\nThe training process involves minimizing the loss function, typically the Mean Squared Error (MSE): \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2\\]\nwhere: - \\(n\\) is the number of training examples. - \\(y_i\\) is the actual output for the \\(i\\)-th example. - \\(\\hat{y_i}\\) is the predicted output for the \\(i\\)-th example.\n\n\nClassification\nFor classification tasks, Logistic Regression is a commonly used algorithm. The model estimates the probability that a given input belongs to a certain class.\nThe logistic model is: \\[P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)\\]\nwhere \\(\\sigma\\) is the sigmoid function: \\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\]\nThe loss function for logistic regression is the Binary Cross-Entropy: \\[\\text{BCE} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i}) \\right]\\]\n\n\n\nExample: Housing Price Prediction\n\nTraining Data\n\n\n\nSize (sq ft)\nBedrooms\nPrice ($)\n\n\n\n\n1500\n3\n300,000\n\n\n1800\n4\n350,000\n\n\n2000\n4\n400,000\n\n\n2200\n5\n450,000\n\n\n\n\n\nLinear Regression Model\nAssume we have a feature vector \\(\\mathbf{x} = [\\text{Size, Bedrooms}]\\) and we want to predict the price \\(y\\).\n\nInitialize weights \\(\\mathbf{w}\\) and bias \\(b\\).\nUse the training data to find the best \\(\\mathbf{w}\\) and \\(b\\) that minimize the MSE.\nPredict the price for a new house with size 2100 sq ft and 4 bedrooms.\n\n\n\n\nExample: Email Spam Detection\n\nTraining Data\n\n\n\nEmail Content\nSpam (1) / Not Spam (0)\n\n\n\n\n“Win a free vacation now!”\n1\n\n\n“Meeting at 10 AM tomorrow”\n0\n\n\n“Limited time offer, click now!”\n1\n\n\n“Your report is due by Friday”\n0\n\n\n\n\n\nLogistic Regression Model\n\nConvert email content to feature vectors (e.g., word counts, presence of certain keywords).\nInitialize weights \\(\\mathbf{w}\\) and bias \\(b\\).\nUse the training data to find the best \\(\\mathbf{w}\\) and \\(b\\) that minimize the BCE.\nPredict whether a new email “Congratulations, you’ve won!” is spam.\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\nYou may also like:\n\n\n\nOverview of Machine Learning"
  },
  {
    "objectID": "posts/2024/sklearn-unsupervised-learning/index.html",
    "href": "posts/2024/sklearn-unsupervised-learning/index.html",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "",
    "text": "Photo by DATAIDEA\nThe following topics are covered in this tutorial:\nLet’s install the required libraries."
  },
  {
    "objectID": "posts/2024/sklearn-unsupervised-learning/index.html#introduction-to-unsupervised-learning",
    "href": "posts/2024/sklearn-unsupervised-learning/index.html#introduction-to-unsupervised-learning",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "Introduction to Unsupervised Learning",
    "text": "Introduction to Unsupervised Learning\nUnsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels. Unsupervised learning is generally use to discover patterns in data and reduce high-dimensional data to fewer dimensions. Here’s how unsupervised learning fits into the landscape of machine learning algorithms(source):\n\nHere are the topics in machine learning that we’re studying in this course (source):\n\nHere’s a cheatsheet to help you decide which model to pick for a given problem. Can you identify the unsupervised learning algorithms?\n\nHere is a full list of unsupervised learning algorithms available in Scikit-learn: https://scikit-learn.org/stable/unsupervised_learning.html"
  },
  {
    "objectID": "posts/2024/sklearn-unsupervised-learning/index.html#clustering",
    "href": "posts/2024/sklearn-unsupervised-learning/index.html#clustering",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "Clustering",
    "text": "Clustering\nClustering is the process of grouping objects from a dataset such that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (Wikipedia). Scikit-learn offers several clustering algorithms. You can learn more about them here: https://scikit-learn.org/stable/modules/clustering.html\nHere is a visual representation of clustering:\n\nHere are some real-world applications of clustering:\n\nCustomer segmentation\nProduct recommendation\nFeature engineering\nAnomaly/fraud detection\nTaxonomy creation\n\nWe’ll use the Iris flower dataset to study some of the clustering algorithms available in scikit-learn. It contains various measurements for 150 flowers belonging to 3 different species.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nsns.set_style('darkgrid')\n%matplotlib inline\nLet’s load the popular iris and penguin datasets. These datasets are already built in seaborn\n# load the iris dataset\niris_df = sns.load_dataset('iris')\niris_df.head()\n\n\n\n\n\n\n\n\nsepal_length\n\n\nsepal_width\n\n\npetal_length\n\n\npetal_width\n\n\nspecies\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\nsetosa\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\nsetosa\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\nsetosa\n\n\n\n\n\n# load the penguin dataset\nsns.get_dataset_names()\nping_df = sns.load_dataset('penguins')\nping_df.head()\n\n\n\n\n\n\n\n\nspecies\n\n\nisland\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nflipper_length_mm\n\n\nbody_mass_g\n\n\nsex\n\n\n\n\n\n\n0\n\n\nAdelie\n\n\nTorgersen\n\n\n39.1\n\n\n18.7\n\n\n181.0\n\n\n3750.0\n\n\nMale\n\n\n\n\n1\n\n\nAdelie\n\n\nTorgersen\n\n\n39.5\n\n\n17.4\n\n\n186.0\n\n\n3800.0\n\n\nFemale\n\n\n\n\n2\n\n\nAdelie\n\n\nTorgersen\n\n\n40.3\n\n\n18.0\n\n\n195.0\n\n\n3250.0\n\n\nFemale\n\n\n\n\n3\n\n\nAdelie\n\n\nTorgersen\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n4\n\n\nAdelie\n\n\nTorgersen\n\n\n36.7\n\n\n19.3\n\n\n193.0\n\n\n3450.0\n\n\nFemale\n\n\n\n\n\nsns.scatterplot(data=ping_df, x='bill_length_mm', y='bill_depth_mm', hue='species')\nplt.title('Penguin Bill Depth against Bill Length per Species')\nplt.ylabel('Bill Depth')\nplt.xlabel('Bill Length')\nplt.show()\n\n\n\npng\n\n\nsns.scatterplot(data=iris_df, x='sepal_length', y='petal_length', hue='species')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\npng\n\n\nWe’ll attempt to cluster observations using numeric columns in the data.\nnumeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nX = iris_df[numeric_cols]\nX.head()\n\n\n\n\n\n\n\n\nsepal_length\n\n\nsepal_width\n\n\npetal_length\n\n\npetal_width\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n\n\n\n\n\n\n\n\n\nK Means Clustering\nThe K-means algorithm attempts to classify objects into a pre-determined number of clusters by finding optimal central points (called centroids) for each cluster. Each object is classifed as belonging the cluster represented by the closest centroid.\n\nHere’s how the K-means algorithm works:\n\nPick K random objects as the initial cluster centers.\nClassify each object into the cluster whose center is closest to the point.\nFor each cluster of classified objects, compute the centroid (mean).\nNow reclassify each object using the centroids as cluster centers.\nCalculate the total variance of the clusters (this is the measure of goodness).\nRepeat steps 1 to 6 a few more times and pick the cluster centers with the lowest total variance.\n\nHere’s a video showing the above steps:\n\n \n\nLet’s apply K-means clustering to the Iris dataset.\nfrom sklearn.cluster import KMeans\nmodel = KMeans(n_clusters=3, random_state=42)\nmodel.fit(X)\n\n\n\nKMeans(n_clusters=3, random_state=42)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  KMeans?Documentation for KMeansiFitted\n\nKMeans(n_clusters=3, random_state=42)\n\n\n\n\n\nWe can check the cluster centers for each cluster.\nmodel.cluster_centers_\narray([[ 6.85384615e+00,  3.07692308e+00,  5.71538462e+00,\n         2.05384615e+00, -8.88178420e-16],\n       [ 5.00600000e+00,  3.42800000e+00,  1.46200000e+00,\n         2.46000000e-01,  1.00000000e+00],\n       [ 5.88360656e+00,  2.74098361e+00,  4.38852459e+00,\n         1.43442623e+00,  2.00000000e+00]])\nWe can now classify points using the model.\n# making predictions on X (clustering)\npreds = model.predict(X)\n# assign each row to their cluster\nX['clusters'] = preds\n# looking at some samples\nX.sample(n=5)\n\n\n\n\n\n\n\n\nsepal_length\n\n\nsepal_width\n\n\npetal_length\n\n\npetal_width\n\n\nclusters\n\n\n\n\n\n\n88\n\n\n5.6\n\n\n3.0\n\n\n4.1\n\n\n1.3\n\n\n2\n\n\n\n\n24\n\n\n4.8\n\n\n3.4\n\n\n1.9\n\n\n0.2\n\n\n1\n\n\n\n\n131\n\n\n7.9\n\n\n3.8\n\n\n6.4\n\n\n2.0\n\n\n0\n\n\n\n\n81\n\n\n5.5\n\n\n2.4\n\n\n3.7\n\n\n1.0\n\n\n2\n\n\n\n\n132\n\n\n6.4\n\n\n2.8\n\n\n5.6\n\n\n2.2\n\n\n0\n\n\n\n\n\nLet’s use seaborn and pyplot to visualize the clusters\nsns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds)\ncenters_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\nplt.plot(centers_x, centers_y, 'xb')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\npng\n\n\nAs you can see, K-means algorithm was able to classify (for the most part) different specifies of flowers into separate clusters. Note that we did not provide the “species” column as an input to KMeans.\nWe can check the “goodness” of the fit by looking at model.inertia_, which contains the sum of squared distances of samples to their closest cluster center. Lower the inertia, better the fit.\nmodel.inertia_\n78.8556658259773\n\n\n\n\n\nLet’s try creating 6 clusters.\nmodel = KMeans(n_clusters=6, random_state=42)\n# fitting the model\nmodel.fit(X)\n# making predictions on X (clustering)\npreds = model.predict(X)\n# assign each row to their cluster\nX['clusters'] = preds\n# looking at some samples\nX.sample(n=5)\n\n\n\n\n\n\n\n\nsepal_length\n\n\nsepal_width\n\n\npetal_length\n\n\npetal_width\n\n\nclusters\n\n\n\n\n\n\n13\n\n\n4.3\n\n\n3.0\n\n\n1.1\n\n\n0.1\n\n\n1\n\n\n\n\n20\n\n\n5.4\n\n\n3.4\n\n\n1.7\n\n\n0.2\n\n\n5\n\n\n\n\n127\n\n\n6.1\n\n\n3.0\n\n\n4.9\n\n\n1.8\n\n\n0\n\n\n\n\n40\n\n\n5.0\n\n\n3.5\n\n\n1.3\n\n\n0.3\n\n\n5\n\n\n\n\n70\n\n\n5.9\n\n\n3.2\n\n\n4.8\n\n\n1.8\n\n\n0\n\n\n\n\n\nLet’s visualize the clusters\nsns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds)\ncenters_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\nplt.plot(centers_x, centers_y, 'xb')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\npng\n\n\n# Let's calculate the new model inertia\nmodel.inertia_\n50.560990643274856\n\n\n\n\n\n\n\nSo, what number of clusters is good enough?\nIn most real-world scenarios, there’s no predetermined number of clusters. In such a case, you can create a plot of “No. of clusters” vs “Inertia” to pick the right number of clusters.\noptions = range(2, 11)\ninertias = []\n\nfor n_clusters in options:\n    model = KMeans(n_clusters, random_state=42).fit(X)\n    inertias.append(model.inertia_)\n\nplt.title(\"No. of clusters vs. Inertia\")\nplt.plot(options, inertias, '-o')\nplt.xlabel('No. of clusters (K)')\nplt.ylabel('Inertia')\nText(0, 0.5, 'Inertia')\n\n\n\npng\n\n\nThe chart is creates an “elbow” plot, and you can pick the number of clusters beyond which the reduction in inertia decreases sharply.\nMini Batch K Means: The K-means algorithm can be quite slow for really large dataset. Mini-batch K-means is an iterative alternative to K-means that works well for large datasets. Learn more about it here: https://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans\n\nEXERCISE: Perform clustering on the Mall customers dataset on Kaggle. Study the segments carefully and report your observations."
  },
  {
    "objectID": "posts/2024/sklearn-unsupervised-learning/index.html#summary-and-references",
    "href": "posts/2024/sklearn-unsupervised-learning/index.html#summary-and-references",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "Summary and References",
    "text": "Summary and References\n\nThe following topics were covered in this tutorial:\n\nOverview of unsupervised learning algorithms in Scikit-learn\nClustering algorithms: K Means, DBScan, Hierarchical clustering etc.\nDimensionality reduction (PCA) and manifold learning (t-SNE)\n\nCheck out these resources to learn more:\n\nhttps://www.coursera.org/learn/machine-learning\nhttps://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\nhttps://scikit-learn.org/stable/unsupervised_learning.html\nhttps://scikit-learn.org/stable/modules/clustering.html"
  },
  {
    "objectID": "posts/2024/sklearn-unsupervised-learning/index.html#credit",
    "href": "posts/2024/sklearn-unsupervised-learning/index.html#credit",
    "title": "Unsupervised Learning using Scikit Learn",
    "section": "Credit",
    "text": "Credit\n\nDo you seriously want to learn Programming and Data Analysis with Python?\nIf you’re serious about learning Programming, Data Analysis with Python and getting prepared for Data Science roles, I highly encourage you to enroll in my Programming for Data Science Course, which I’ve taught to hundreds of students. Don’t waste your time following disconnected, outdated tutorials\nMy Complete Programming for Data Science Course has everything you need in one place.\nThe course offers:\n\nDuration: Usually 3-4 months\nSessions: Four times a week (one on one)\nLocation: Online or/and at UMF House, Sir Apollo Kagwa Road\n\nWhat you’l learn:\n\nFundamentals of programming\nData manipulation and analysis\nVisualization techniques\nIntroduction to machine learning\nDatabase Management with SQL (optional)\nWeb Development with Django (optional)\n\nBest\nJuma Shafara\nData Scientist, Instructor\njumashafara0@gmail.com / dataideaorg@gmail.com\n+256701520768 / +256771754118"
  },
  {
    "objectID": "posts/2024/anova-for-feature-selection/index.html#anova-for-feature-selection",
    "href": "posts/2024/anova-for-feature-selection/index.html#anova-for-feature-selection",
    "title": "ANOVA for Feature Selection",
    "section": "ANOVA for Feature Selection",
    "text": "ANOVA for Feature Selection\nIn this notebook, we demonstrate how ANOVA (Analysis of Variance) can be used to identify better features for machine learning models. We’ll use the Fantasy Premier League (FPL) dataset to show how ANOVA helps in selecting features that best differentiate categories.\n# Uncomment the line below if you need to install the dataidea package\n# !pip install -U dataidea\nFirst, we’ll import the necessary packages: scipy for performing ANOVA, dataidea for loading the FPL dataset, and SelectKBest from scikit-learn for univariate feature selection based on statistical tests.\nimport scipy as sp\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport dataidea as di\nLet’s load the FPL dataset and preview the top 5 rows.\n# Load FPL dataset\nfpl = di.loadDataset('fpl')\n\n# Preview the top 5 rows\nfpl.head(n=5)\n\n\n\n\n\n\n\n\n\nFirst_Name\n\n\nSecond_Name\n\n\nClub\n\n\nGoals_Scored\n\n\nAssists\n\n\nTotal_Points\n\n\nMinutes\n\n\nSaves\n\n\nGoals_Conceded\n\n\nCreativity\n\n\nInfluence\n\n\nThreat\n\n\nBonus\n\n\nBPS\n\n\nICT_Index\n\n\nClean_Sheets\n\n\nRed_Cards\n\n\nYellow_Cards\n\n\nPosition\n\n\n\n\n\n\n0\n\n\nBruno\n\n\nFernandes\n\n\nMUN\n\n\n18\n\n\n14\n\n\n244\n\n\n3101\n\n\n0\n\n\n36\n\n\n1414.9\n\n\n1292.6\n\n\n1253\n\n\n36\n\n\n870\n\n\n396.2\n\n\n13\n\n\n0\n\n\n6\n\n\nMID\n\n\n\n\n1\n\n\nHarry\n\n\nKane\n\n\nTOT\n\n\n23\n\n\n14\n\n\n242\n\n\n3083\n\n\n0\n\n\n39\n\n\n659.1\n\n\n1318.2\n\n\n1585\n\n\n40\n\n\n880\n\n\n355.9\n\n\n12\n\n\n0\n\n\n1\n\n\nFWD\n\n\n\n\n2\n\n\nMohamed\n\n\nSalah\n\n\nLIV\n\n\n22\n\n\n6\n\n\n231\n\n\n3077\n\n\n0\n\n\n41\n\n\n825.7\n\n\n1056.0\n\n\n1980\n\n\n21\n\n\n657\n\n\n385.8\n\n\n11\n\n\n0\n\n\n0\n\n\nMID\n\n\n\n\n3\n\n\nHeung-Min\n\n\nSon\n\n\nTOT\n\n\n17\n\n\n11\n\n\n228\n\n\n3119\n\n\n0\n\n\n36\n\n\n1049.9\n\n\n1052.2\n\n\n1046\n\n\n26\n\n\n777\n\n\n315.2\n\n\n13\n\n\n0\n\n\n0\n\n\nMID\n\n\n\n\n4\n\n\nPatrick\n\n\nBamford\n\n\nLEE\n\n\n17\n\n\n11\n\n\n194\n\n\n3052\n\n\n0\n\n\n50\n\n\n371.0\n\n\n867.2\n\n\n1512\n\n\n26\n\n\n631\n\n\n274.6\n\n\n10\n\n\n0\n\n\n3\n\n\nFWD\n\n\n\n\n\nANOVA helps us determine if there’s a significant difference between the means of different groups. We use it to select features that best show the difference between categories. Features with higher F-statistics are preferred.\n\nANOVA for Goals Scored\nWe will create groups of goals scored by each player position (forwards, midfielders, defenders, and goalkeepers) and run an ANOVA test.\n# Create groups of goals scored for each player position\nforwards_goals = fpl[fpl.Position == 'FWD']['Goals_Scored']\nmidfielders_goals = fpl[fpl.Position == 'MID']['Goals_Scored']\ndefenders_goals = fpl[fpl.Position == 'DEF']['Goals_Scored']\ngoalkeepers_goals = fpl[fpl.Position == 'GK']['Goals_Scored']\n\n# Perform the ANOVA test for the groups\nf_statistic, p_value = sp.stats.f_oneway(forwards_goals, midfielders_goals, defenders_goals, goalkeepers_goals)\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\nF-statistic: 33.281034594400445\np-value: 3.9257634156019246e-20\nWe observe an F-statistic of 33.281 and a p-value of 3.926e-20, indicating a significant difference at multiple confidence levels.\n\n\nANOVA for Assists\nNext, we’ll create groups for assists and run an ANOVA test.\n# Create groups of assists for each player position\nforwards_assists = fpl[fpl.Position == 'FWD']['Assists']\nmidfielders_assists = fpl[fpl.Position == 'MID']['Assists']\ndefenders_assists = fpl[fpl.Position == 'DEF']['Assists']\ngoalkeepers_assists = fpl[fpl.Position == 'GK']['Assists']\n\n# Perform the ANOVA test for the groups\nf_statistic, p_value = sp.stats.f_oneway(forwards_assists, midfielders_assists, defenders_assists, goalkeepers_assists)\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\nF-statistic: 19.263717036430815\np-value: 5.124889288362087e-12\nWe observe an F-statistic of 19.264 and a p-value of 5.125e-12, again indicating significance.\n\n\nComparing Results\nBoth features show significant F-statistics, but goals scored has a higher value, indicating it is a better feature for differentiating player positions.\n\n\nUsing SelectKBest for Feature Selection\nWe can also use SelectKBest from scikit-learn to automate this process.\n# Use scikit-learn's SelectKBest (with f_classif)\ntest = SelectKBest(score_func=f_classif, k=1)\n\n# Fit the model to the data\nfit = test.fit(fpl[['Goals_Scored', 'Assists']], fpl.Position)\n\n# Get the F-statistics\nscores = fit.scores_\n\n# Select the best feature\nfeatures = fit.transform(fpl[['Goals_Scored', 'Assists']])\n\n# Get the indices of the selected features (optional)\nselected_indices = test.get_support(indices=True)\n\n# Print indices and scores\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\nFeature Scores:  [33.28103459 19.26371704]\nSelected Features Indices:  [0]\nThe 0th feature (Goals Scored) is selected as the best feature based on the F-statistics.\n\n\n\n\n\n\n\nDon’t Miss Any Updates!\n\n\n\nTo be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\n\nSummary\nIn this notebook, we demonstrated how to use ANOVA for feature selection in the Fantasy Premier League dataset. By comparing the F-statistics of different features, we identified that ‘Goals Scored’ is a more significant feature than ‘Assists’ for differentiating player positions. Using SelectKBest from scikit-learn, we confirmed that ‘Goals Scored’ is the best feature among the two. This method can be applied to other datasets and features to enhance the performance of machine learning models.\n\n\nYou may also like:\n\n\n\nOverview of Machine Learning"
  },
  {
    "objectID": "posts/2024/rethinking-chatgpt/index.html#introduction",
    "href": "posts/2024/rethinking-chatgpt/index.html#introduction",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "1 Introduction",
    "text": "1 Introduction\nArtificial Intelligence (AI) has revolutionized the way we interact with technology, making many tasks more efficient and accessible. Among the AI applications, chatbots and virtual assistants have gained significant popularity, with ChatGPT being one of the leading models developed by OpenAI. While ChatGPT and similar AI tools offer numerous benefits, there are also several reasons to be cautious about their use. This blog aims to explore in detail why you might reconsider relying on ChatGPT, touching on aspects such as privacy concerns, ethical considerations, potential biases, reliability, and the broader societal implications of AI."
  },
  {
    "objectID": "posts/2024/rethinking-chatgpt/index.html#privacy-concerns",
    "href": "posts/2024/rethinking-chatgpt/index.html#privacy-concerns",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "2 Privacy Concerns",
    "text": "2 Privacy Concerns\n\n2.1 Data Collection and Storage\nOne of the primary concerns with using ChatGPT is related to data privacy. When you interact with ChatGPT, your conversations are often recorded and stored. Although companies like OpenAI implement stringent data security measures, there is always a risk of data breaches. The data collected from users can potentially be accessed by unauthorized individuals, leading to privacy violations.\n\n\n2.2 Misuse of Personal Information\nThe personal information you share with ChatGPT could be misused. While AI developers claim that user data is anonymized and used solely to improve AI models, there is always a risk that this data could be sold to third parties or used for targeted advertising. This not only undermines your privacy but also raises ethical questions about the exploitation of user data.\n\n\n2.3 Lack of Transparency\nMany users are unaware of how their data is being used when they interact with AI models like ChatGPT. The lack of transparency in data handling practices can lead to mistrust. Users should be informed about what data is being collected, how it is stored, and who has access to it. Without this transparency, it is difficult to trust that your data is being handled responsibly."
  },
  {
    "objectID": "posts/2024/rethinking-chatgpt/index.html#ethical-considerations",
    "href": "posts/2024/rethinking-chatgpt/index.html#ethical-considerations",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "3 Ethical Considerations",
    "text": "3 Ethical Considerations\n\n3.1 Manipulation and Misinformation\nAI models like ChatGPT can be used to manipulate opinions and spread misinformation. The ability of AI to generate human-like text makes it a powerful tool for creating fake news, misleading articles, and deceptive social media posts. This can have serious consequences, including influencing elections, spreading conspiracy theories, and undermining public trust in information sources.\n\n\n3.2 Dehumanization of Interaction\nRelying on AI for communication can lead to the dehumanization of interactions. While ChatGPT can simulate human conversation, it lacks the emotional intelligence and empathy that come from genuine human interaction. This can result in a diminished quality of communication, where the nuances of human emotions and connections are lost.\n\n\n3.3 Ethical Use in Sensitive Areas\nThe use of AI in sensitive areas such as mental health support, legal advice, and medical consultations raises ethical concerns. AI models may provide inaccurate or inappropriate advice, potentially causing harm to users. The reliance on AI in these critical areas should be approached with caution, ensuring that human oversight is always present to verify and validate the information provided by AI."
  },
  {
    "objectID": "posts/2024/rethinking-chatgpt/index.html#potential-biases",
    "href": "posts/2024/rethinking-chatgpt/index.html#potential-biases",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "4 Potential Biases",
    "text": "4 Potential Biases\n\n4.1 Inherent Biases in Training Data\nAI models like ChatGPT are trained on vast amounts of data from the internet, which inevitably includes biased and prejudiced content. These biases can be reflected in the responses generated by the AI, perpetuating stereotypes and discriminatory attitudes. For instance, gender, racial, and cultural biases can be inadvertently reinforced through AI-generated text.\n\n\n4.2 Impact on Marginalized Communities\nThe biases present in AI models can disproportionately impact marginalized communities. AI-generated content that reflects societal biases can contribute to the marginalization and discrimination of these groups. It is essential to recognize and address these biases to ensure that AI technologies do not perpetuate inequality and injustice.\n\n\n4.3 Difficulty in Mitigating Biases\nWhile efforts are being made to reduce biases in AI models, it is a challenging task. Biases are deeply ingrained in the data used to train these models, and completely eliminating them is nearly impossible. Continuous monitoring and updating of AI models are required to mitigate these biases, but the effectiveness of these measures is still a subject of ongoing research and debate."
  },
  {
    "objectID": "posts/2024/rethinking-chatgpt/index.html#reliability-and-accuracy",
    "href": "posts/2024/rethinking-chatgpt/index.html#reliability-and-accuracy",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "5 Reliability and Accuracy",
    "text": "5 Reliability and Accuracy\n\n5.1 Hallucinations and Errors\nAI models like ChatGPT are prone to generating incorrect or nonsensical information, often referred to as “hallucinations.” These errors can be misleading and potentially harmful, especially when users rely on the AI for accurate information. The inability to distinguish between correct and incorrect responses can undermine the reliability of AI-generated content.\n\n\n5.2 Lack of Accountability\nWhen using AI models, it can be challenging to determine who is accountable for the information provided. Unlike human professionals who can be held responsible for their advice and actions, AI lacks accountability. This lack of accountability can lead to a scenario where misinformation is spread without any consequences for those responsible.\n\n\n5.3 Dependence on AI\nOver-reliance on AI for information and decision-making can reduce critical thinking skills and the ability to independently verify information. Users may become dependent on AI-generated responses, leading to a decline in their analytical abilities and judgment. It is crucial to maintain a balance between using AI as a tool and exercising independent thought."
  },
  {
    "objectID": "posts/2024/rethinking-chatgpt/index.html#broader-societal-implications",
    "href": "posts/2024/rethinking-chatgpt/index.html#broader-societal-implications",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "6 Broader Societal Implications",
    "text": "6 Broader Societal Implications\n\n6.1 Job Displacement\nThe increasing use of AI technologies, including ChatGPT, has the potential to displace jobs. As AI becomes more capable of performing tasks traditionally done by humans, there is a risk of job loss in various sectors. This can lead to economic instability and exacerbate issues related to unemployment and income inequality.\n\n\n6.2 Ethical Dilemmas in AI Development\nThe development and deployment of AI technologies pose significant ethical dilemmas. Decisions about what data to use, how to handle biases, and how to ensure the responsible use of AI are complex and multifaceted. The ethical challenges associated with AI development require careful consideration and the involvement of diverse stakeholders to navigate effectively.\n\n\n6.3 Impact on Human Creativity and Innovation\nThe reliance on AI for creative tasks, such as writing, music composition, and art, raises questions about the impact on human creativity and innovation. While AI can assist in these areas, there is a concern that it may stifle human creativity by providing easy solutions and reducing the incentive to develop original ideas. It is important to find a balance where AI complements human creativity rather than replacing it.\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe."
  },
  {
    "objectID": "posts/2024/rethinking-chatgpt/index.html#conclusion",
    "href": "posts/2024/rethinking-chatgpt/index.html#conclusion",
    "title": "Why You Should Use ChatGPT Sparingly.",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nWhile ChatGPT and similar AI models offer numerous benefits, it is essential to be aware of the potential drawbacks and risks associated with their use. Privacy concerns, ethical considerations, potential biases, reliability issues, and broader societal implications are all important factors to consider. As we continue to integrate AI into our lives, it is crucial to approach its use with caution, ensuring that we prioritize ethical practices, transparency, and human oversight to mitigate the risks and maximize the benefits."
  },
  {
    "objectID": "posts/2024/openai_suspends_her/index.html",
    "href": "posts/2024/openai_suspends_her/index.html",
    "title": "OpenAI Suspends ChatGPT Voice Allegedly Mimicking Scarlett Johansson in “Her”",
    "section": "",
    "text": "IMDb\n\n\nOpenAI is suspending its ChatGPT voice, Sky, after users claimed it resembled Scarlett Johansson’s voice from the 2013 film Her.\nThe company released a statement on X (formerly Twitter) today, announcing that it is “working to pause the use of Sky” to address the issue. Many users noted the voice’s similarity to Johansson’s portrayal of an AI companion in the Spike Jonze-directed movie.\nSky is one of several ChatGPT voices available to users. OpenAI provided an explanation on its website about how these voices are selected and created.\n“We support the creative community and worked closely with the voice acting industry to ensure we took the right steps to cast ChatGPT’s voices. Each actor receives compensation above top-of-market rates, and this will continue for as long as their voices are used in our products.”\nThe statement further clarified, “We believe that AI voices should not deliberately mimic a celebrity’s distinctive voice—Sky’s voice is not an imitation of Scarlett Johansson but belongs to a different professional actress using her own natural speaking voice. To protect their privacy, we cannot share the names of our voice talents.”\n\n\n\n\n\nBackground on the Allegations\nThe controversy began last week when OpenAI revealed its new GPT-4o model during a livestream event, showcasing its ability to have realistic conversations about any topic. This functionality drew comparisons to Her, where Joaquin Phoenix’s character falls in love with an AI named Samantha, voiced by Johansson.\nAdding to the speculation, OpenAI CEO Sam Altman posted the word “her” on X after the event. In a subsequent post, he likened the new voice and video mode to scenes from sci-fi movies. In a September interview with The San Francisco Standard, Altman even cited Her as his favorite sci-fi film, praising its depiction of AI-human interactions as “incredibly prophetic.”\n\n\n\n\n\n\nMoving Forward\nIt remains unclear how OpenAI will resolve the concerns about Sky mimicking Johansson’s voice or prevent similar issues in the future. For more information on artificial intelligence developments, check out the experimental AI-made video game that failed.\nJuma Shafara contributor with DATAIDEA. Follow him on Twitter @juma_shafara.\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2022/what-is-demographic-data/index.html",
    "href": "posts/2022/what-is-demographic-data/index.html",
    "title": "What is Demographic Data, Understanding and Utilizing It",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDemographic data helps us to deepen our knowledge of the target audience and to create buyer personas. It is primarily used to strategically tailor offerings to specific target groups and can serve as the basis for business analysis and performance reporting. Practical business intelligence relies on the synergy between analytics and reporting, where analytics uncovers valuable insights, and reporting communicates these findings to stakeholders.\n\nWhat is Demographic Data?\nDemographic data is information about groups of people according to certain attributes such as age, sex, and place of residence. It can include socioeconomic factors such as occupation, family status, or income. Demographics and interests are among the most important web analytics and consumer buying behavior analytics statistics. In marketing, the demographics approach focuses more on age, gender, and interests rather than fertility and mortality data.\n\n\nImportance of Demographics\nIn web analytics and online marketing, demographic data is collected to gain deeper insights into the target group of a web page or to create buyer personas based on this information. It is primarily used for strategic supply targeting and can also be used for business analysis and performance reporting.\n\n\nExamples of Demographic Data\nHere are some examples of data you can request in a demographic survey:\n\n\n\n\n\nAge: One of the most important demographic factors, age is a good indicator of the groups of users that visit a web page and the age groups that buy the most. It provides information about content that is interesting to a particular age group and where potential can be identified.\nGender: Gender information shows which parts of a website or which products are more suitable for men or women. Classifying visits according to gender survey questions can serve as the basis for planning campaigns targeting men or women.\nEducation: Data on education can indicate whether users have attended university.\nIncome: Income information helps target high-income individuals, for example, to buy a high-end product.\nInterests: Data on user interests shows what visitors of a web page are interested in and allows marketers to draw conclusions about consumer behavior. For example, if users have an affinity for certain product categories, marketers can create ads focused on these interests.\nLanguage: For online marketing and website design, the language of the target group is important, especially for internationally oriented online stores. Advertising and content should be geared towards the language spoken by the target group.\nCountries: Knowing the region, city, or country users come from is important for targeting advertising measures specifically to these geographic segments.\n\nAdditionally, the use of demographic data allows for segmenting user groups, for example, to establish a connection between people aged 18 to 24 with certain keywords and interests. This type of targeting is especially useful for remarketing campaigns.\n\n\n\n\n\n\nWhat Does Demographic Data Tell Us?\nDemographic data can provide answers to the following questions:\n\nWhat groups of users visit the website? Young users have different interests than older users.\nWhich of these groups provides the most income? The most profitable clientele usually belongs to a certain age group.\nWhere should content be placed to increase sales? Relevant content can be tailored to age, gender, and interests.\nHow can ads be better targeted? Young female users want to see different types of ads compared to older male users.\nWhat factors improve remarketing? Thanks to segmentation, subsequent actions can be tailored to the target group and corresponding interests.\nHow can email campaigns be more effective and directly target specific groups? Emails can be sent to specific groups based on demographic data.\n\n\n\n\n\nDemographic data provides much deeper insight into user behavior. Information about user groups can be used to improve the effectiveness of advertising campaigns, optimize website offerings, and drive sales. At the same time, the legal use of this data should not be ignored: it should be anonymous, and users should be informed about the collection of data, as well as the use of cookies. Users must also have the opportunity to object to data collection.\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2022/how-the-decision-tree-classifier-works/index.html",
    "href": "posts/2022/how-the-decision-tree-classifier-works/index.html",
    "title": "How The Decision Tree Classifiers Works",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\n\nIntroduction\nDecision tree classifiers are a type of supervised machine learning algorithm used for classification tasks. They are popular due to their simplicity and interpretability. This article will explain how decision tree classifiers work in simple terms, breaking down the key concepts and processes involved.\n\n\nWhat is a Decision Tree?\nA decision tree is a flowchart-like structure where:\n\nEach internal node represents a “decision” based on the value of a feature.\nEach branch represents the outcome of a decision.\nEach leaf node represents a class label (the decision made after all features are considered).\n\n\n\nHow Does a Decision Tree Classifier Work?\n\nStarting at the Root Node:\nThe process begins at the root node, which contains the entire dataset. The goal is to split this dataset into subsets that are more homogenous in terms of the target variable (class label).\nChoosing the Best Feature to Split On:\nAt each step, the algorithm selects the feature that best separates the classes. This is done using a metric like Gini impurity or Information Gain.\n\nGini Impurity: Measures the frequency at which any element of the dataset would be misclassified. It’s calculated as: \\[\nGini = 1 - \\sum_{i=1}^{n} (p_i)^2\n\\] where \\(p_i\\) is the probability of an element being classified into a particular class.\nInformation Gain: Measures the reduction in entropy or impurity before and after a split. It’s calculated as: \\[\n\\text{Information Gain} = \\text{Entropy(before split)} - \\sum_{i=1}^{k} \\frac{n_i}{n} \\times \\text{Entropy}(i)\n\\] where \\(n_i\\) is the number of instances in the \\(i\\)-th subset.\n\nSplitting the Node:\nOnce the best feature is chosen, the dataset is split into subsets based on the feature’s values. Each subset forms a new node.\nRepeating the Process:\nThe algorithm recursively repeats the process for each new node, selecting the best feature to split on and creating further branches, until one of the stopping criteria is met:\n\nAll instances in a node belong to the same class.\nNo more features are left to split on.\nA pre-defined maximum tree depth is reached.\n\nMaking Predictions:\nAfter the tree is built, it can be used to classify new instances. Starting from the root, the instance is evaluated against the decision rules at each node, following the branches until it reaches a leaf node, which gives the predicted class.\n\n\n\nExample of a Decision Tree Classifier\nConsider a simple example where we want to classify whether a person will buy a computer based on their age and income.\n\nTraining Data\n\n\n\nAge\nIncome\nBuys Computer\n\n\n\n\n&lt;30\nHigh\nNo\n\n\n&lt;30\nHigh\nNo\n\n\n31-40\nHigh\nYes\n\n\n&gt;40\nMedium\nYes\n\n\n&gt;40\nLow\nNo\n\n\n&gt;40\nLow\nYes\n\n\n31-40\nLow\nYes\n\n\n&lt;30\nMedium\nNo\n\n\n&lt;30\nLow\nYes\n\n\n&gt;40\nMedium\nYes\n\n\n&lt;30\nMedium\nYes\n\n\n31-40\nMedium\nYes\n\n\n\n\n\nBuilding the Tree\n\nRoot Node:\n\nCalculate the Gini impurity for the entire dataset.\nSelect the feature (Age or Income) that provides the best split based on Gini impurity or Information Gain.\n\nFirst Split:\n\nSuppose Age is selected. The data is split into three groups: &lt;30, 31-40, and &gt;40.\n\nFurther Splits:\n\nFor each age group, calculate the Gini impurity or Information Gain again and split further based on Income.\n\n\n\n\nResulting Tree\n          [Age]\n         /  |   \\\n      &lt;30 31-40 &gt;40\n      /     |     \\\n  [Income]  Yes  [Income]\n    /  \\          /   \\\n  High Medium    Medium Low\n   No   Yes       Yes   No\n\n\n\nAdvantages of Decision Trees\n\nSimple to Understand: They are easy to visualize and interpret.\nNon-linear Relationships: Can capture non-linear relationships between features and the target variable.\nLittle Data Preparation: Requires little data preprocessing compared to other algorithms.\n\n\n\nDisadvantages of Decision Trees\n\nOverfitting: Trees can become very complex and overfit the training data, especially if not pruned.\nUnstable: Small changes in the data can lead to different splits and thus different trees.\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\nYou may also like:\n\n\n\nWhat is Supervised Machine Learning\n\n \n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023/budget-ml-deploy-options/index.html",
    "href": "posts/2023/budget-ml-deploy-options/index.html",
    "title": "Budget-Friendly Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models doesn’t have to break the bank. If you’re working within a budget, there are several cost-effective options available that still provide robust capabilities. In this blog, we’ll explore some of the best budget-friendly deployment strategies for machine learning models.\n\n\nServerless computing is a great way to minimize costs because you only pay for what you use, without worrying about managing infrastructure. Here are some economical serverless options:\nAWS Lambda:\n\nCost: Pay-per-request pricing.\nPros: Automatically scales with demand, integrates well with other AWS services.\nCons: Limited to short-duration tasks and smaller memory limits.\n\nGoogle Cloud Functions:\n\nCost: Pay-per-use based on the number of invocations, compute time, and memory allocated.\nPros: Excellent for event-driven architectures, integrates smoothly with other Google Cloud services.\nCons: Similar constraints on execution time and memory as AWS Lambda.\n\nAzure Functions:\n\nCost: Pay-per-execution pricing model.\nPros: Scalable and integrates with a wide range of Azure services.\nCons: Limited by the maximum execution duration and resource allocation.\n\n\n\n\nContainers can be a cost-effective way to deploy models by ensuring efficient use of resources and consistent performance across different environments.\nDocker:\n\nCost: Free to use; additional costs come from the underlying infrastructure you deploy it on.\nPros: Lightweight, portable, and scalable. Works well with various cloud and on-premises environments.\nCons: Requires some setup and management expertise.\n\nKubernetes (K8s):\n\nCost: Free, but incurs costs related to the infrastructure on which it runs.\nPros: Manages containerized applications at scale, great for complex deployments.\nCons: More complex setup and management compared to Docker alone.\n\n\n\n\nPaaS solutions can reduce operational overhead and are generally cost-effective for small to medium-scale deployments.\nGoogle App Engine:\n\nCost: Offers a free tier and then pay-as-you-go pricing.\nPros: Fully managed, scalable service for web applications and APIs.\nCons: Less control over the underlying infrastructure.\n\nHeroku:\n\nCost: Free tier available, with affordable pricing plans for higher usage.\nPros: Easy to use, supports multiple programming languages, and integrates with various databases.\nCons: Might become expensive as you scale up.\n\n\n\n\nIf you need more control over your deployment environment but still want to keep costs low, consider budget-friendly cloud services:\nDigitalOcean:\n\nCost: Very competitive pricing with flexible plans starting as low as $5 per month.\nPros: Simple to set up, provides good performance for the price, supports Docker and Kubernetes.\nCons: Fewer advanced features compared to larger cloud providers.\n\nLinode:\n\nCost: Affordable plans starting at $5 per month.\nPros: Good performance, simple pricing structure, supports various deployment configurations.\nCons: Smaller ecosystem and fewer services compared to larger cloud providers.\n\nVultr:\n\nCost: Plans also starting at $5 per month.\nPros: Wide range of instances, including high-frequency compute instances.\nCons: Similar to DigitalOcean and Linode in terms of limitations.\n\n\n\n\nFor specific use cases where you need low latency and reduced data transfer costs, edge deployment can be both effective and economical:\nRaspberry Pi:\n\nCost: Low-cost hardware starting around $35.\nPros: Great for small-scale deployments, supports various machine learning frameworks.\nCons: Limited computational power, suitable for less intensive tasks.\n\nNVIDIA Jetson Nano:\n\nCost: Around $99.\nPros: Affordable, powerful enough for real-time AI tasks on edge devices.\nCons: Higher cost than Raspberry Pi but offers better performance."
  },
  {
    "objectID": "posts/2023/budget-ml-deploy-options/index.html#budget-friendly-options-for-deploying-machine-learning-models",
    "href": "posts/2023/budget-ml-deploy-options/index.html#budget-friendly-options-for-deploying-machine-learning-models",
    "title": "Budget-Friendly Options for Deploying Machine Learning Models",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nDeploying machine learning models doesn’t have to break the bank. If you’re working within a budget, there are several cost-effective options available that still provide robust capabilities. In this blog, we’ll explore some of the best budget-friendly deployment strategies for machine learning models.\n\n\nServerless computing is a great way to minimize costs because you only pay for what you use, without worrying about managing infrastructure. Here are some economical serverless options:\nAWS Lambda:\n\nCost: Pay-per-request pricing.\nPros: Automatically scales with demand, integrates well with other AWS services.\nCons: Limited to short-duration tasks and smaller memory limits.\n\nGoogle Cloud Functions:\n\nCost: Pay-per-use based on the number of invocations, compute time, and memory allocated.\nPros: Excellent for event-driven architectures, integrates smoothly with other Google Cloud services.\nCons: Similar constraints on execution time and memory as AWS Lambda.\n\nAzure Functions:\n\nCost: Pay-per-execution pricing model.\nPros: Scalable and integrates with a wide range of Azure services.\nCons: Limited by the maximum execution duration and resource allocation.\n\n\n\n\nContainers can be a cost-effective way to deploy models by ensuring efficient use of resources and consistent performance across different environments.\nDocker:\n\nCost: Free to use; additional costs come from the underlying infrastructure you deploy it on.\nPros: Lightweight, portable, and scalable. Works well with various cloud and on-premises environments.\nCons: Requires some setup and management expertise.\n\nKubernetes (K8s):\n\nCost: Free, but incurs costs related to the infrastructure on which it runs.\nPros: Manages containerized applications at scale, great for complex deployments.\nCons: More complex setup and management compared to Docker alone.\n\n\n\n\nPaaS solutions can reduce operational overhead and are generally cost-effective for small to medium-scale deployments.\nGoogle App Engine:\n\nCost: Offers a free tier and then pay-as-you-go pricing.\nPros: Fully managed, scalable service for web applications and APIs.\nCons: Less control over the underlying infrastructure.\n\nHeroku:\n\nCost: Free tier available, with affordable pricing plans for higher usage.\nPros: Easy to use, supports multiple programming languages, and integrates with various databases.\nCons: Might become expensive as you scale up.\n\n\n\n\nIf you need more control over your deployment environment but still want to keep costs low, consider budget-friendly cloud services:\nDigitalOcean:\n\nCost: Very competitive pricing with flexible plans starting as low as $5 per month.\nPros: Simple to set up, provides good performance for the price, supports Docker and Kubernetes.\nCons: Fewer advanced features compared to larger cloud providers.\n\nLinode:\n\nCost: Affordable plans starting at $5 per month.\nPros: Good performance, simple pricing structure, supports various deployment configurations.\nCons: Smaller ecosystem and fewer services compared to larger cloud providers.\n\nVultr:\n\nCost: Plans also starting at $5 per month.\nPros: Wide range of instances, including high-frequency compute instances.\nCons: Similar to DigitalOcean and Linode in terms of limitations.\n\n\n\n\nFor specific use cases where you need low latency and reduced data transfer costs, edge deployment can be both effective and economical:\nRaspberry Pi:\n\nCost: Low-cost hardware starting around $35.\nPros: Great for small-scale deployments, supports various machine learning frameworks.\nCons: Limited computational power, suitable for less intensive tasks.\n\nNVIDIA Jetson Nano:\n\nCost: Around $99.\nPros: Affordable, powerful enough for real-time AI tasks on edge devices.\nCons: Higher cost than Raspberry Pi but offers better performance."
  },
  {
    "objectID": "posts/2023/budget-ml-deploy-options/index.html#key-tips-for-budget-friendly-deployment",
    "href": "posts/2023/budget-ml-deploy-options/index.html#key-tips-for-budget-friendly-deployment",
    "title": "Budget-Friendly Options for Deploying Machine Learning Models",
    "section": "Key Tips for Budget-Friendly Deployment",
    "text": "Key Tips for Budget-Friendly Deployment\n\nOptimize Your Model: Smaller and optimized models can reduce computational requirements and, consequently, deployment costs.\nLeverage Free Tiers: Many cloud providers offer free tiers or credits for new users; take advantage of these offers.\nAuto-scaling: Use auto-scaling features to ensure you’re only paying for the resources you need at any given time.\nMonitor and Optimize: Regularly monitor resource usage and optimize configurations to avoid unnecessary costs.\nUse Spot Instances: For non-critical workloads, consider using spot instances, which are significantly cheaper than regular instances.\n\nBy carefully selecting your deployment strategy and optimizing your resources, you can deploy machine learning models effectively without exceeding your budget.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝"
  },
  {
    "objectID": "posts/2023/programmer-puns/index.html",
    "href": "posts/2023/programmer-puns/index.html",
    "title": "Puns Only Programmers Will Get",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nWhat are the best puns? Definitely the ones only a specific group of people will understand and enjoy. Especially in the era we’re living in, we need to feel like a family and share “inside” jokes with one another. So, let’s take a look at some of the best programming puns we came upon during the pre-summer times.\n\nWhy do programmers like dark mode?\n\nBecause light attracts bugs.\n\nI used to work as a programmer for autocorrect…\n\nThen they fried me for no raisin.\n\nWhat do NASA programmers do on the weekends?\n\nThey hit the space bar.\n\nWhy did the programmer quit his job?\n\nBecause he didn’t get arrays.\n\nWhat was the SNES programmers’ favorite drink?\n\nSprite\n\nWhat do programmers do when they’re hungry?\n\nThey grab a byte.\n\nWhy couldn’t the programmer dance to the song?\n\nBecause he didn’t get the… algo-rhythm…\n\nWhat you call it when computer programmers make fun of each other?\n\nCyber boolean…\n\nI am now a successful programmer…\n\nBut back in the days I was a noobgrammer.\n\nWhy do programmers always mix up Halloween and Christmas?\n\nBecause Oct 31 equals Dec 25.\n\nWhy did the programmer get a huge telephone bill?\n\nBecause his program was CALLING a lot of subroutines.\n\nWhat do Spanish programmers code in?\n\nSí ++\n\nWhich way did the programmer go?\n\nHe went data way!\n\nWhy was the programmer always running into walls?\n\nHe couldn’t see sharp.\n\nHow programmers curse?\n\nOh shift!\n\nWhy do programmers make good politicians?\n\nTheir goto is to switch statements.\n\nHow did the programmer lose weight?\n\nHe switched to a byte-sized diet.\n\nI almost bought a huge library out of old computer programming books…\n\nBut the ascii price was way too high.\n\nWhat’s a Jedi’s favorite programming language?\n\nJabbaScript…\n\n\n\nWant to learn programming and become an expert?\nIf you’re serious about learning Programming and getting a job as a Developer, I highly encourage you to enroll in my programming courses for Python, JavaScript, Data Science and Web Development.\nDon’t waste your time following disconnected, outdated tutorials. I can teach you all that you need to kickstart your career.\nContact me at +256771754118 or jumashafara@proton.me\n\n\nDon’t Miss Any Updates!\n\n\n\nTo be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html",
    "href": "posts/2023/mr-robot-softwares/index.html",
    "title": "All Software used in Mr. Robot",
    "section": "",
    "text": "Thumbnail\nHere’s a list of the most popular softwares used by the hackers in the Emmy and Golden Globe award winning drama/thriller series Mr. Robot."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#kali-linux",
    "href": "posts/2023/mr-robot-softwares/index.html#kali-linux",
    "title": "All Software used in Mr. Robot",
    "section": "Kali Linux",
    "text": "Kali Linux\nKali Linux is a Linux distro made for security researchers for penetration testing, but is also used by hackers since it is jam packed with hacking tools. It is regularly featured in Mr. Robot since it is the hackers’ operating system of choice."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#tor-browser",
    "href": "posts/2023/mr-robot-softwares/index.html#tor-browser",
    "title": "All Software used in Mr. Robot",
    "section": "Tor Browser",
    "text": "Tor Browser\nTor Browser is widely considered to be the best anonymizing tool out there. It will make your Internet activity very hard to trace, which fsociety takes advantage of when Trenton in season 2, episode 8 uploads a leaked FBI conference call about illegal mass surveillance to Vimeo using Tor Browser."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#raspberry-pi",
    "href": "posts/2023/mr-robot-softwares/index.html#raspberry-pi",
    "title": "All Software used in Mr. Robot",
    "section": "Raspberry Pi",
    "text": "Raspberry Pi\nRaspberry Pi is a small, programmable computer board designed to teach children about computer science. It is also favourite among hobbyists and programmers due to its low-cost, versatility and simplicity. In season 1, episode 5 Elliot installs a Raspberry Pi into Steel Mountain’s climate control system so that fsociety at a later point in time can remotely raise the temperature in the storage room where Evil Corp’s tape backups are stored, resulting in the backups of the records of a significant portion of the US’ consumer debt being destroyed."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#proton-mail",
    "href": "posts/2023/mr-robot-softwares/index.html#proton-mail",
    "title": "All Software used in Mr. Robot",
    "section": "Proton Mail",
    "text": "Proton Mail\nProtonMail is a secure, end-to-end encrypted e-mail service based in Switzerland that is used by Elliot in season 1, episode 8. The team behind Mr. Robot researched secure e-mail services to the extent that they actually contacted the ProtonMail developers and asked if it was possible for users to monitor their own e-mail activity in ProtonMail. The ProtonMail developers liked the idea of account access logs so much that they ended up implementing it in their v2.0 release of ProtonMail."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#pycharm",
    "href": "posts/2023/mr-robot-softwares/index.html#pycharm",
    "title": "All Software used in Mr. Robot",
    "section": "PyCharm",
    "text": "PyCharm\nPyCharm is a Python and Django IDE (Integrated Developer Environment), which is a type of code editor software. It is used by Trenton in season 1, episode 4."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#bluesniff",
    "href": "posts/2023/mr-robot-softwares/index.html#bluesniff",
    "title": "All Software used in Mr. Robot",
    "section": "Bluesniff",
    "text": "Bluesniff\nBluesniff is a Bluetooth device discovery tool. In season 1, episode 6 Elliot uses Bluesniff in combination with btscanner and Metasploit when he connects to the computer in a nearby police car via a MultiBlue Bluetooth USB Dongle to compromise a prison’s network in order to break a drug dealer called Vera out of prison."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#btscanner",
    "href": "posts/2023/mr-robot-softwares/index.html#btscanner",
    "title": "All Software used in Mr. Robot",
    "section": "btscanner",
    "text": "btscanner\nbtscanner is a tool that is included in Kali Linux that extracts as much information as possible about Bluetooth devices without having to pair. In season 1, episode 6 Elliot uses btscanner in combination with Bluesniff and Metasploit when he connects to the computer in a nearby police car using a MultiBlue Bluetooth USB Dongle to compromise a prison’s network in order to break a drug dealer called Vera out of prison."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#mozilla-firefox",
    "href": "posts/2023/mr-robot-softwares/index.html#mozilla-firefox",
    "title": "All Software used in Mr. Robot",
    "section": "Mozilla Firefox",
    "text": "Mozilla Firefox\nElliot uses Firefox as his default web browser. Trenton uses Firefox in season 2, episode 8."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#vlc-media-player",
    "href": "posts/2023/mr-robot-softwares/index.html#vlc-media-player",
    "title": "All Software used in Mr. Robot",
    "section": "VLC Media Player",
    "text": "VLC Media Player\nVLC Media Player was used in season 2, episode 4 when Elliot and Darlene watched a VHS rip of the fake horror movie Careful Massacre of the Bourgeoisie together. VLC is also used in season 2, episode 8 when fsociety preview the video they are about to upload a leaked FBI conference call about illegal mass surveillance."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#µtorrent",
    "href": "posts/2023/mr-robot-softwares/index.html#µtorrent",
    "title": "All Software used in Mr. Robot",
    "section": "µTorrent",
    "text": "µTorrent\nIn season 2, episode 4 Darlene was downloading a VHS rip of the fake horror movie Careful Massacre of the Bourgeoisie using µTorrent."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#flexispy",
    "href": "posts/2023/mr-robot-softwares/index.html#flexispy",
    "title": "All Software used in Mr. Robot",
    "section": "FlexiSPY",
    "text": "FlexiSPY\nFlexiSPY is a spyware software for Android, iOS and BlackBerry that lets the user monitor all activities on the victims phone. In season 1, episode 3 Tyrell Wellick covertly installs it on a co-worker’s Android phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#kingoroot",
    "href": "posts/2023/mr-robot-softwares/index.html#kingoroot",
    "title": "All Software used in Mr. Robot",
    "section": "KingoRoot",
    "text": "KingoRoot\nKingo Root is used by Tyrell Wellick in season 1, episode 3 to root a co-worker’s Android phone so that he can covertly install the FlexiSPY spyware on the phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#kvm-kernel-based-virtual-machine",
    "href": "posts/2023/mr-robot-softwares/index.html#kvm-kernel-based-virtual-machine",
    "title": "All Software used in Mr. Robot",
    "section": "KVM (Kernel-based Virtual Machine)",
    "text": "KVM (Kernel-based Virtual Machine)\nKVM is a hypervisor, which is a software that can run other operating systems via virtual machines. Elliot uses KVM to virtualize Windows 7 inside of Kali Linux. In season 1, episode 6 Elliot uses KVM to run Metasploit and Metapreter and in season 1, episode 8 he uses KVM to run DeepSound."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#metasploit",
    "href": "posts/2023/mr-robot-softwares/index.html#metasploit",
    "title": "All Software used in Mr. Robot",
    "section": "Metasploit",
    "text": "Metasploit\nMetasploit Framework is a software included in Kali Linux that makes it easier to discover vulnerabilities in networks for penetration testers. Meterpreter is one of several hundreds of payloads that can be run in the Metasploit Framework and it is used in season 1, episode 6. In season 1, episode 6 Elliot uses Metasploit Framwork and Metapreter in combination with btscanner and Bluesniff when he connects to the computer in a nearby police car via a MultiBlue Bluetooth USB Dongle to compromise a prison’s network in order to break a drug dealer called Vera out of prison."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#framaroot",
    "href": "posts/2023/mr-robot-softwares/index.html#framaroot",
    "title": "All Software used in Mr. Robot",
    "section": "Framaroot",
    "text": "Framaroot\nFramaroot - called RooterFrame in the show - is used by Tyrell Wellick in season 1, episode 3 to root a co-worker’s Android phone so that he can covertly install the FlexiSPY spyware on the phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/2023/mr-robot-softwares/index.html#supersu",
    "href": "posts/2023/mr-robot-softwares/index.html#supersu",
    "title": "All Software used in Mr. Robot",
    "section": "SuperSU",
    "text": "SuperSU\nSuperSU is an app that managed superuser privileges on rooted Android phones. In season 1, episode 3 Tyrell Wellick covertly installs FlexiSPY - which uses SuperSU to give itself superuser access - on a co-worker’s Android phone in order to get access to secret information about who is going to be the next chief technology officer of Evil Corp."
  },
  {
    "objectID": "posts/2024/understanding-knearest-neighbors-regression/index.html",
    "href": "posts/2024/understanding-knearest-neighbors-regression/index.html",
    "title": "Understanding K-Nearest Neighbors (KNN) Regression",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/2024/understanding-knearest-neighbors-regression/index.html#introduction-to-knn-regression",
    "href": "posts/2024/understanding-knearest-neighbors-regression/index.html#introduction-to-knn-regression",
    "title": "Understanding K-Nearest Neighbors (KNN) Regression",
    "section": "Introduction to KNN Regression",
    "text": "Introduction to KNN Regression\nK-Nearest Neighbors (KNN) regression is a type of instance-based learning algorithm used for regression problems. It makes predictions based on the \\(k\\) most similar instances (neighbors) in the training dataset. The algorithm is non-parametric, meaning it makes predictions without assuming any underlying data distribution.\n\n\nDon’t Miss Any Updates!\n\n\n\nTo be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel."
  },
  {
    "objectID": "posts/2024/understanding-knearest-neighbors-regression/index.html#key-concepts",
    "href": "posts/2024/understanding-knearest-neighbors-regression/index.html#key-concepts",
    "title": "Understanding K-Nearest Neighbors (KNN) Regression",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nDistance Metric: The method used to calculate the distance between instances. Common metrics include Euclidean, Manhattan, and Minkowski distances.\nk: The number of neighbors to consider when making a prediction. Choosing the right \\(k\\) is crucial for the algorithm’s performance.\nWeighted KNN: In some variants, closer neighbors have a higher influence on the prediction than more distant ones, often implemented by assigning weights inversely proportional to the distance."
  },
  {
    "objectID": "posts/2024/understanding-knearest-neighbors-regression/index.html#how-knn-regression-works",
    "href": "posts/2024/understanding-knearest-neighbors-regression/index.html#how-knn-regression-works",
    "title": "Understanding K-Nearest Neighbors (KNN) Regression",
    "section": "How KNN Regression Works",
    "text": "How KNN Regression Works\n\nStep-by-Step Process\n\nLoad the Data: Start with a dataset consisting of feature vectors and their corresponding target values.\nChoose the Number of Neighbors (k): Select the number of nearest neighbors to consider for making predictions.\nDistance Calculation: For a new data point, calculate the distance between this point and all points in the training dataset.\nFind Nearest Neighbors: Identify the \\(k\\) points in the training data that are closest to the new point.\nPredict the Target Value: Compute the average (or a weighted average) of the target values of the \\(k\\) nearest neighbors.\n\n\n\nExample\nLet’s walk through an example with a simple dataset.\nDataset:\n\n\n\nFeature (X)\nTarget (Y)\n\n\n\n\n1.0\n2.0\n\n\n2.0\n3.0\n\n\n3.0\n4.5\n\n\n4.0\n6.0\n\n\n5.0\n7.5\n\n\n\nNew Point: \\(X_{new} = 3.5\\)\n\nChoose \\(k\\): Let’s select \\(k = 3\\).\nCalculate Distances:\n\nDistance to (1.0, 2.0): \\(\\sqrt{(3.5-1.0)^2} = 2.5\\)\nDistance to (2.0, 3.0): \\(\\sqrt{(3.5-2.0)^2} = 1.5\\)\nDistance to (3.0, 4.5): \\(\\sqrt{(3.5-3.0)^2} = 0.5\\)\nDistance to (4.0, 6.0): \\(\\sqrt{(3.5-4.0)^2} = 0.5\\)\nDistance to (5.0, 7.5): \\(\\sqrt{(3.5-5.0)^2} = 1.5\\)\n\nFind Nearest Neighbors:\n\nNeighbors: (3.0, 4.5), (4.0, 6.0), and (2.0, 3.0) (distances 0.5, 0.5, and 1.5 respectively)\n\nPredict the Target Value:\n\nAverage the target values of the nearest neighbors: \\(\\frac{4.5 + 6.0 + 3.0}{3} = \\frac{13.5}{3} = 4.5\\)\n\n\nSo, the predicted target value for \\(X_{new} = 3.5\\) is 4.5."
  },
  {
    "objectID": "posts/2024/understanding-knearest-neighbors-regression/index.html#visualizing-knn-regression",
    "href": "posts/2024/understanding-knearest-neighbors-regression/index.html#visualizing-knn-regression",
    "title": "Understanding K-Nearest Neighbors (KNN) Regression",
    "section": "Visualizing KNN Regression",
    "text": "Visualizing KNN Regression\nBelow is a visual representation of the KNN regression process:\n\n\nThe blue points represent the training data.\nThe red point is the new input for which we want to predict the target value.\nThe green points are the nearest neighbors considered for the prediction.\n\n\nAdvantages and Disadvantages\n\nAdvantages:\n\nSimplicity: Easy to understand and implement.\nNo Training Phase: The algorithm stores the training dataset and makes predictions at runtime.\n\n\n\nDisadvantages:\n\nComputationally Intensive: Requires computing the distance to all training points for each prediction, which can be slow for large datasets.\nChoosing ( k ): Selecting the optimal ( k ) can be challenging and often requires cross-validation.\nCurse of Dimensionality: Performance can degrade in high-dimensional spaces as distances become less meaningful.\n\n\n\n\nConclusion\nKNN regression is a straightforward and intuitive algorithm for making predictions based on the similarity of data points. Despite its simplicity, it can be quite powerful, especially for smaller datasets. However, it can become computationally expensive for large datasets and high-dimensional data, and it requires careful selection of the number of neighbors ( k ).\nBy understanding and visualizing the KNN regression process, you can better appreciate its applications and limitations, allowing you to apply it effectively in your machine learning projects.\n\n\n\n\n\n\n\n\nDon’t Miss Any Updates!\n\n\n\nTo be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\n\nYou may also like:\n\n\n\nHow the Decision Tree Classifier Works"
  },
  {
    "objectID": "posts/2024/hyperparameter-tuning-with-grid-search-cv/index.html",
    "href": "posts/2024/hyperparameter-tuning-with-grid-search-cv/index.html",
    "title": "GridSearchCV",
    "section": "",
    "text": "Photo by DATAIDEA"
  },
  {
    "objectID": "posts/2024/hyperparameter-tuning-with-grid-search-cv/index.html#what-is-gridsearchcv",
    "href": "posts/2024/hyperparameter-tuning-with-grid-search-cv/index.html#what-is-gridsearchcv",
    "title": "GridSearchCV",
    "section": "What is GridSearchCV",
    "text": "What is GridSearchCV\nGridSearchCV is a method in the scikit-learn library, which is a popular machine learning library in Python. It’s used for hyperparameter optimization, which involves searching for the best set of hyperparameters for a machine learning model. In this notebook, we’ll learn:\n\nhow to setup a proper GridSearchCV and\nhow to use it for hyperparameter optimization.\n\n\n\nDon’t Miss Any Updates!\n\n\n\nBefore we get started, if you want to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\nLet’s import some packages\nWe begin by importing necessary packages and modules. The KNeighborsRegressor model is imported from the sklearn.neighbors module. KNN regression is a non-parametric method that, in an intuitive manner, approximates the association between independent variables and the continuous outcome by averaging the observations in the same neighbourhood. Read more about the KNN Regressor from this link\n# Let's import some packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport dataidea as di\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nLet’s import necessary components from sklearn\nWe import essential components from sklearn, including Pipeline, which we’ll use to create a pipe as from the previous section, ColumnTransformer, StandardScaler, and OneHotEncoder which we’ll use to transform the numeric and categorical columns respectively to be good for modelling.\n# lets import the Pipeline from sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nLoading the dataset\nWe load the dataset named boston using the loadDataset function, which is inbuilt in the dataidea package. The loaded dataset is stored in the variable data.\n# loading the data set\ndata = di.loadDataset('boston')\n# looking at the top part\ndata.head()\n\n\n\n\n\n\n\n\nCRIM\n\n\nZN\n\n\nINDUS\n\n\nCHAS\n\n\nNOX\n\n\nRM\n\n\nAGE\n\n\nDIS\n\n\nRAD\n\n\nTAX\n\n\nPTRATIO\n\n\nB\n\n\nLSTAT\n\n\nMEDV\n\n\n\n\n\n\n0\n\n\n0.00632\n\n\n18.0\n\n\n2.31\n\n\n0\n\n\n0.538\n\n\n6.575\n\n\n65.2\n\n\n4.0900\n\n\n1\n\n\n296.0\n\n\n15.3\n\n\n396.90\n\n\n4.98\n\n\n24.0\n\n\n\n\n1\n\n\n0.02731\n\n\n0.0\n\n\n7.07\n\n\n0\n\n\n0.469\n\n\n6.421\n\n\n78.9\n\n\n4.9671\n\n\n2\n\n\n242.0\n\n\n17.8\n\n\n396.90\n\n\n9.14\n\n\n21.6\n\n\n\n\n2\n\n\n0.02729\n\n\n0.0\n\n\n7.07\n\n\n0\n\n\n0.469\n\n\n7.185\n\n\n61.1\n\n\n4.9671\n\n\n2\n\n\n242.0\n\n\n17.8\n\n\n392.83\n\n\n4.03\n\n\n34.7\n\n\n\n\n3\n\n\n0.03237\n\n\n0.0\n\n\n2.18\n\n\n0\n\n\n0.458\n\n\n6.998\n\n\n45.8\n\n\n6.0622\n\n\n3\n\n\n222.0\n\n\n18.7\n\n\n394.63\n\n\n2.94\n\n\n33.4\n\n\n\n\n4\n\n\n0.06905\n\n\n0.0\n\n\n2.18\n\n\n0\n\n\n0.458\n\n\n7.147\n\n\n54.2\n\n\n6.0622\n\n\n3\n\n\n222.0\n\n\n18.7\n\n\n396.90\n\n\n5.33\n\n\n36.2\n\n\n\n\n\n\n\nReveal more about the Boston dataset\n\nThe Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n\nCRIM - per capita crime rate by town\nZN - proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS - proportion of non-retail business acres per town.\nCHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\nNOX - nitric oxides concentration (parts per 10 million)\nRM - average number of rooms per dwelling\nAGE - proportion of owner-occupied units built prior to 1940\nDIS - weighted distances to five Boston employment centres\nRAD - index of accessibility to radial highways\nTAX - full-value property-tax rate per $10,000\nPTRATIO - pupil-teacher ratio by town\nB - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\nLSTAT - % lower status of the population\nMEDV - Median value of owner-occupied homes in $1000’s\n\n\n\n\nSelecting features (X) and target variable (y)\nWe separate the features (X) from the target variable (y). Features are stored in X, excluding the target variable ‘MEDV’, which is stored in y.\n# Selecting our X set and y\nX = data.drop('MEDV', axis=1)\ny = data.MEDV\n\n\nDefining numeric and categorical columns\nWe define lists of column names representing numeric and categorical features in the dataset. We identified these columns as the best features from the previous section of this week. Click here to learn about feature selection\n# numeric columns\nnumeric_cols = [\n    'INDUS', 'NOX', 'RM',\n    'TAX', 'PTRATIO', 'LSTAT'\n    ]\n\n# categorical columns\ncategorical_cols = ['CHAS', 'RAD']\n\n\nPreprocessing steps\nWe define transformers for preprocessing numeric and categorical features. StandardScaler is used for standardizing numeric features, while OneHotEncoder is used for one-hot encoding categorical features. These transformers are applied to respective feature types using ColumnTransformer as we learned in the previous section.\n# Preprocessing steps\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\n# Combine preprocessing steps\ncolumn_transformer = ColumnTransformer(\n    transformers=[\n        ('numeric', numeric_transformer, numeric_cols),\n        ('categorical', categorical_transformer, categorical_cols)\n    ])\n\n\nDefining the pipeline\nWe construct a machine learning pipeline using Pipeline. The pipeline consists of preprocessing steps (defined in column_transformer) and a KNeighborsRegressor model with 10 neighbors. Learn about Machine Learning Pipelining here\n# Pipeline\npipe = Pipeline([\n    ('column_transformer', column_transformer),\n    ('model', KNeighborsRegressor(n_neighbors=10))\n])\n\npipe\n\n\n\nPipeline(steps=[('column_transformer',\n\n                 ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor(n_neighbors=10))])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n  Pipeline?Documentation for PipelineiNot fitted\n\nPipeline(steps=[('column_transformer',\n                 ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor(n_neighbors=10))])\n\n\n\n\n\n\n\n column_transformer: ColumnTransformer?Documentation for column_transformer: ColumnTransformer\n\nColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                 ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO',\n                                  'LSTAT']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['CHAS', 'RAD'])])\n\n\n\n\n\n\n\n\nnumeric\n\n['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']\n\n\n\n\n\n\n StandardScaler?Documentation for StandardScaler\n\nStandardScaler()\n\n\n\n\n\n\n\n\n\n\ncategorical\n\n['CHAS', 'RAD']\n\n\n\n\n\n\n OneHotEncoder?Documentation for OneHotEncoder\n\nOneHotEncoder(handle_unknown='ignore')\n\n\n\n\n\n\n\n\n\n\n KNeighborsRegressor?Documentation for KNeighborsRegressor\n\nKNeighborsRegressor(n_neighbors=10)\n\n\n\n\n\n\n\n\n\nFitting the pipeline\nAs we learned, the Pipeline has the fit, score and predict methods which we use to fit on the dataset (X, y) and evaluate the model’s performance using the score() method, finally making predictions.\n# Fit the pipeline\npipe.fit(X, y)\n\n# Score the pipeline\npipe_score = pipe.score(X, y)\n\n# Predict using the pipeline\npipe_predicted_y = pipe.predict(X)\n\nprint('Pipe Score:', pipe_score)\nPipe Score: 0.818140222027107\n\n\nHyperparameter tuning using GridSearchCV\nWe perform hyperparameter tuning using GridSearchCV. The pipeline (pipe) serves as the base estimator, and we define a grid of hyperparameters to search through.\nFor this demonstration, we will focus on the number of neighbors for the KNN model.\nfrom sklearn.model_selection import GridSearchCV\nmodel = GridSearchCV(\n    estimator=pipe,\n    param_grid={\n        'model__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    },\n    cv=3\n    )\n\n\nFitting the model for hyperparameter tuning\nWe fit the GridSearchCV model on the dataset to find the optimal hyperparameters. This involves preprocessing the data and training the model multiple times using cross-validation.\nmodel.fit(X, y)\n\n\n\nGridSearchCV(cv=3,\n\n             estimator=Pipeline(steps=[('column_transformer',\n                                        ColumnTransformer(transformers=[('numeric',\n                                                                         StandardScaler(),\n                                                                         ['INDUS',\n                                                                          'NOX',\n                                                                          'RM',\n                                                                          'TAX',\n                                                                          'PTRATIO',\n                                                                          'LSTAT']),\n                                                                        ('categorical',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['CHAS',\n                                                                          'RAD'])])),\n                                       ('model',\n                                        KNeighborsRegressor(n_neighbors=10))]),\n             param_grid={'model__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n  GridSearchCV?Documentation for GridSearchCViFitted\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('column_transformer',\n                                        ColumnTransformer(transformers=[('numeric',\n                                                                         StandardScaler(),\n                                                                         ['INDUS',\n                                                                          'NOX',\n                                                                          'RM',\n                                                                          'TAX',\n                                                                          'PTRATIO',\n                                                                          'LSTAT']),\n                                                                        ('categorical',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['CHAS',\n                                                                          'RAD'])])),\n                                       ('model',\n                                        KNeighborsRegressor(n_neighbors=10))]),\n             param_grid={'model__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n\n\n\n\n\n\n\n\nestimator: Pipeline\n\nPipeline(steps=[('column_transformer',\n                 ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor(n_neighbors=10))])\n\n\n\n\n\n\n\n\n\n column_transformer: ColumnTransformer?Documentation for column_transformer: ColumnTransformer\n\nColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                 ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO',\n                                  'LSTAT']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['CHAS', 'RAD'])])\n\n\n\n\n\n\n\n\nnumeric\n\n['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']\n\n\n\n\n\n\n StandardScaler?Documentation for StandardScaler\n\nStandardScaler()\n\n\n\n\n\n\n\n\n\n\ncategorical\n\n['CHAS', 'RAD']\n\n\n\n\n\n\n OneHotEncoder?Documentation for OneHotEncoder\n\nOneHotEncoder(handle_unknown='ignore')\n\n\n\n\n\n\n\n\n\n\n KNeighborsRegressor?Documentation for KNeighborsRegressor\n\nKNeighborsRegressor(n_neighbors=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting and displaying cross-validation results\nWe extract the results of cross-validation performed during hyperparameter tuning and present them in a tabular format using a DataFrame.\ncv_results = pd.DataFrame(model.cv_results_)\ncv_results\n\n\n\n\n\n\n\n\nmean_fit_time\n\n\nstd_fit_time\n\n\nmean_score_time\n\n\nstd_score_time\n\n\nparam_model__n_neighbors\n\n\nparams\n\n\nsplit0_test_score\n\n\nsplit1_test_score\n\n\nsplit2_test_score\n\n\nmean_test_score\n\n\nstd_test_score\n\n\nrank_test_score\n\n\n\n\n\n\n0\n\n\n0.006702\n\n\n0.003290\n\n\n0.003588\n\n\n0.000087\n\n\n1\n\n\n{’model__n_neighbors’: 1}\n\n\n0.347172\n\n\n0.561780\n\n\n0.295295\n\n\n0.401415\n\n\n0.115356\n\n\n10\n\n\n\n\n1\n\n\n0.004681\n\n\n0.000273\n\n\n0.003889\n\n\n0.000292\n\n\n2\n\n\n{’model__n_neighbors’: 2}\n\n\n0.404829\n\n\n0.612498\n\n\n0.276690\n\n\n0.431339\n\n\n0.138369\n\n\n9\n\n\n\n\n2\n\n\n0.005089\n\n\n0.000512\n\n\n0.003540\n\n\n0.000681\n\n\n3\n\n\n{’model__n_neighbors’: 3}\n\n\n0.466325\n\n\n0.590333\n\n\n0.243375\n\n\n0.433345\n\n\n0.143552\n\n\n8\n\n\n\n\n3\n\n\n0.004812\n\n\n0.000723\n\n\n0.003431\n\n\n0.000099\n\n\n4\n\n\n{’model__n_neighbors’: 4}\n\n\n0.569672\n\n\n0.619854\n\n\n0.246539\n\n\n0.478688\n\n\n0.165428\n\n\n4\n\n\n\n\n4\n\n\n0.004633\n\n\n0.000316\n\n\n0.003406\n\n\n0.000133\n\n\n5\n\n\n{’model__n_neighbors’: 5}\n\n\n0.613900\n\n\n0.600994\n\n\n0.230320\n\n\n0.481738\n\n\n0.177857\n\n\n2\n\n\n\n\n5\n\n\n0.004805\n\n\n0.000379\n\n\n0.003908\n\n\n0.000318\n\n\n6\n\n\n{’model__n_neighbors’: 6}\n\n\n0.620587\n\n\n0.607083\n\n\n0.225238\n\n\n0.484302\n\n\n0.183269\n\n\n1\n\n\n\n\n6\n\n\n0.004646\n\n\n0.000606\n\n\n0.003733\n\n\n0.000206\n\n\n7\n\n\n{’model__n_neighbors’: 7}\n\n\n0.639693\n\n\n0.583685\n\n\n0.218612\n\n\n0.480663\n\n\n0.186704\n\n\n3\n\n\n\n\n7\n\n\n0.005043\n\n\n0.000516\n\n\n0.003801\n\n\n0.000189\n\n\n8\n\n\n{’model__n_neighbors’: 8}\n\n\n0.636143\n\n\n0.567841\n\n\n0.209472\n\n\n0.471152\n\n\n0.187125\n\n\n5\n\n\n\n\n8\n\n\n0.004495\n\n\n0.000117\n\n\n0.003897\n\n\n0.000195\n\n\n9\n\n\n{’model__n_neighbors’: 9}\n\n\n0.649335\n\n\n0.542624\n\n\n0.197917\n\n\n0.463292\n\n\n0.192639\n\n\n6\n\n\n\n\n9\n\n\n0.004543\n\n\n0.000164\n\n\n0.003654\n\n\n0.000323\n\n\n10\n\n\n{’model__n_neighbors’: 10}\n\n\n0.653370\n\n\n0.535112\n\n\n0.191986\n\n\n0.460156\n\n\n0.195674\n\n\n7\n\n\n\n\n\n\n\nReveal the interpretation of the CV results\n\nThese are the results of a grid search cross-validation performed on our pipeline (pipe). Let’s break down each column:\n\nmean_fit_time: The average time taken to fit the estimator on the training data across all folds.\nstd_fit_time: The standard deviation of the fitting time across all folds.\nmean_score_time: The average time taken to score the estimator on the test data across all folds.\nstd_score_time: The standard deviation of the scoring time across all folds.\nparam_model__n_neighbors: The value of the n_neighbors parameter of the KNeighborsRegressor model in our pipeline for this particular grid search iteration.\nparams: A dictionary containing the parameters used in this grid search iteration.\nsplit0_test_score, split1_test_score, split2_test_score: The test scores obtained for each fold of the cross-validation. Each fold corresponds to one entry here.\nmean_test_score: The average test score across all folds.\nstd_test_score: The standard deviation of the test scores across all folds.\nrank_test_score: The rank of this model configuration based on the mean test score. Lower values indicate better performance.\n\nThese results allow you to compare different parameter configurations and select the one that performs best based on the mean test score and other relevant metrics.\n\nFrom the results above, it appears that the best number of neighbors to is 6.\nFrom now on, I would like you to consider a GridSearchCV whenever you want to build a machine learning model."
  },
  {
    "objectID": "posts/2024/hyperparameter-tuning-with-grid-search-cv/index.html#congratulations",
    "href": "posts/2024/hyperparameter-tuning-with-grid-search-cv/index.html#congratulations",
    "title": "GridSearchCV",
    "section": "Congratulations!",
    "text": "Congratulations!\nIf you reached here, you have learned the following:\n\nSelecting Features\nPreprocessing data\nCreating a Machine Learning Pipeline\nCreating a GridSearchCV\nUsing the GridSearchCV to find the best Hyperparameters for our Machine Learning model."
  },
  {
    "objectID": "posts/2024/hyperparameter-tuning-with-grid-search-cv/index.html#what-do-you-think-put-it-in-the-comments-below",
    "href": "posts/2024/hyperparameter-tuning-with-grid-search-cv/index.html#what-do-you-think-put-it-in-the-comments-below",
    "title": "GridSearchCV",
    "section": "What do you think? Put it in the comments below!",
    "text": "What do you think? Put it in the comments below!\n\n\n\n\n\n\n\n\n\nDon’t Miss Any Updates!\n\n\n\nTo be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel."
  }
]